Conditional random fields are configured as a linear-chain (finite state machine) for this purpose,and tagging is performed using the Viterbi algorithm to efficiently find the most likely label se quence for a given character sequence.
One advantage of CRFs (as well as traditional maximum entropy models) is its flexibility in using arbitrary features of the input.
To specifically evaluate the importance ofdomain knowledge beyond the training data, we divide our features into two categories: closed fea tures and open features, (i.e., features allowed in thecompetition?s ?closed test? and ?open test? respec tively).
Since CRFs are log-linear models, feature conjunctions are required to form complex, non-linear de cision boundaries in the original feature space.
We consider here new word detection as an integral part of segmentation, aimingto improve both segmentation and new word detec tion: detected new words are added to the word list lexicon in order to improve segmentation; improved segmentation can potentially further improve new word detection.
We measure the performance ofnew word detection by its improvements on seg mentation.
The standard forward-backward algorithm (Rabiner, 1990) calculates Zx, the total like lihood of all label sequences y given a sequence x. Constrained forward-backward algorithm calculates Z ?x, total likelihood of all paths passing through a constrained segment (in our case, a sequence of characters starting with a START tag followed by a few NONSTART tags before the next START tag).
The confidence in this segment is then Z ? x Zx , a real number between 0 and 1.In order to increase recall of new words, we consider not only the most likely (Viterbi) segmen tation, but the segmentations in the top N most likely segmentations (an N -best list), and detect new words according to the above criteria in all N segmentations.Many errors can be corrected by new word detection.
The standard bake-off scoring program is used to calculate precision, recall, F1, and OOV word recall.
Since CTB and PK are provided in the GB encod ing while AS and HK use the Big5 encoding, we convert AS and HK datasets to GB in order to make cross-training-and-testing possible.
We use cross-validation to choose Markov-order and perform feature selection.
Thus, each training set is randomly split?80% used for training and theremaining 20% for validation?and based on vali dation set performance, choices are made for model structure, prior, and which word lexicons to include.
Encoding #Train words #Test Words OOV rate (%) UPenn Chinese Treebank CTB GB 250K 40K 18.1 Beijing University PK GB 1.1M 17K 6.9 Hong Kong City U HK Big 5 240K 35K 7.1 Academia Sinica AS Big 5 5.8M 12K 2.2 Table 1: Datasets statistics bin-Size M Markov order CTB 10 first-order + transitions PK 15 first-order + transitions HK 1 first-order AS 15 first-order + transitions Table 2: Optimal prior and Markov order setting in Section 3.1.
Closed Precision Recall F1 Roov CTB 0.828 0.870 0.849 0.550 PK 0.935 0.947 0.941 0.660 HK 0.917 0.940 0.928 0.531 AS 0.950 0.962 0.956 0.292 Open Precision Recall F1 Roov CTB 0.889 0.898 0.894 0.619 PK 0.941 0.952 0.946 0.676 HK 0.944 0.948 0.946 0.629 AS 0.953 0.961 0.957 0.403 Table 3: Overall results of CRF segmentation on closed and open tests To compare our results against other systems, we summarize the competition results reported in (Sproat and Emerson, 2003) in Table 4.
Only one site (S01)achieved two best runs (CTBc and PKc) with an av erage of 91.8% over 6 runs.
We also achieve two best runs (ASo and HKc), with a comparable average of 91.9% over the same 6 runs, and a 92.7% average over all the 8 runs.Second, performance varies significantly across dif ferent datasets, indicating that the four datasets havedifferent characteristics and use very different seg mentation guidelines.
We randomly split the training data into 80% training and 20%testing, and run the experiments for 3 times, result ing in a testing F1 of 97.13%.
They participated in two datasets, with an average of 93.8%.
Our average over the same two runs is 94.2%.
An interesting observation is CTB PK HK AS w/o NWD 0.792 0.934 0.916 0.956 NWD 0.849 0.941 0.928 0.946 Table 6: New word detection effects: w/o NWD is the results without new word detection and NWD is the results with new word detection.
