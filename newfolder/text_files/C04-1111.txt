Previous approaches to extracting is-a relations fall under two categories: pattern-based and co occurrence-based approaches.
2.1 Pattern-based approaches.
Marti Hearst (1992) was the first to use a pat tern-based approach to extract hyponym relations from a raw corpus.
She used an iterative process to semi-automatically learn patterns.
However, a corpus of 20MB words yielded only 400 examples.
Our pattern-based algorithm is very similar to the one used by Hearst.
She uses seed examples to manually discover her patterns whearas we use a minimal edit distance algorithm to automatically discover the patterns.
771Riloff and Shepherd (1997) used a semi automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision.
Berland and Charniak (1999) used similar pattern-based tech niques and other heuristics to extract meronymy (part-whole) relations.
They reported an accuracy of about 55% precision on a corpus of 100,000 words.
Girju et al (2003) improved upon Berland and Charniak's work using a machine learning filter.
Mann (2002) and Fleischman et al (2003) used part of speech patterns to extract a subset of hyponym relations involving proper nouns.
Our pattern-based algorithm differs from these approaches in two ways.
We learn lexico-POS patterns in an automatic way.
Also, the patterns are learned with the specific goal of scaling to the terascale (see Table 2).
2.2 Co-occurrence-based approaches.
The second class of algorithms uses co occurrence statistics (Hindle 1990, Lin 1998).
These systems mostly employ clustering algo rithms to group words according to their meanings in text.
Assuming the distributional hypothesis (Harris 1985), words that occur in similar gram matical contexts are similar in meaning.
Curran and Moens (2002) experimented with corpus size and complexity of proximity features in building automatic thesauri.
CBC (Clustering by Commit tee) proposed by Pantel and Lin (2002) achieves high recall and precision in generating similarity lists of words discriminated by their meaning and senses.
However, such clustering algorithms fail to name their classes.
Caraballo (1999) was the first to use clustering for labeling is-a relations using conjunction and apposition features to build noun clusters.
Re cently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic de pendency features for each noun.
Much of the research discussed above takes a similar approach of searching text for simple sur face or lexico-syntactic patterns in a bottom-up approach.
Our co-occurrence model (Pantel and Ravichandran 2004) makes use of semantic classes like those generated by CBC.
Hyponyms are gen erated in a top-down approach by naming each group of words and assigning that name as a hypo nym of each word in the group (i.e., one hyponym per instance/group label pair).
The input to the extraction algorithm is a list of semantic classes, in the form of clusters of words, which may be generated from any source.
For example, following are two semantic classes discov ered by CBC: (A) peach, pear, pineapple, apricot, mango, raspberry, lemon, cherry, strawberry, melon, blueberry, fig, apple, plum, nectarine, avocado, grapefruit, papaya, banana, cantaloupe, cranberry, blackberry, lime, orange, tangerine, ...
(B) Phil Donahue, Pat Sajak, Arsenio Hall, Geraldo Rivera, Don Imus, Larry King, David Letterman, Conan O'Brien, Rosie O'Donnell, Jenny Jones, Sally Jessy Raph ael, Oprah Winfrey, Jerry Springer, Howard Stern, Jay Leno, Johnny Carson, ...
The extraction algorithm first labels concepts (A) and (B) with fruit and host respectively.
Then, is-a relationships are extracted, such as: apple is a fruit, pear is a fruit, and David Letterman is a host.
An instance such as pear is assigned a hypernym fruit not because it necessarily occurs in any par ticular syntactic relationship with the word fruit, but because it belongs to the class of instances that does.
The labeling of semantic classes is performed in three phases, as outlined below.
3.1 Phase I. In the first phase of the algorithm, feature vec tors are extracted for each word that occurs in a semantic class.
Each feature corresponds to a grammatical context in which the word occurs.
For example, ?catch __?
is a verb-object context.
If the word wave occurred in this context, then the con text is a feature of wave.
We then construct a mutual information vector MI(e) = (mie1, mie2, ?, miem) for each word e, where mief is the pointwise mutual information between word e and context f, which is defined as: N c N c N c ef m j ej n i if ef mi ??
== ? = 11 log Table 2.
Sample of 10 is-a relationships discovered by our co-occurrence and pattern-based systems.
CO-OCCURRENCE SYSTEM PATTERN-BASED SYSTEM Word Hypernym Word Hypernym azalea flower American airline bipolar disorder disease Bobby Bonds coach Bordeaux wine radiation therapy cancer treatment Flintstones television show tiramisu dessert salmon fish Winona Ryder actress Table 1.
Approximate processing time on a single Pentium-4 2.5 GHz machine.
TOOL 15 GB ORPUS 1 TB CORPUS POS Tagger 2 days 125 days NP Chunker 3 days 214 days Dependency Parser 56 days 10.2 years Syntactic Parser 5.8 years 388.4 years 772 where n is the number of elements to be clustered, cef is the frequency count of word e in grammatical context f, and N is the total frequency count of all features of all words.
3.2 Phase II.
Following (Pantel and Lin 2002), a committee for each semantic class is constructed.
A committee is a set of representative elements that unambi guously describe the members of a possible class.
For example, in one of our experiments, the committees for semantic classes (A) and (B) from Sec tion 3 were: A) peach, pear, pineapple, apricot, mango, raspberry, lemon, blueberry B) Phil Donahue, Pat Sajak, Arsenio Hall, Geraldo Rivera, Don Imus, Larry King, David Letterman 3.3 Phase III.
By averaging the feature vectors of the commit tee members of a particular semantic class, we obtain a grammatical template, or signature, for that class.
For example, Figure 1 shows an excerpt of the grammatical signature for semantic class (B).
The vector is obtained by averaging the fea ture vectors of the words in the committee of this class.
The ?V:subj:N:joke?
feature indicates a sub ject-verb relationship between the class and the verb joke while ?N:appo:N:host?
indicates an ap position relationship between the class and the noun host.
The two columns of numbers indicate the frequency and mutual information scores.
To name a class, we search its signature for cer tain relationships known to identify class labels.
These relationships, automatically learned in (Pantel and Ravichandran 2004), include apposi tions, nominal subjects, such as relationships, and like relationships.
We sum up the mutual information scores for each term that occurs in these rela tionships with a committee of a class.
The highest scoring term is the name of the class.
The syntactical co-occurrence approach has worst-case time complexity O(n2k), where n is the number of words in the corpus and k is the feature space (Pantel and Ravichandran 2004).
Just to parse a 1 TB corpus, this approach requires ap proximately 10.2 years (see Table 2).
We propose an algorithm for learning highly scalable lexico-POS patterns.
Given two sentences with their surface form and part of speech tags, the algorithm finds the optimal lexico-POS alignment.
For example, consider the following 2 sentences: 1) Platinum is a precious metal.
2) Molybdenum is a metal.
Applying a POS tagger (Brill 1995) gives the following output: Surface Platinum is a precious metal . POS NNP VBZ DT JJ NN . Surface Molybdenum is a metal . POS NNP VBZ DT NN . A very good pattern to generalize from the alignment of these two strings would be Surface is a metal . POS NNP . We use the following notation to denote this alignment: ?_NNP is a (*s*) metal.?, where ?_NNP represents the POS tag NNP?.
To perform such alignments we introduce two wildcard operators, skip (*s*) and wildcard (*g*).
The skip operator represents 0 or 1 instance of any word (similar to the \w* pattern in Perl), while the wildcard operator represents exactly 1 instance of any word (similar to the \w+ pattern in Perl).
4.1 Algorithm.
We present an algorithm for learning patterns at multiple levels.
Multilevel representation is de fined as the different levels of a sentence such as the lexical level and POS level.
Consider two strings a(1, n) and b(1, m) of lengths n and m re spectively.
Let a1(1, n) and a2(1, n) be the level 1 (lexical level) and level 2 (POS level) representa tions for the string a(1, n).
Similarly, let b1(1, m) and b2(1, m) be the level 1 and level 2 representa tions for the string b(1, m).
The algorithm consists of two parts: calculation of the minimal edit dis tance and retrieval of an optimal pattern.
The minimal edit distance algorithm calculates the number of edit operations (insertions, deletions and replacements) required to change one string to another string.
The optimal pattern is retrieved by {Phil Donahue,Pat Sajak,Arsenio Hall} N:gen:N talk show 93 11.77 television show 24 11.30 TV show 25 10.45 show 255 9.98 audience 23 7.80 joke 5 7.37 V:subj:N joke 39 7.11 tape 10 7.09 poke 15 6.87 host 40 6.47 co-host 4 6.14 banter 3 6.00 interview 20 5.89 N:appo:N host 127 12.46 comedian 12 11.02 King 13 9.49 star 6 7.47 Figure 1.
Excerpt of the grammatical signature for the television host class.
773 keeping track of the edit operations (which is the second part of the algorithm).
Algorithm for calculating the minimal edit distance between two strings D[0,0]=0 for i = 1 to n do D[i,0] = D[i-1,0] + cost(insertion) for j = 1 to m do D[0,j] = D[0,j-1] + cost(deletion) for i = 1 to n do for j = 1 to m do D[i,j] = min( D[i-1,j-1] + cost(substitution), D[i-1,j] + cost(insertion), D[i,j-1] + cost(deletion)) Print (D[n,m]) Algorithm for optimal pattern retrieval i = n, j = m; while i ? 0 and j ? 0 if D[i,j] = D[i-1,j] + cost(insertion) print (*s*), i = i-1 else if D[i,j] = D[i,j-1] + cost(deletion) print(*s*), j = j-1 else if a1i = b1j print (a1i), i = i -1, j = j =1 else if a2i = b2j print (a2i), i = i -1, j = j =1 else print (*g*), i = i -1, j = j =1 We experimentally set (by trial and error): cost(insertion) = 3 cost(deletion) = 3 cost(substitution) = 0 if a1i=b1j = 1 if a1i?b1j, a2i=b2j = 2 if a1i?b1j, a2i?b2j 4.2 Implementation and filtering.
The above algorithm takes O(y2) time for every pair of strings of length at most y. Hence, if there are x strings in the collection, each string having at most length y, the algorithm has time complexity O(x2y2) to extract all the patterns in the collection.
Applying the above algorithm on a corpus of 3GB with 50 is-a relationship seeds, we obtain a set of 600 lexico-POS.
Following are two of them: 1) X_JJ#NN|JJ#NN#NN|NN _CC Y_JJ#JJ#NN|JJ |NNS|NN|JJ#NNS|NN#NN|JJ#NN|JJ#NN#NN e.g. ?caldera or lava lake?
2) X_NNP#NNP|NNP#NNP#NNP#NNP#NNP#CC#NNP |NNP|VBN|NN#NN|VBG#NN|NN ,_, _DT Y_NN#IN#NN|JJ#JJ#NN|JJ|NN|NN#IN#NNP |NNP#NNP|NN#NN|JJ#NN|JJ#NN#NN e.g. ?leukemia, the cancer of ...
Note that we store different POS variations of the anchors X and Y. As shown in example 1, the POS variations of the anchor X are (JJ NN, JJ NN NN, NN).
The variations for anchor Y are (JJ JJ NN, JJ, etc.).
The reason is quite straightforward: we need to determine the boundary of the anchors X and Y and a reasonable way to delimit them would be to use POS information.
All the patterns produced by the multi-level pattern learning algo rithm were generated from positive examples.
From amongst these patterns, we need to find the most important ones.
This is a critical step because frequently occurring patterns have low precision whereas rarely occurring patterns have high preci sion.
From the Information Extraction point of view neither of these patterns is very useful.
We need to find patterns with relatively high occurrence and high precision.
We apply the log likeli hood principle (Dunning 1993) to compute this score.
The top 15 patterns according to this metric are listed in Table 3 (we omit the POS variations for visibility).
Some of these patterns are similar to the ones discovered by Hearst (1992) while other patterns are similar to the ones used by Fleischman et al (2003).
4.3 Time complexity.
To extract hyponym relations, we use a fixed number of patterns across a corpus.
Since we treat each sentences independently from others, the algorithm runs in linear time O(n) over the corpus size, where n is number of sentences in the corpus.
In this section, we empirically compare the pattern-based and co-occurrence-based models pre sented in Section 3 and Section 4.
The focus is on the precision and recall of the systems as a func tion of the corpus size.
5.1 Experimental Setup.
We use a 15GB newspaper corpus consisting of TREC9, TREC 2002, Yahoo!
News ~0.5GB, AP newswire ~2GB, New York Times ~2GB, Reuters ~0.8GB, Wall Street Journal ~1.2GB, and various online news website ~1.5GB.
For our experiments, we extract from this corpus six data sets of differ ent sizes: 1.5MB, 15 MB, 150 MB, 1.5GB, 6GB and 15GB.
For the co-occurrence model, we used Minipar (Lin 1994), a broad coverage parser, to parse each data set.
We collected the frequency counts of the grammatical relationships (contexts) output by Minipar and used them to compute the pointwise mutual information vectors described in Section 3.1.
For the pattern-based approach, we use Brill?s. POS tagger (1995) to tag each data set.
5.2 Precision.
We performed a manual evaluation to estimate the precision of both systems on each dataset.
For each dataset, both systems extracted a set of is-a Table 3.
Top 15 lexico-syntactic patterns discovered by our system.
X, or Y X, _DT Y _(WDT|IN) Y like X and X, (a|an) Y X, _RB known as Y _NN, X and other Y X, Y X ( Y ) Y, including X, Y, or X Y such as X Y, such as X X is a Y X, _RB called Y Y, especially X 774relationships.
Six sets were extracted for the pattern-based approach and five sets for the co occurrence approach (the 15GB corpus was too large to process using the co-occurrence model ? see dependency parsing time estimates in Table 2).
From each resulting set, we then randomly se lected 50 words along with their top 3 highest ranking is-a relationships.
For example, Table 4 shows three randomly selected names for the pat tern-based system on the 15GB dataset.
For each word, we added to the list of hypernyms a human generated hypernym (obtained from an annotator looking at the word without any system or Word Net hyponym).
We also appended the WordNet hypernyms for each word (only for the top 3 senses).
Each of the 11 random samples contained a maximum of 350 is-a relationships to manually evaluate (50 random words with top 3 system, top 3 WordNet, and human generated relationship)..
We presented each of the 11 random samples to two human judges.
The 50 randomly selected words, together with the system, human, and WordNet generated is-a relationships, were ran domly ordered.
That way, there was no way for a judge to know the source of a relationship nor each system?s ranking of the relationships.
For each relationship, we asked the judges to assign a score of correct, partially correct, or incorrect.
We then computed the average precision of the system, human, and WordNet on each dataset.
We also computed the percentage of times a correct rela tionship was found in the top 3 is-a relationships of a word and the mean reciprocal rank (MRR).
For each word, a system receives an MRR score of 1 / M, where M is the rank of the first name judged correct.
Table 5 shows the results comparing the two automatic systems.
Table 6 shows similar results for a more lenient evaluation where both correct and partially correct are judged correct.
For small datasets (below 150MB), the pattern based method achieves higher precision since the co-occurrence method requires a certain critical mass of statistics before it can extract useful class signatures (see Section 3).
On the other hand, the pattern-based approach has relatively constant precision since most of the is-a relationships se lected by it are fired by a single pattern.
Once the co-occurrence system reaches its critical mass (at 150MB), it generates much more precise hypo nyms.
The Kappa statistics for our experiments were all in the range 0.78 ? 0.85.
Table 7 and Table 8 compare the precision of the pattern-based and co-occurrence-based methods with the human and WordNet hyponyms.
The variation between the human and WordNet scores across both systems is mostly due to the relative cleanliness of the tokens in the co-occurrencebased system (due to the parser used in the ap proach).
WordNet consistently generated higher precision relationships although both algorithms approach WordNet quality on 6GB (the pattern based algorithm even surpasses WordNet precision on 15GB).
Furthermore, WordNet only generated a hyponym 40% of the time.
This is mostly due to the lack of proper noun coverage in WordNet.
On the 6 GB corpus, the co-occurrence approach took approximately 47 single Pentium-4 2.5 GHz processor days to complete, whereas it took the pattern-based approach only four days to complete on 6 GB and 10 days on 15 GB.
5.3 Recall.
The co-occurrence model has higher precision than the pattern-based algorithm on most datasets.
Table 4.
Is-a relationships assigned to three randomly selected words (using pattern-based system on 15GB dataset).
RANDOM WORD HUMAN WORDNET PATTERN-BASED SYSTEM (RANKED) Sanwa Bank bank none subsidiary / lender / bank MCI Worldcom Inc. telecommunications company none phone company / competitor / company cappuccino beverage none item / food / beverage Table 5.
Average precision, top-3 precision, and MRR for both systems on each dataset.
PATTERN SYSTEM CO-OCCURRENCE SYSTEM Prec Top-3 MRR Prec Top-3 MRR 1.5MB 38.7% 41.0% 41.0% 4.3% 8.0% 7.3% 15MB 39.1% 43.0% 41.5% 14.6% 32.0% 24.3% 150MB 40.6% 46.0% 45.5% 51.1% 73.0% 67.0% 1.5GB 40.4% 39.0% 39.0% 56.7% 88.0% 77.7% 6GB 46.3% 52.0% 49.7% 64.9% 90.0% 78.8% 15GB 55.9% 54.0% 52.0% Too large to process Table 6.
Lenient average precision, top-3 precision, and MRR for both systems on each dataset.
PATTERN SYSTEM CO-OCCURRENCE SYSTEM Prec Top-3 MRR Prec Top-3 MRR 1.5MB 56.6% 60.0% 60.0% 12.4% 20.0% 15.2% 15MB 57.3% 63.0% 61.0% 23.2% 50.0% 37.3% 150MB 50.7% 56.0% 55.0% 60.6% 78.0% 73.2% 1.5GB 52.6% 51.0% 51.0% 69.7% 93.0% 85.8% 6GB 61.8% 69.0% 67.5% 78.7% 92.0% 86.2% 15GB 67.8% 67.0% 65.0% Too large to process 775 However, Figure 2 shows that the pattern-based approach extracts many more relationships.
Semantic extraction tasks are notoriously diffi cult to evaluate for recall.
To approximate recall, we defined a relative recall measure and conducted a question answering (QA) task of answering defi nition questions.
5.3.1 Relative recall Although it is impossible to know the number of is-a relationships in any non-trivial corpus, it is possible to compute the recall of a system relative to another system?s recall.
The recall of a system A, RA, is given by the following formula: C C R AA = where CA is the number of correct is-a relation ships extracted by A and C is the total number of correct is-a relationships in the corpus.
We define relative recall of system A given system B, RA,B, as: B A B A BA C C R R R == ,Using the precision estimates, PA, from the pre vious section, we can estimate CA ? PA ? |A|, where A is the total number of is-a relationships discov ered by system A. Hence, BP AP R B A BA ? ?
= ,Figure 3 shows the relative recall of A = pattern based approach relative to B = co-occurrence model.
Because of sparse data, the pattern-based approach has much higher precision and recall (six times) than the co-occurrence approach on the small 15MB dataset.
In fact, only on the 150MB dataset did the co-occurrence system have higher recall.
With datasets larger than 150MB, the co occurrence algorithm reduces its running time by filtering out grammatical relationships for words that occurred fewer than k = 40 times and hence recall is affected (in contrast, the pattern-based approach may generate a hyponym for a word that it only sees once).
5.3.2 Definition questions Following Fleischman et al (2003), we select the 50 definition questions from the TREC2003 (Voorhees 2003) question set.
These questions are of the form ?Who is X??
and ?What is X??
For each question (e.g., ?Who is Niels Bohr??, ?What is feng shui??)
we extract its respective instance (e.g., ?Neils Bohr?
and ?feng shui?), look up their corresponding hyponyms from our is-a table, and present the corresponding hyponym as the answer.
We compare the results of both our systems with WordNet.
We extract at most the top 5 hyponyms provided by each system.
We manually evaluate the three systems and assign 3 classes ?Correct (C)?, ?Partially Correct (P)?
or ?Incorrect (I)?
to each answer.
This evaluation is different from the evaluation performed by the TREC organizers for definition questions.
However, by being consistent across all Total Number of Is-A Relationships vs. Dataset 0 200000 400000 600000 800000 1000000 1200000 1400000 1.5MB 15MB 150MB 1.5GB 6GB 15GB Datasets To ta l Is A Re la tio n s hi ps s Pattern-based System Co-occurrence-based System Figure 2.
Number of is-a relationships extracted by the pattern-based and co-occurrence-based approaches.
Table 7.
Average precision of the pattern-based sys tem vs. WordNet and human hyponyms.
PRECISION MRR Pat.
WNet Human Pat.
WNet Human 1.5MB 38.7% 45.8% 83.0% 41.0% 84.4% 83.0% 15MB 39.1% 52.4% 81.0% 41.5% 95.0% 91.0% 150MB 40.6% 49.4% 84.0% 45.5% 88.9% 94.0% 1.5GB 40.4% 43.4% 79.0% 39.0% 93.3% 89.0% 6GB 46.3% 46.5% 76.0% 49.7% 75.0% 76.0% 15GB 55.9% 45.6% 79.0% 52.0% 78.0% 79.0% Table 8.
Average precision of the co-occurrence based system vs. WordNet and human hyponyms.
PRECISION MRR Co-occ WNet Human Co-occ WNet Human 1.5MB 4.3% 42.7% 52.7% 7.3% 87.7% 95.0% 15MB 14.6% 38.1% 48.7% 24.3% 86.6% 95.0% 150MB 51.1% 57.5% 65.8% 67.0% 85.1% 98.0% 1.5GB 56.7% 62.8% 70.3% 77.7% 93.0% 98.0% 6GB 64.9% 68.9% 75.2% 78.8% 94.3% 98.0% Relative Recall (Pattern-based vs. Co-occurrence-based) 0.00 1.00 2.00 3.00 4.00 5.00 6.00 7.00 1.5MB 15MB 150MB 1.5GB 6GB 15GB (projected) Datesets Re la tiv e Re ca ll Figure 3.
Relative recall of the pattern-based approach relative to the co-occurrence approach.
776 systems during the process, these evaluations give an indication of the recall of the knowledge base.
We measure the performance on the top 1 and the top 5 answers returned by each system.
Table 9 and Table 10 show the results.
The corresponding scores for WordNet are 38% accuracy in both the top-1 and top-5 categories (for both strict and lenient).
As seen in this experiment, the results for both the pattern-based and cooccurrence-based systems report very poor per formance for data sets up to 150 MB.
However, there is an increase in performance for both systems on the 1.5 GB and larger datasets.
The per formance of the system in the top 5 category is much better than that of WordNet (38%).
There is promise for increasing our system accuracy by re ranking the outputs of the top-5 hypernyms.
