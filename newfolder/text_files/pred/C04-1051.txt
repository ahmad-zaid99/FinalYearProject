 While the idea of exploiting multiple news reports for paraphrase acquisition is not new, previous efforts (for example, Shinyama et al 2002; Barzilay and Lee 2003) have been restricted to at most two news sources. Pairings that were identical or differing only by punctuation were rejected, as were those where the shorter sentence in the pair was less than two thirds the length of the longer, this latter constraint in effect placing an upper bound on edit distance relative to the length of the sentence. 3.1.1 First sentences The second extraction technique was specifically intended to capture paraphrases which might contain very different sets of content words, word order, and so on. In order to automatically identify sentence pairs of this type, we have attempted to take advantage of some of the unique characteristics of the dataset. Even so, any arbitrary pair of sentences from different articles within a cluster is unlikely to exhibit a paraphrase relationship: The Phi-X174 genome is short and compact. To isolate just those sentence pairs that represent likely paraphrases without requiring significant string similarity, we exploited a common journalistic convention: the first sentence or two of 3A maximum Levenshtein distance of 12 was selected for the purposes of this paper on the basis of experiments with corpora extracted at various edit distances. One might reasonably expect, therefore, that initial sentences from one article in a cluster will be paraphrases of the initial sentences in other articles in that cluster. This heuristic turns out to be a powerful one, often correctly associating sentences that are very different at the string level: In only 14 days, US researchers have created an artificial bacteria-eating virus from synthetic genes. Also consider the following example, in which related words are obscured by different parts of speech: Chosun Ilbo, one of South Korea's leading newspapers, said North Korea had finished developing a new ballistic missile last year and was planning to deploy it. The Chosun Ilbo said development of the new missile, with a range of up to %%number%% kilometres (%%number%% miles), had been completed and deployment was imminent. A corpus was produced by extracting the first two sentences of each article, then pairing these across documents within each cluster. The combination of the first-two sentences heuristic plus topical article clusters allows us to take advantage of meta-information implicit in our corpus, since clustering exploits lexical information from the entire document, not just the few sentences that are our focus. To prevent too high an incidence of unrelated sentences, one string-based heuristic filter was found useful: a pair is discarded if the sentences do not share at least 3 words of 4+ characters. This constraint succeeds in filtering out many unrelated pairs, although it can sometimes be too restrictive, excluding completely legitimate paraphrases: There was no chance it would endanger our planet, astronomers said. Given the relatively long sentences in our corpus (average length 18.6 words), these filters allowed us to maintain a degree of semantic relatedness between sentences. Accordingly, the dataset encompasses many paraphrases that would have been excluded under a more stringent edit-distance threshold, for example, the following non-paraphrase pair that contain an element of paraphrase: A staggering %%number%% million Americans have been victims of identity theft in the last five years , according to federal trade commission survey out this week. Nevertheless, even after filtering in these ways, a significant amount of unfiltered noise remains in the F2 corpus, which consisted of 214K sentence pairs. Out of a sample of 448 held-out sentence pairs, 118 (26.3%) were rated by two independent human evaluators as sentence-level paraphrases, while 151 (33.7%) were rated as partial paraphrases. The remaining ~40% were assessed as News article clusters: URLs Download URLs, Isolate content (HMM), Sentence separate Textual content of articles Select and filter first sentence pairs Approximately parallel monolingual corpus Figure 1. 4 Thus, although the F2 data set is nominally larger than the L12 data set, when the noise factor is taken into account, the actual number of full paraphrase sentences in this data set is estimated to be in the region of 56K sentences, with a further estimated 72K sentences containing some paraphrase material that might be a potential source of alignment. The following pair, for example, would be unlikely to pass muster on edit distance grounds, but nonetheless contains an inversion of deep semantic roles, employing different lexical items. The F2 data also retains pairs like the following that involve both high-level semantic alternations and long distance dependencies: Two men who robbed a jeweller's shop to raise funds for the Bali bombings were each jailed for %%number%% years by Indonesian courts today. In order to address such questions, we used word Alignment Error Rate (AER), a metric borrowed from the field of statistical machine translation (Och & Ney 2003). AER measures how accurately an automatic algorithm can align words in corpus of parallel sentence pairs, with a human 4 This contrasts with 16.7% pairs assessed as unrelated in a 10,000 pair sampling of the L12 data. Paraphrase data is of course monolingual, but otherwise the task is very similar to the MT alignment problem, posing the same issues with one-to-many, many-to-many, and one/many-to null word mappings. We closely followed the evaluation standards established in Melamed (2001) and Och & Ney (2000, 2003). Following Och & Ney?s methodology, two annotators each created an initial annotation for each dataset, subcategorizing alignments as either SURE (necessary) or POSSIBLE (allowed, but not required). To compute Precision, Recall, and Alignment Error Rate (AER) for the twin datasets, we used exactly the formulae listed in Och & Ney (2003). After an initial training pass and refinement of the linking specification, interrater agreement measured in terms of AER5 was 93.1% for the edit distance test set versus 83.7% for the F2 test set, suggestive of the greater variability in the latter data set. Giza++ is a freely available implementation of IBM Models 1-5 (Brown et al 1993) and the HMM alignment (Vogel et al 1996), along with various improvements and modifications motivated by experimentation by Och & Ney (2000). While these models have proven effective at the word alignment task (Mihalcea & Pedersen 2003), there are significant practical limitations in their output. To mitigate this limitation on final mappings, we follow the approach of Och (2000): we align once in the forward direction and again in the backward direction. These alignments can subsequently be recombined in a variety of ways, 5 The formula for AER given here and in Och & Ney (2003) is intended to compare an automatic alignment against a gold standard alignment. Because the AER is asymmetric (though each direction differs by less than 5%), we have presented the average of the directional AERs. The best overall performance, irrespective of test data type, is achieved by the L12 training set, with an 11.58% overall AER on the 250 sentence pair edit distance test set (20.88% AER for non-identical words). The F2 training data is probably too sparse and, with 40% unrelated sentence pairs, too noisy to achieve equally good results; nevertheless the gap between the results for the two training data types is dramatically narrower on the F2 test data. The nearly comparable numbers for the two training data sets, at 13.2% and 14.7% respectively, suggest that the L12 training corpus provides no substantive advantage over the F2 data when tested on the more complex test data. To explore some of the differences between the training sets, we hand-examined a random sample of sentence pairs from each corpus type. Elaboration: Sentence pairs can differ in total information content, with an added word, phrase or clause in one sentence that has no Training Data Type: L12 F2 L12 F2 Test Data Type: 250 Edit Dist 250 Edit Dist 116 F2 Heuristic 116 F2 Heuristic Precision 87.46% 86.44% 85.07% 84.16% Recall 89.52% 82.64% 88.70% 86.55% AER 11.58% 15.41% 13.24% 14.71% Identical word precision 89.36% 88.79% 92.92% 93.41% Identical word recall 89.50% 83.10% 93.49% 92.47% Identical word AER 10.57% 14.14% 6.80% 7.06% Non-Identical word precision 76.99% 71.86% 60.54% 53.69% Non-Identical word recall 90.22% 69.57% 59.50% 50.41% Non-Identical word AER 20.88% 28.57% 39.81% 47.46% Table 1. Precision, recall, and alignment error rates (AER) for F2 and L12 counterpart in the other (e.g. Some are non-compositional idioms (has pulled the plug on / is dropping plans for); others involve different phrasing (electronically / in electronic form, more than a million people / a massive crowd). Cases of NP anaphora (ISS / the Atlanta-based security company) are also common in the data, but in quantifying paraphrase types we restricted our attention to the simpler case of pronominal anaphora. Reordering: Words, phrases, or entire constituents occur in different order in two related sentences, either because of major syntactic differences (e.g. These categories do not cover all possible alternations between pairs of paraphrased sentences; moreover, categories often overlap in the same sequence of words. phrase), and one example of Elaboration (terror attacks occurs in only one sentence). To quantify the differences between L12 and F2, we randomly chose 100 sentence pairs from each dataset and counted the number of times each phenomenon was encountered. A given sentence pair might exhibit multiple instances of a single phenomenon, such as two phrasal paraphrase changes or two synonym replacements. Lower-frequency changes that fell outside of the above categories were not tallied: for example, the presence or absence of a definite article (had authority / had the authority) in Figure 2 was ignored. After summing all alternations in each sentence pair, we calculated the average number of occurrences of each paraphrase type in each data set. First, the F2 data is less parallel, as evidenced by the higher percentage of Elaborations found in those sentence pairs. Thus while string difference methods may produce relatively clean training data, this is achieved at the cost of filtering out common (and interesting) paraphrase relationships.