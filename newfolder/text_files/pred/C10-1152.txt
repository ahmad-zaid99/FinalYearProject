 We processed the dataset as follows: Article Pairing 65,133 articles from SimpleWikipedia3 and Wikipedia4 were paired by fol lowing the ?language link? Plain Text Extraction We use JWPL (Zesch etal., 2008) to extract plain texts from Wikipedia ar ticles by removing specific Wiki tags. Pre-processing including sentence boundary detection and tokenization with the Stanford 1http://simple.wikipedia.org 2http://en.wikipedia.org 3As of Aug 17th, 2009 4As of Aug 22nd, 2009 5http://download.wikimedia.org Parser package (Klein and Manning, 2003), and lemmatization with the TreeTagger (Schmid, 1994). In order to achieve the best sentence alignment on our dataset, we tested three similarity measures: (i) sentence-level TF*IDF (Nelken and Shieber, 2006), (ii) word overlap (Barzilay and Elhadad, 2003) and (iii)word-based maximum edit distance (MED) (Lev enshtein, 1966) with costs of insertion, deletionand substitution set to 1. We manually adjusted the similarity threshold to obtain a recallvalue as close as possible to 55.8% which was pre viously adopted by Nelken and Shieber (2006). 1 show that sentence-levelTF*IDF clearly outperforms the other two mea sures, which is consistent with the results reported by Nelken and Shieber (2006). Len #Sen.Pairscomplex simple complex simple 25.01 20.87 5.06 4.89 108,016 Table 2: Statistics for the PWKP datasetIn order to account for sentence splitting, we al low 1 to n sentence alignment to map one complexsentence to several simple sentences. We apply the following simplification operations to the parse tree of a complex sentence: splitting, 1354dropping, reordering and substitution. 1 shows the parse tree of c (we skip the POS level).c: August was the sixth month in the ancient Ro man calendar which started in 735BC. NP VP S August was NPinsixththe SBAR NP NP PP WHNP S VP started PP in 735BC ancient calendar whichthe Roman month Figure 1: Parse Tree of c 3.1 Splitting. The first operation is sentence splitting, which wefurther decompose into two subtasks: (i) segmen tation, which decides where and whether to split a sentence and (ii) completion, which makes the new split sentences complete. In our model, the splitting point is judged by the syntactic constituent of the split boundary word in the complex sentence. SBAR 2 false 0.9165 Table 3: Segmentation Feature Table (SFT) Actually, we do not use the direct constituent of a word in the parse tree. In stead, we use iLength as the feature which is calculated as iLength = ceiling( comLengthavgSimLength), where comLength is the length of the complex sentence and avgSimLength is the average length of simple sentences in the training dataset. Algorithm 1 adjustConstituent(word, tree) constituent? father.father; end while return constituent; In our model, one complex sentence can be split into two or more sentences. The probability of a segmentation op eration is calculated as: P (seg|c) = ? w:c SFT (w|c) (1) where w is a word in the complex sentence c and SFT (w|c) is the probability of the word w in the Segmentation Feature Table (SFT); Fig. NP VP S August was NPinsixththe SBAR NP NP PP WHNP S VP started PP in 735BC ancient calendar which the Roman month Figure 2: Segmentation The second step is completion. to the left of ?started?to obtain the complete sentence ?the ancient Ro man calendar started in 735BC?. In our model, whether the border word should be dropped or retained depends on two features of the border word: the direct constituent of the word and the word itself, as shown in Tab. WHNP which True 1.0 WHNP which False Prob.Min Table 4: Border Drop Feature Table (BDFT) In order to copy the necessary parts to complete the new sentences, we must decide which parts should be copied and where to put these parts in the new sentences. in the secondsplit sentence is ?gov nsubj?.6 The direct constituent of ?started? gov nsubj VP(VBD) True left 0.9000 gov nsubj VP(VBD) True right 0.0994 gov nsubj VP(VBD) False - 0.0006 Table 5: Copy Feature Table (CFT) For dependent NPs, we copy the whole NP phrase rather than only the head noun.7 In ourexample, we copy the whole NP phrase ?the an cient Roman calendar? The probability of a completion operation can be calculated as P (com|seg) = Y bw:s BDFT (bw|s) Y w:s Y dep:w CFT (dep). where s are the split sentences, bw is a border word in s, w is a word in s, dep is a dependency of w which is out of the scope of s. Fig. NP VP pt1 August was NPinsixththe NP NP PPpt2 VP started PP in 735BC ancient calendarthe RomanNP ancient calendarthe Roman month Figure 3: Completion 3.2 Dropping and Reordering. We use the same features for both drop ping and reordering: the node?s direct constituent and its children?s constituents pattern, see Tab. NP DT JJ NNP NN 1101 7.66E-4 NP DT JJ NNP NN 0001 1.26E-7 Table 6: Dropping Feature Table (DFT) 6With Stanford Parser, ?which? NP DT JJ NN 012 0.8303 NP DT JJ NN 210 0.0039 Table 7: Reordering Feature Table (RFT)The bits ?1? P (dp|node) = DFT (node) (2) P (ro|node) = RFT (node) (3) In our example, one of the possible results is dropping the NNP ?Roman?, as shown in Fig. NP VP pt1 August was NPinsixththe NP NP PPpt2 VP started PP in 735BC ancient calendartheNP ancient calendarthe month Figure 4: Dropping & Reordering 3.3 Substitution. The probability of a word substitu tion operation can be calculated as P (sub|w) = SubFT (Substitution|Origin). ancient ancient 0.963 ancient old 0.0183 ancient than transport 1.83E-102 old ancient 0.005 Table 8: Substitution Feature Table (SubFT) 3.3.2 Phrase SubstitutionPhrase substitution happens on the non terminal nodes and uses the same conditioningfeatures as word substitution. When we apply phrase substitution on anon-terminal node, then any simplification operation (including dropping, reordering and substitu tion) cannot happen on its descendants any more 1356 because when a node has been replaced then its descendants are no longer existing. We perform substitution for a non terminal node if the following constraint is met: Max(SubFT (?|node)) ? Y ch:node Max(SubFT (?|ch)). The proba bility of the phrase substitution is calculated as P (sub|node) = SubFT (Substitution|Origin).Fig. NP VP pt1 August was NPinsixththe NP NP PPpt2 VP started PP in 735BC old calendartheNP old calendarthe month Figure 5: Substitution As a result of all the simplification operations, we obtain the following two sentences: s1 = Str(pt1)=?August was the sixth month in the old calendar.? and s2 = Str(pt2)=?The old calendar started in 735BC.? Our model can be formalized as a direct translation model from complex to simple P (s|c) multi plied by a language model P (s) as shown in Equ. 4.
s = argmax s P (s|c)P (s) (4) We combine the parts described in the previous sections to get the direct translation model: P (s|c) = ? Following the work of Yamada and Knight (2001), we train our model by maximizing P (s|c) over the training corpuswith the EM algorithm described in Alg. P (s|c) is equal to the inside probability of the root in theTraining Tree. Algorithm 2 EM Training (dataset)Initialize all probability tables using the uniform distribu tion; for several iterations do reset al cnt = 0; for each sentence pair < c, s > in dataset do tt = buildTrainingTree(< c, s >); calcInsideProb(tt); calcOutsideProb(tt); update cnt for each conditioning feature in each node of tt: cnt = cnt + node.insideProb ? node.outsideProb/root.insideProb; end for updateProbability(); end for root sp sp_res1 sp_res2 dp ro mp mp_res1 mp_res2 sub mp mp_res subsub dp ro mp_res root sp sp_res sp_res dp ro ro_res ro_res sub ro_res subsub dp ro ro_res sub_res sub_res sub_res Figure 6: Training Tree (Left) and Decoding Tree (Right) We illustrate the construction of the training tree with our running example. In our example, sp splits the parse tree into two parse trees pt1 and pt2 (Fig. Algorithm 3 calcInsideProb (TrainingTree tt) for each node from level = N to root of tt do if node is a sub node then node.insideProb = P (sub|node); else if node is a mp OR sp node then node.insideProb =Qchild child.insideProb;else node.insideProb =Pchild child.insideProb;end if end for Algorithm 4 calcOutsideProb (TrainingTree tt) for each node from root to level = N of tt do if node is the root then node.outsideProb = 1.0; else if node is a sp res OR mp res node then {COMMENT: father are the fathers of the current node, sibling are the children of father excluding the current node} node.outsideProb = P father father.outsideProb ?Qsibling sibling.insideProb;else if node is a mp node then node.outsideProb = father.outsideProb ? 1.0; else if node is a sp, ro, dp or sub node then node.outsideProb = father.outsideProb ? P (sp or ro or dp or sub|node); end if end for August was the sixth in the ancient Roman calendar statedwhich in 735BC August was the sixth in the old Roman calendar stated in 735BCThe old calendar. Complex sentence Simple sentences month month Figure 7: Monolingual Word Mapping
For decoding, we construct the decoding tree(Fig. We train the language model with SRILM (Stolcke, 2002). The secondbaseline system is a sentence compression sys tem (Filippova and Strube, 2008a) whose demo system is available online.8 As the compressionsystem can only perform dropping, we further ex tend it to our third and fourth baseline systems, in order to make a reasonable comparison. and ?that?, and dis carding the border words. In total, there are 5systems in our evaluation: Moses, the MT system; C, the compression system; CS, the compression+substitution system; CSS, the compres sion+substitution+split system; TSM, our model.We also provide evaluation measures for the sen tences in the evaluation dataset: CW: complexsentences from Normal Wikipedia and SW: par allel simple sentences from Simple Wikipedia. TSM achieves an average token length which is the same as the Simple Wikipedia (SW). CW: ?Genetic engineering has ex panded the genes available to breeders to utilize in creating desired germlines for new crops.? SW: 8http://212.126.215.106/compression/?New plants were created with genetic engineer ing.? TSM: ?Engineering has expanded the genes available to breeders to use in making germlines for new crops.? CW: ?An umbrella term is a word thatprovides a superset or grouping of related con cepts, also called a hypernym.? SW: ?An umbrellaterm is a word that provides a superset or group ing of related concepts.? ?In the first example, both substitution and dropping happen. However, the original complex sentences (CW) from Normal Wikipedia get a rather high BLEU (0.50), when compared to the simple sentences. 9, 25 sentences generated by Moses are exactly identical to the com plex sentences, while the number for TSM is 2 which is closer to SW. As discussed in Jurafsky and Martin (2008), ?BLEU does poorly at comparing systems with radically different architectures and is most appropriate when evaluating incremental changes with similar architectures.? In our case, TSM andCSS can be considered as having similar architec tures as both of them can do splitting, dropping 9http://www.statmt.org/moses/ 1359 TokLen SenLen #Sen BLEU NIST #Same Flesch Lix(Grade) OOV% PPL CW 4.95 27.81 100 0.50 6.89 100 49.1 53.0 (10) 52.9 384 SW 4.76 17.86 131 1.00 10.98 3 60.4 (PE) 44.1 (8) 50.7 179 Moses 4.81 26.08 100 0.55 7.47 25 54.8 48.1 (9) 52.0 363 C 4.98 18.02 103 0.28 5.37 1 56.2 45.9 (8) 51.7 481 CS 4.90 18.11 103 0.19 4.51 0 59.1 45.1 (8) 49.5 616 CSS 4.98 10.20 182 0.18 4.42 0 65.5 (PE) 38.3 (6) 53.4 581 TSM 4.76 13.57 180 0.38 6.21 2 67.4 (PE) 36.7 (5) 50.8 353 Table 9: Evaluation and substitution. Here is an exam ple to illustrate this: (CW): ?Almost as soon as heleaves, Annius and the guard Publius arrive to es cort Vitellia to Titus, who has now chosen her as his empress.? (SW): ?Almost as soon as he leaves,Annius and the guard Publius arrive to take Vitellia to Titus, who has now chosen her as his empress.? (TSM): ?An nius and the guard Publius arrive to take Vitellia to Titus. ?In this example, Moses generates an exactly iden tical sentence to SW, thus the BLUE and NIST scores of Moses is the highest. TSM simplifies the complex sentence by dropping, splitting and substitution, which results in two sentences that are quite different from the SW sentence and thus gets lower BLUE and NIST scores. OOVis the percentage of words that are not in the Ba sic English BE850 list.10 TSM is ranked as the second best system for this criterion.The perplexity (PPL) is a score of text probability measured by a language model and normal ized by the number of words in the text (Equ. Language models constitute an important feature for assessing readabil ity (Schwarm and Ostendorf, 2005). PPL(text) = P (w1w2...wN )? For example, we copy the dependent NP to the new sentences. Sometimes, excessive droppings oc cur, e.g., ?older? This results in a problematic sentence: ?She has anbrother and a brother...?.