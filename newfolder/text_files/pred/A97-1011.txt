 Basically, the barrier can be used to limit the test only to the current clause (by using clause boundary markers and &quot; stop words &quot;) or to a constituent (by using &quot; stop categories &quot;) instead of the whole sentence. For example, the following rule discards the syntactic function' CI-OBJ (indirect object): The rule holds if the closest finite verb to the left is unambiguously (C) a finite verb (VFIN), and there is no ditransitive verb or participle (subcategorisation SV00) between the verb and the indirect object. there is no SVO0 in the same verb (LINK NOT 0 SV00), the CI-OBJ reading will be discarded. In essence, the same formalism is used in the syntactic analysis in Jarvinen (1994) and Anttila (1995). For example, labels for functional heads (such as CSUBJ, COBJ, CI-OBJ) mark the word which is a head of a noun phrase having that function in the clause, but the parent is not indicated. 3 Dependency grammars in a nutshell Our notation follows the classical model of dependency theory (Heringer, 1993) introduced by Lucien Tesniere (1959) and later advocated by Igor Mel'auk (1987). Hudson (1991), several heads are allowed. Projectivity (or adjacency4) was not an issue for Tesniere (1959, ch. 10), because he thought that the linear order of the words does not belong to the syntactic level of representation which comprises the structural order only. (Hays, 1964), have brought the strict projectivity (context-free) requirement into the dependency framework. This kind of restriction is present in many dependency-based parsing systems (McCord, 1990; Sleator and Ternperley, 1991; Eisner, 1996). In valency theory, usually, complements (obligatory) and adjuncts (optional) are distinguished. The default is a nominal type of complement, but there might also be additional information concerning the range of possible complements, e.g., the verb say may have an object (SVO), which may also be realised as a to-infinitive clause, WH-clause, that-clause or quote structure. Usually, both the dependent element and its head are implicitly (and ambiguously) present in the Constraint Grammar type of rule. For example, the subject label (OSOBJ) is chosen and marked as a dependent of the immediately following auxiliary (AUXMOD) in the following rule: SELECT (@SUBJ) IF (IC AUXMOD HEAD); To get the full benefit of the parser, it is also useful to name the valency slot in the rule. This has two effects: (1) the valency slot is unique, i.e. no more than one subject is linked to a finite verb', and (2) we can explicitly state in rules which kind of valency slots we expect to be filled. The rule thus is of the form:
IF (IC AUXMOD HEAD = subject); The rule above works well in an unambiguous context but there is still need to specify more tolerant rules for ambiguous contexts. The rule differs from the previous rule in that it leaves the other readings of the noun intact and only adds a (possible) subject dependency, while both the previous rules disambiguated the noun reading also. Therefore, when the syntactic function is not rashly disambiguated, the correct reading may survive even after illegitimate linking, as the global pruning (Section 5) later extracts dependency links that form consistent trees. For example, when a head of an object phrase (OM) is found and indexed to a verb, the noun phrase to the right (if any) is probably an object complement (CPCOMPL-0). The relation of the object complement and its head is such that the noun phrase to the left of the object complement is an object (CM) that has established a dependency relation (object) to the verb. Naturally, the dependency relations may also be followed downwards (DOWN). Moreover, there is no longer any need to remove these readings explicitly by rules, because the global pruning removes readings which have not obtained any &quot;extra evidence&quot;. We have the following hypotheses: (1) the dependency forest is quite sparse and a whole parse tree can not always be found; (2) pruning should favour large (sub)trees; (3) unlinked readings of a word can be removed when there is a linked reading present among the alternatives; (4) unambiguous subtrees are more likely to be correct than ambiguous ones; and (5) pruning need not force the words to be unambiguous. This has two advantages: very heuristic links would confuse the pruning mechanism, and words that would not otherwise have a head, may still get one. Some comments concerning the rules in the toy grammar (Figure 3) are in order: The rule states: the first noun phrase head label to the right is a subject (OSOBJ), link subj exists and is followed up to the finite verb (0+F) in a verb chain (v-ch), which is then followed up to the main verb. Then object or complement links are followed downwards (BOTTOM), to the last verbal reading (here decide). This kind of rule that starts from word A, follows links up to word B and then down to word C, introduces a non-projective dependency link if word B is between words A and C. Note that the conditions TOP and BOTTOM follow the chain of named link, if any, to the upper or lower end of a chain of a multiple (zero or more) links with the same name. The syntactic relationship between the verbs is established by a rule stating that the rightmost main verb is the (clause) object of a main verb to the left, which allows such objects. Finally, there is a single main verb, which is indexed to the root (<s>) (in position 00). All text samples were excerpted from three different genres in the Bank of English (Jarvinen, 1994) data: American National Public Radio (broadcast), British Books data (literature), and The Independent (newspaper). Because both systems leave some amount of the ambiguity pending, two figures are given: the success rate, which is the percentage of correct morphosyntactic labels present in the output, and the ambiguity rate, which is the percentage of words containing more than one label. The ENGCG results compare to those reported elsewhere (Jirvinen, 1994; Tapanainen and Jarvinen, 1994). The major improvement over ENGCG is the level of explicit dependency representation, which makes it possible to excerpt modifiers of certain elements, such as arguments of verbs. A similar measure is used in (Eisner, 1996) except that every word has a head, i.e. Dekang Lin (1996) has earlier used this kind of evaluation, where precision and recall were for subjects 87% and 78%, and for complements (including objects) 84% and 72 %, respectively.