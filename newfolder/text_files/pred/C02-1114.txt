 Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998). Each node represents a noun and two nodes have a link between them if they co-occur separated by the conjunctions and or or, and each link is weighted according to the number of times the co-occurrence is observed.Various cutoff functions were used to deter mine how many times a relationship must be observed to be counted as a link in the graph. This decision may have effectively boosted precision at the expense of recall, because the preferred links are to fairlycommon and (probably) more stable words. The graph used in the experiments described has 99,454 nodes (nouns) and 587,475 links. There were roughly 400,000 different types tagged as nouns in the corpus, so the graph model represents about one quarter of these nouns, including most of the more common ones. (Roark and Charniak, 1998, ?3) where the inclu sion of an out-of-category word which happens to co-occur with one of the category words can significantly distort the final list. to a set of nodes: Definition 1 Let A be a set of nodes and let N(A), the neighbours of A, be the nodes which are linked to any a ? N(A)\A, let the affinity between u and A be given by the ratio |N(u) ?N(A)| |N(u)| . occurs in the corpus, the novell node will not be addedto this list because it doesn?t have a link to or ange, banana or any of their neighbours except for apple. The ten classes of words chosen were crimes, places, tools, vehicles, musical instruments, clothes, diseases, body parts, academic subjects and foodstuffs. (The first 4 cat egories are also used by (Riloff and Shepherd, 1997) and (Roark and Charniak, 1998) and so were included for comparison.) Having chosen these classes, 20 words were retrieved using asingle seed-word chosen from the class in ques tion. The influence of semantic Proto type Theory (Rosch, 1988) is apparent in this process, a link we would like to investigate in more detail. suggested by psychologi cal experiments (Mervis and Rosch, 1981). The only exceptions we have made are the terms wynd and planetology (marked in bold), whichare not in WordNet but are correct nonethe less. These conditions are at least as stringent as those of previous experiments, particularly those of Riloff and Shepherd (1997) who also give credit for words associated with but not belonging to a particular category. (It has been pointed out that many polysemous words may occur in several classes, making the task easier because for many words there are several classes which our algorithm would give credit for. )With these conditions, our algorithm re trieves only 36 incorrect terms out of a total of 200, giving an accuracy of 82%. Our results are an order of magnitude better than those reported by Riloff and Shepherd (1997) and Roark and Charniak (1998), whoreport average accuracies of 17% and 35% re spectively. The experiments in (Riloff and Shepherd, 1997) were performed on the 500,000 word MUC-4 corpus, and those of (Roark and Charniak, 1998) were performedusing MUC-4 and the Wall Street Journal cor pus (some 30 million words). Our model was built using the British National Corpus (100 million words). The high accuracy achieved thus questions the conclusion drawn by Roark and Charniak (1998) that ?parsing is invaluable?. This claim could of course be tested by comparing techniques on the same corpus.To evaluate the advantage of using PoS infor mation, we compared the graph model with asimilarity thesaurus generated using Latent Se mantic Indexing (Manning and Schu?tze, 1999, Ch 15), a ?bag-of-words? The LSI similarity thesaurus obtained an accuracy of 31%, much less than the graph model?s 82%. This is because LSI retrieves words which are related by context but are not in the same class: for example, the neighbours of piano found using LSI cosine-similarity on the BNC corpus include words such as composer,music, Bach, concerto and dance, which are re lated but certainly not in the same semantic class.The incremental clustering algorithm of Def inition (1) works well at preventing ?infections? Class Seed Word Neighbours Produced by Graph Model crimes murder crime theft arson importuning incest fraud larceny parricideburglary vandalism indecency violence offences abuse brig andage manslaughter pillage rape robbery assault lewdness places park path village lane viewfield church square road avenue garden castle wynd garage house chapel drive crescent home place cathedral street tools screwdriver chisel naville nail shoulder knife drill matchstick morgenthau gizmo hand knee elbow mallet penknife gallie leg arm sickle bolster hammer vehicle conveyance train tram car driver passengers coach lorry truck aeroplane coons plane trailer boat taxi pedestrians vans vehicles jeep bus buses helicopter musical instruments piano fortepiano orchestra marimba clarsach violin cizek viola oboeflute horn bassoon culbone mandolin clarinet equiluz contra bass saxophone guitar cello clothes shirt chapeaubras cardigan trousers breeches skirt jeans boots pair shoes blouse dress hat waistcoat jumper sweater coat cravat tie leggings diseases typhoid malaria aids polio cancer disease atelectasis illnesses cholerahiv deaths diphtheria infections hepatitis tuberculosis cirrho sis diptheria bronchitis pneumonia measles dysentery body parts stomach head hips thighs neck shoulders chest back eyes toes breasts knees feet face belly buttocks haws ankles waist legs academic subjectsphysics astrophysics philosophy humanities art religion science politics astronomy sociology chemistry history theology eco nomics literature maths anthropology culture mathematics geography planetology foodstuffs cake macaroons confectioneries cream rolls sandwiches croissant buns scones cheese biscuit drinks pastries tea danish butter lemonade bread chocolate coffee milk Table 1: Classes of similar words given by the graph model. In conclusion, it is clear that the graph modelcombined with the incremental clustering algo rithm of Definition 1 performs better than mostprevious methods at the task of automatic lex ical acquisition. We now take a step furtherand present a simple method for not only as sembling words with similar meanings, but for empirically recognising when a word has several meanings. The traditional Word Sense Disambiguation(WSD) problem addresses only the ambiguityresolution part of the problem: compiling a suit able list of polysemous words and their possiblesenses is a task for which humans are tradition ally needed (Kilgarriff and Rosenzweig, 2000).This makes traditional WSD an intensively supervised and costly process. Breadth of cover age does not in itself solve this problem: general lexical resources such as WordNet can provide too many senses many of which are rarely used in particular domains or corpora (Gale et al, 1992).The graph model presented in this paper suggests a new method for recognising relevant polysemy. We will need a small amount of termi nology from graph theory (Bolloba?s, 1998). Definition 2 (Bolloba?s, 1998, Ch 1 ?1) Let G = (V,E) be a graph, where V is the set of vertices (nodes) of G and E ? Definition 3 Let G be a graph of words closely related to a seed-word w, and let G \ w be the subgraph which results from the removal of the seed-node w. The connected components of the subgraph G \ w are the senses of the word w with respect to the graph G. As an illustrative example, consider the localgraph generated for the word apple (6). Definition 3 recognises the ambiguity of apple because this word is linked to both banana and novell, words which otherwise have nothing to do with one another. It is well-known that any graph can be thought of as a collection of feature-vectors, forexample by taking the row-vectors in the adja cency matrix (Bolloba?s, 1998, Ch 2 ?3). Thestandard method for this task is to use hand labelled data to train a learning algorithm, which will often pick out particular words as Bayesian classifiers which indicate one sense or the other. Clearly, the words in the different componentsin Diagram 6 can potentially be used as classi fiers for just this purpose, obviating the need fortime-consuming human annotation. This research was supported in part by theResearch Collaboration between the NTT Communication Science Laboratories, Nippon Tele graph and Telephone Corporation and CSLI,Stanford University, and by EC/NSF grant IST 1999-11438 for the MUCHMORE project.