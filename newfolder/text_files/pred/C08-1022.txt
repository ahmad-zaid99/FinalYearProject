 Determiners pose a somewhat different problem from prepositions as, unlike them, their choice is more dependent on the wider discourse contextthan on individual lexical items. The relation be tween a noun and a determiner is less strict than that between a verb or noun and a preposition, the main factor in determiner choice being the specific properties of the noun?s context. For example, wecan say boys like sport or the boys like sport, depending on whether we are making a general state ment about all boys or referring to a specific group.Equally, both she ate an apple and she ate the ap ple are grammatically well-formed sentences, butonly one may be appropriate in a given context, de pending on whether the apple has been mentioned previously. The approach proposed in this paper is based on the belief that although it is difficult to formulatehard and fast rules for correct preposition and determiner usage, there is enough underlying regularity of characteristic syntactic and semantic con texts to be able to predict usage to an acceptabledegree of accuracy. The classifier can therefore learn to associate a given preposition or determiner to particular contexts, and re liably predict a class when presented with a novel instance of a context for one or the other. Adj grade superlative POS ?3 VV, DT, JJS, IN, DT, NN Table 1: Determiner feature set for Pick the juiciest apple on the tree. yes, type = location POS ?3 NNP, VBD, NNP Grammatical relation iobj Table 2: Preposition feature set for John drove to London.Corpus (BNC) as we believe this offers a represen tative sample of different text types. Further determiner features note whether the nounis modified by a predeterminer, possessive, nu meral, and/or a relative clause, and whether it ispart of a ?there is. Additional preposi tion features refer to the grade of any adjectives or adverbs modified (base, comparative, superlative) and to whether the items modified are modified by more than one PP 1 . As for ourchoice of features, we aim to capture all the ele ments of a sentence which we believe to have an effect on preposition and determiner choice, and which can be easily extracted automatically - this is a key consideration as all the features derivedrely on automatic processing of the text. (Briscoe et al, 1 A full discussion of each feature, including motivation for its inclusion and an assessment of its contribution to the model, is found in De Felice (forthcoming). 170 Author Accuracy Baseline 26.94% Gamon et al 08 64.93% Chodorow et al 07 69.00% Our model 70.06% Table 3: Classifier performance on L1 prepositions 2006)). occurrence of a preposition or determiner in the corpus, we obtain a feature vector consisting ofthe preposition or determiner and its context, de scribed in terms of the features noted above. At the moment, we restrict our analysis to the nine most frequent prepositions in the data: at, by, for, from, in, of, on, to, and with, to ensure a sufficient amount of data for training. We use a standard maximum entropy classifier 4 and donot omit any features, although we plan to experiment with different feature combinations to deter mine if, and how, this would impact the classifier?s performance. Before testing our model on learner data, it is important to ascertain that it can correctlyassociate prepositions to a given context in gram matical, well-edited data. Our best result to date is 70.06% accuracy (test set size: 536,193). The baseline refers to always choosing the most frequent option, namely of.We can see that our model?s performance com pares favourably to the best results in the literature, although direct comparisons are hard to draw sincedifferent groups train and test on different preposi tion sets and on different types of data (British vs. American English, BNC vs. news reports, and so 2 No word sense disambiguation was performed at this stage. Proportion of training data Precision Recall of 27.83% (2,501,327) 74.28% 90.47% to 20.64% (1,855,304) 85.99% 81.73% in 17.68% (1,589,718) 60.15% 67.60% for 8.01% (720,369) 55.47% 43.78% on 6.54% (587,871) 58.52% 45.81% with 6.03% (541,696) 58.13% 46.33% at 4.72% (424,539) 57.44% 52.12% by 4.69% (421,430) 63.83% 56.51% from 3.86% (347,105) 59.20% 32.07% Table 4: L1 results - individual prepositions on). Furthermore, it should be noted that Gamon et al report more than one figure in their results, as there are two components to their model: one determining whether a preposition is needed, and the other deciding what the preposition should be. Additionally, Chodorow et al also discusssome modifications to their model which can in crease accuracy; the result noted here is the one more directly comparable to our own approach. 5.1.1 Further discussion To fully assess the model?s performance on the L1data, it is important to consider factors such as performance on individual prepositions, the relation ship between training dataset size and accuracy, and the kinds of errors made by the model.Table 4 shows the classifier?s performance on in dividual prepositions together with the size of their training datasets. At first glance, a clear correlationappears between the amount of data seen in training and precision and recall, as evidenced for ex ample by of or to, for which the classifier achievesa very high score. For example by has one of the smallest data sets in training but higher scores than many of the other prepositions, whilefor is notable for the opposite reason, namely hav ing a large dataset but some of the lowest scores. One simple way of verify ing the latter case is by looking at the number of senses assigned to the prepositions by a resource 171 Target prep Confused with at by for from in of on to with at xx 4.65% 10.82% 2.95% 36.83% 19.46% 9.17% 10.28% 5.85% by 6.54% xx 8.50% 2.58% 41.38% 19.44% 5.41% 10.04% 6.10% for 8.19% 3.93% xx 1.91% 25.67% 36.12% 5.60% 11.29% 7.28% from 6.19% 4.14% 6.72% xx 26.98% 26.74% 7.70% 16.45% 5.07% in 7.16% 9.28% 10.68% 3.01% xx 43.40% 10.92% 8.96% 6.59% of 3.95% 2.00% 18.81% 3.36% 40.21% xx 9.46% 14.77% 7.43% on 5.49% 3.85% 8.66% 2.29% 32.88% 27.92% xx 12.20% 6.71% to 9.77% 3.82% 11.49% 3.71% 24.86% 27.95% 9.43% xx 8.95% with 3.66% 4.43% 12.06% 2.24% 28.08% 26.63% 6.81% 16.10% xx Table 5: Confusion matrix for L1 data - prepositions such as the Oxford English Dictionary. However, we find no good correlation between the two as the preposition with the most senses is of (16), and that with the fewest is from (1), thus negating the idea that fewer senses make a preposition easierto learn. A good picture of the model?s errors can be had by looking at the confusion matrix in Table 5,which reports, for each preposition, what the clas sifier?s incorrect decision was. Analysis of these errors may establish whether they are related to thedataset size issue noted above, or have a more lin guistically grounded explanation.From the table, the frequency effect appears evi dent: in almost every case, the three most frequentwrong choices are the three most frequent prepo sitions, to, of, and in, although interestingly not inthat order, in usually being the first choice. Conversely, the less frequent prepositions are less of ten suggested as the classifier?s choice. We see for example that there seems to be a strong relation between of and for, the cause of which is not immediately clear: perhaps they both often occur within noun phrases(e.g. More pre dictable is the confusion between to and from, andbetween locative prepositions such as to and at, al though the effect is less strong for other potentially confusable pairs such as in and at or on. In most cases, the clas sifier?s suggestion is also grammatically correct, Classifier choice Correct phrase demands of the sector demands for. .Table 6: Examples of classifier errors on preposi tion L1 task Author Accuracy Baseline 59.83% Han et al 06 83.00% Gamon et al 08 86.07% Turner and Charniak 07 86.74% Our model 92.15% Table 7: Classifier performance - L1 determiners but the overall meaning of the phrases changes somewhat. Recall a 9.61% (388,476) 70.52% 53.50% the 29.19% (1,180,435) 85.17% 91.51% null 61.20% (2,475,014) 98.63% 98.79% Table 8: L1 results - individual determiners 5.2 Determiners. For the determiner task, we also consider only the three most frequent cases (a, the, null), which gives us a training dataset consisting of 4,043,925 instances. We achieve accuracy of 92.15% on theL1 data (test set size: 305,264), as shown in Table 7. As in the case of the prepositions, it is interesting to see whether this high performance is equally distributed across thethree classes; this information is reported in Ta ble 8. The indefinite arti cle?s lower ?learnability?, and its lower frequency appears not to be peculiar to our data, as it is also found by Gamon et al among others.The disparity in training is a reflection of the dis tribution of determiners in the English language. Perhaps if this imbalance were addressed, the model would more confidently learn contexts of use for a, too, which would be desirable in view of using this information for error correction. We plan to experiment with smaller scale, more similar datasets to ascertain whether the issue is one of training size or of inherent difficulty in learning about the indefinite article?s occurrence.In looking at the confusion matrix for determin ers (Table 9), it is interesting to note that for theclassifier?s mistakes involving a or the, the erroneous choice is in the almost always the other de terminer rather than the null case. This suggests that the frequency effect is not so strong as to over Target det Confused with a the null a xx 92.92% 7.08% the 80.66% xx 19.34% null 14.51% 85.49% xx Table 9: Confusion matrix for L1 determiners ride any true linguistic information the model has acquired, otherwise the predominant choice wouldalways be the null case. On the contrary, these results show that the model is indeed capable of distinguishing between contexts which require a determiner and those which do not, but requires fur ther fine tuning to perform better in knowing which of the two determiner options to choose. To evaluate the model?s performance on learner data, we use a subsection of the Cambridge Learner Corpus (CLC) 5 . For the preposition task, we extract 2523 instances of preposition use from the CLC (1282 correct, 1241 incorrect) and ask the classifier to mark them 5 The CLC is a computerised database of contemporary written learner English (currently over 25m words). 173 Instance type Accuracy Correct 66.7% Incorrect 70%Table 10: Accuracy on L2 data - prepositions. These first results sug gest that the model is fairly robust: the accuracy rate on the correct data, for example, is not much lower than that on the L1 data. In an application designed to assist learners, it is important to aim to reduce the rate of false alarms - cases where the original is correct, but the model flags an error - toa minimum, so it is positive that this result is com paratively high. However, ifwe look at the suggestions the model makes to re place the erroneous preposition, we find that theseare correct only 51.5% of the time, greatly reduc ing its usefulness. For ex ample, in the sentence I?m Franch, responsable on the computer services, the classifier is not able to suggest a correct alternative to the erroneous on:since it does not recognise the adjective as a misspelling of responsible, it loses the information associated with this lexical feature, which could po tentially determine the preposition choice. In this example, I wold like following equipment to my speech: computer, modem socket and microphone, the missing the leads the parser to treat following as a verb, and believes it to be the verb to which the preposition is attached. However, this was not what the PP was meant to modify: impaired performance from the parser could be a significant negative factor in the model?s performance. It would be interesting to test themodel on texts written by students of different lev els of proficiency, as their grammar may be more error-free and more likely to be parsed correctly. Alternatively, we could modify the parser so as to skip cases where it requires several attempts before producing a parse, as these more challenging casescould be indicative of very poorly structured sentences in which misused prepositions are depen dent on more complex errors.A different kind of problem impacting our accu racy scores derives from those instances where theclassifier selects a preposition which can be cor rect in the given context, but is not the correct one in that particular case. In the example I received a beautiful present at my birthday, the classifier identifies the presence of the error, and suggests the grammatically and pragmatically appropriate correction for. A better indication of the model?sperformance may be to independently judge its de cisions, to avoid being subject to the annotators?bias. Finally, we are beginning to look at the rela tions between preposition errors and other types oferror such as verb choice, and how these are anno tated in the data. This sug gests that the gap in performance between L1 and L2 is due more to the challenges posed by learner text than by inherent shortcomings in the model, and therefore that the key to better performance is likely to lie in overcoming these problems. We follow a similar procedure to the prepositions task, selecting a number of both correct and incorrect instances. 1200), however, accuracy is less than 10%. At the moment, themodel detects very few of these errors, no doubt in fluenced by the preponderance of null cases seen in training. In addition to those, though, in this task more than for prepositions we believe that differences intext type between the training texts - the BNC and the testing material - learner essays - has a sig nificant negative effect on the model. or ?postcards to penpals?.Also, the BNC was created with material from almost 20 years ago, and learners writing in contem porary English may use lexical items which are notvery frequently seen in the BNC. However, it is interesting to see whether human learners and classifiers display similar patterns of errors in preposition choice.This information has twofold value: as well as being of pedagogical assistance to instructors of En glish L2, were the classifier to display student-like error patterns, insights into ?error triggers? A clear one is the confusion between the three locative and temporal prepositions at, in, and on (typical sentence: The training programme will start at the 1st August). This type of error is made often by both learners and the model on both types of data, suggesting that perhaps further attentionto features might be necessary to improve discrim ination between these three prepositions.There are also interesting divergences. However, this confu sion is not very frequent in the model, a difference which could be explained either by the fact that, as noted above, performance on from is very low and so the classifier is unlikely to suggest it, or that in training the contexts seen for by are sufficiently distinctive that the classifier is not misled like the learners. However, in noting both divergences and similarities between the two learners, human and machine, we may be able to derive useful insights into the way the learning processes operate, and what factors could be more or less important for them.