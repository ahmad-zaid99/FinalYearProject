 For example, the set of attribute-value pairs { $cityfr = New York City, $city-to = Seattle , $time-dep = 6 a.m., $date-dep = Wednesday } represent the meaning of the noun phrase &quot;a flight to Seattle that departs from New York City at 6 a.m. on Wednesday&quot;. For example, the NLG system should automatically decide if the attribute ordering in &quot;flights to New York in the evening&quot; is better or worse than the ordering in &quot;flights in the evening to New York&quot;. Furthermore, it should automatically decide if the lexical choice in &quot;flights departing to New York&quot; is better or worse than the choice in &quot;flights leaving to New York&quot;. Figure 1 shows a sample of training data, where only words marked with a &quot;8&quot; are attributes. The systems NLG1, NLG2 and NLG3 all implement step 1; they produce a sequence of words intermixed with attributes, i.e., a template, from the the attributes alone. Specifically, nlgi(A) returns the phrase that corresponds to the attribute set A: [empty string] TA = where TA are the phrases that have occurred with A in the training data, and where C(phrase, A) is the training data frequency of the natural language phrase phrase and the set of attributes A. NLG1 will fail to generate anything if A is a novel combination of attributes. The probability model in NLG2 is a conditional distribution over V U * stop*, where V is the generation vocabulary and where *stop* is a special &quot;stop&quot; symbol. The form of the maximum entropy probability model is identical to the one used in (Berger et al., 1996; Ratnaparkhi, 1998): where wi ranges over V U *stop* and {wi-i wi-2, attri} is the history, where wi denotes the ith word in the phrase, and attri denotes the attributes that remain to be generated at position i in the phrase. The features used in NLG2 are described in the next section, and the feature weights ai, obtained from the Improved Iterative Scaling algorithm (Berger et al., 1996), are set to maximize the likelihood of the training data. The probability of the sequence W = wn, given the attribute set A, (and also given that its length is n) is: The feature patterns, used in NLG2 are shown in Table 3. The actual features are created by matching the patterns over the training data, e.g., an actual feature derived from the word hi-gram template might be: f(w1,tvi-1,w2-2,attri). 01 if w = from and wi-i = flight and $city  fr E attri otherwise Input to Step 1: 1 $city-fr, $city-to, $time-dep, $date-dep 1 Output of Step 1: &quot;a flight to $city-to that departs from $city-fr at $time-dep on $date-dep&quot; Input to Step 2: &quot;a flight to $city-to that departs from $city-fr at $time-dep on $date-dep&quot;, $city-fr = New York City, $city-to = Seattle , $time-dep = 6 a.m., $date-dep = Wednesday } Output of Step 2: &quot;a flight to Seattle that departs from New York City at 6 a.m. on Wednesday&quot; Low frequency features involving word ngrams tend to be unreliable; the NLG2 system therefore only uses features which occur K times or more in the training data. More specifically, the search procedure implements the recurrence: VVNa = top(N,{wlw E V}) 147.mi-1-1 = top(N,next(WN,i)) The set WN,i is the top N scoring sequences of length i, and the expression next(WN,i) returns all sequences w1 w,+1 such that wi w, E WN,, and wi-Fi E V U *stop*. Currently, there is no normalization for different lengths, i.e., all sequences of length n < M are equiprobable: NLG2 chooses the best answer to express the attribute set A as follows: where Wnig2 are the completed word sequences that satisfy the conditions of the NLG2 search described above. The expression chi(w) denotes the ith closest child to the headword w, par(w) denotes the parent of the headword w, dir E {left, right} denotes the direction of the child relative to the parent, and attr,i denotes the attributes that remain to be generated in the tree when headword w is predicting its ith child. For example, in Figure 1, if w =&quot;flights&quot;, then chi (w) =&quot;evening&quot; when generating the left children, and chl(w) =&quot;from&quot; when generating the right children. Furthermore, if a feature derived from Table 4 looks at a particular word chi (w) and attribute a, we only allow it if a has occurred as a descendent of 1-We use a dummy ROOT node to generate the top most head word of the phrase chi(w) in some dependency tree in the training set. As an example, this condition allows features that look at chi(w) =&quot;to&quot; and $city-toE attr,i but disallows features that look at chi(w) =&quot;to&quot; and $cityfrE The idea behind the search procedure for NLG3 is similar to the search procedure for NLG2, namely, to explore only a fraction of the possible trees by continually sorting and advancing only the top N trees at any given point. However, the dependency trees are not built left-to-right like the word sequences in NLG2; instead they are built from the current head (which is initially the root node) in the following order: As before, any incomplete trees that have generated a particular attribute twice, as well as completed trees that have not generated a necessary attribute are discarded by the search. For example, if the test set contains the template &quot;flights to $city-to leaving at $time-dep&quot;, the surface generation systems will be told to generate a phrase for the attribute set { $city-to, $time-dep }. The output of NLG3 on the attribute set { $city-to, $city-fr, $time-dep } is shown in Table 9. Bad: Words are missing or extraneous words are present No Output: The system failed to produce any output While there were a total 1946 attribute sets from the test examples, the judges only needed to evaluate the 190 unique attribute sets, e.g., the attribute set { $city-fr $city-to } occurs 741 times in the test data. The weighted results in Tables 5 and 6 account for multiple occurrences of attribute sets, whereas the unweighted results in Tables 7 and 8 count each unique attribute set once, i.e., { $city-fr Scity-to } is counted 741 times in the weighted results but once in the unweighted results. The NLG2 and NLG3 systems automatically attempt to generalize from the knowledge inherent in the training corpus of templates, so that they can generate templates for novel attribute sets. Typical values for attributes: $time-dep = &quot;10 a.m.&quot;, $city-fr = &quot;New York&quot;, $city-to = &quot;Miami&quot; is some additional cost associated with producing the syntactic dependency annotation necessary for NLG3, but virtually no additional cost is associated with NLG2, beyond collecting the data itself and identifying the attributes. NLG2 and NLG3 solve the lexical choice problem by learning the words (via features in the maximum entropy probability model) that correlate with a given attribute and local context, whereas (Elhadad et al., 1997) uses a rule-based approach to decide the word choice. (Langkilde and Knight, 1998) maps from semantics to words with a concept ontology, grammar, and lexicon, and ranks the resulting word lattice with corpus-based statistics, whereas NLG2 and NLG3 automatically learn the mapping from semantics to words from a corpus. This limitation can be overcome by using features on values, so that NLG2 and NLG3 might discover  to use a hypothetical example  that &quot;flights leaving $city-fr&quot; is preferred over &quot;flights from $city-fr&quot; when $city-fr is a particular value, such as &quot;Miami&quot;.