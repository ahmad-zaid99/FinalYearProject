We present two systems for identifying sentence boundaries.
One is targeted at high performance and uses some knowledge about the structure of English financial newspaper text which may not be applicable to text from other genres or in other languages.
The other system uses no domain-specific knowledge and is aimed at being portable across English text genres and Roman alphabet languages.
Potential sentence boundaries are identified by scanning the text for sequences of characters separated by whitespace (tokens) containing one of the symbols !, . or ?.
We use information about the token containing the potential sentence boundary, as well as contextual information about the tokens immediately to the left and to the right.
We also conducted tests using wider contexts, but performance did not improve.
We call the token containing the symbol which marks a putative sentence boundary the Candidate.
'Hie portion of the Candidate preceding the potential sentence boundary is called the Prefix and the portion following it is called the Suffix.
The system that focused on maximizing performance used the following hints, or contextual &quot;templates&quot;: The templates specify only the form of the information.
The exact information used by the maximum entropy model for the potential sentence boundary marked by . in Corp. in Example 1 would be: PreviousWordIsCapitalized, Prefix= Corp, Suffix=NULL, PrefixFeature=CorporateDesignator.
The highly portable system uses only the identity of the Candidate and its neighboring words, and a list of abbreviations induced from the training data.2 Specifically, the &quot;templates&quot; used are: The information this model would use for Example 1 would be: PreviousWord=ANLP, FollowingWord=chairmon, Prefix=Corp, Suffix=NULL, PrefixFeature=InducedAbbreviation.
The abbreviation list is automatically produced from the training data, and the contextual questions are also automatically generated by scanning the training data. with question templates.
As a. result, no hand-crafted rules or lists are required by the highly portable system and it can be easily retrained for other languages or text genres.
The model used here for sentence-boundary detection is based on the maximum entropy model used for POS tagging in (Ratna.parkhi, 1996).
For each potential sentence boundary token (., ?, and !, we estimate a. joint, probability distribution p of the token and its surrounding context, both of which are denoted by c, occurring as an actual sentence boundary.
The distribution is given by: p(b, c) = Ir „,,,.f-(b„c), where b e no, yes}, where the cri's are the unknown parameters of the model, and where each aj corresponds to a fi, or a feature.
Thus the probability of seeing an actual sentence boundary in the context c is given by p(yes, c).
The contextual information deemed useful for sentence-boundary detection, which. we described earlier, must be encoded using features.
For example, a useful feature might be: This feature will allow the model to discover that the period at the end of the word Mr. seldom occurs as a sentence boundary.
Therefore the parameter corresponding to this feature will hopefully boost the probability p(no, c) if the Prefix is Mr.
The parameters are chosen to maximize the likelihood of the training data, using the Generalized Iterative Scaling (Darroch and Ratcliff, 1972) algorithm.
The model also can be viewed under the Maximum Entropy framework, in which we choose a distribution p that maximizes the entropy H (p) where /:5(b, c) is the observed distribution of sentenceboundaries and contexts in the training data.
As a result, the model in practice tends not to commit towards a particular outcome (yes or no) unless it has seen sufficient evidence for that outcome; it is maximally uncertain beyond meeting the evidence.
All experiments use a simple decision rule to classify each potential sentence boundary: a potential sentence boundary is an actual sentence boundary if and only if p(yesic) > .5, where and where c is the context including the potential sentence boundary.
We trained our system on 39441 sentences (898737 words) of Wall Street Journal text from sections 00 through 24 of the second release of the Penn Treebank3 (Marcus, Santorini, and Marcinkiewicz, 1993).
We corrected punctuation mistakes and erroneous sentence boundaries in the training data.
Performance figures for our best performing system, which used a hand-crafted list of honorifics and corporate designators, are shown in Table 1.
The first test set, WSJ, is Palmer and Hearst's initial test data and the second is the entire Brown corpus.
We present the Brown corpus performance to show the importance of training on the genre of text. on which testing will be performed.
Table 1 also shows the number of sentences in each corpus, the number of candidate punctuation marks, the accuracy over potential sentence boundaries, the number of false positives and the number of false negatives.
Performance on the WSJ corpus was, as we expected, higher than performance on the Brown corpus since we trained the model on financial newspaper text.
Possibly more significant. than the system's performance is its portability to new domains and languages.
A trimmed down system which used no information except that derived from the training corpus performs nearly as well, and requires no resources other than a training corpus.
Its performance on the same two corpora is shown in Table 2.
Since 39441 training sentences is considerably more than might exist in a new domain or a language other than English, we experimented with the quantity of training data required to maintain performance.
Table 3 shows performance on the WSJ corpus as a. function of training set size using the best performing system and the more portable system.
As can seen from the table, performance degrades a.s the quantity of training data decreases, but even with only 500 example sentences performance is beter than the baselines of 64.00/0 if a. sentence boundary is guessed at every potential site and 78.4%, if only token-final instances of sentence-ending punctuation are assumed to be boundaries.
We would like to thank David Palmer for giving us the test data he and Marti Hearst used for their sentence detection experiments.
We would also like to thank the anonymous reviewers for their helpful insights.
