Our co-occurrence model (Pantel and Ravichandran 2004) makes use of semantic classes like those generated by CBC.
The input to the extraction algorithm is a list of semantic classes, in the form of clusters of words, which may be generated from any source.
The extraction algorithm first labels concepts (A) and (B) with fruit and host respectively.
The labeling of semantic classes is performed in three phases, as outlined below.
In the first phase of the algorithm, feature vec tors are extracted for each word that occurs in a semantic class.
Following (Pantel and Lin 2002), a committee for each semantic class is constructed.
By averaging the feature vectors of the commit tee members of a particular semantic class, we obtain a grammatical template, or signature, for that class.
The syntactical co-occurrence approach has worst-case time complexity O(n2k), where n is the number of words in the corpus and k is the feature space (Pantel and Ravichandran 2004).
Just to parse a 1 TB corpus, this approach requires ap proximately 10.2 years (see Table 2).
Given two sentences with their surface form and part of speech tags, the algorithm finds the optimal lexico-POS alignment.
The algorithm consists of two parts: calculation of the minimal edit dis tance and retrieval of an optimal pattern.
Algorithm for calculating the minimal edit distance between two strings D[0,0]=0 for i = 1 to n do D[i,0] = D[i-1,0] + cost(insertion) for j = 1 to m do D[0,j] = D[0,j-1] + cost(deletion) for i = 1 to n do for j = 1 to m do D[i,j] = min( D[i-1,j-1] + cost(substitution), D[i-1,j] + cost(insertion), D[i,j-1] + cost(deletion)) Print (D[n,m]) Algorithm for optimal pattern retrieval i = n, j = m; while i ? 0 and j ? 0 if D[i,j] = D[i-1,j] + cost(insertion) print (*s*), i = i-1 else if D[i,j] = D[i,j-1] + cost(deletion) print(*s*), j = j-1 else if a1i = b1j print (a1i), i = i -1, j = j =1 else if a2i = b2j print (a2i), i = i -1, j = j =1 else print (*g*), i = i -1, j = j =1 We experimentally set (by trial and error): cost(insertion) = 3 cost(deletion) = 3 cost(substitution) = 0 if a1i=b1j = 1 if a1i?b1j, a2i=b2j = 2 if a1i?b1j, a2i?b2j 4.2 Implementation and filtering.
The above algorithm takes O(y2) time for every pair of strings of length at most y. Hence, if there are x strings in the collection, each string having at most length y, the algorithm has time complexity O(x2y2) to extract all the patterns in the collection.
Applying the above algorithm on a corpus of 3GB with 50 is-a relationship seeds, we obtain a set of 600 lexico-POS.
we empirically compare the pattern-based and co-occurrence-based models
We use a 15GB newspaper corpus consisting of TREC9, TREC 2002, Yahoo!
For our experiments, we extract from this corpus six data sets of differ ent sizes: 1.5MB, 15 MB, 150 MB, 1.5GB, 6GB and 15GB.
For the co-occurrence model, we used Minipar (Lin 1994), a broad coverage parser, to parse each data set.
For the pattern-based approach, we use Brill?s. POS tagger (1995) to tag each data set.
For small datasets (below 150MB), the pattern based method achieves higher precision since the co-occurrence method requires a certain critical mass of statistics before it can extract useful class signatures (see Section 3).
On the other hand, the pattern-based approach has relatively constant precision since most of the is-a relationships se lected by it are fired by a single pattern.
Once the co-occurrence system reaches its critical mass (at 150MB), it generates much more precise hypo nyms.
WordNet consistently generated higher precision relationships although both algorithms approach WordNet quality on 6GB (the pattern based algorithm even surpasses WordNet precision on 15GB).
Furthermore, WordNet only generated a hyponym 40% of the time.
On the 6 GB corpus, the co-occurrence approach took approximately 47 single Pentium-4 2.5 GHz processor days to complete, whereas it took the pattern-based approach only four days to complete on 6 GB and 10 days on 15 GB.
The co-occurrence model has higher precision than the pattern-based algorithm on most datasets.
Because of sparse data, the pattern-based approach has much higher precision and recall (six times) than the co-occurrence approach on the small 15MB dataset.
The corresponding scores for WordNet are 38% accuracy in both the top-1 and top-5 categories (for both strict and lenient).
The per formance of the system in the top 5 category is much better than that of WordNet (38%).
