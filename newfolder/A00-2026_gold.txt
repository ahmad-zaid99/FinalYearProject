The probability model in NLG2 is a conditional distribution over V U * stop*, where V is the generation vocabulary and where *stop* is a special &amp;quot;stop&amp;quot; symbol.
The form of the maximum entropy probability model is identical to the one used in (Berger et al., 1996; Ratnaparkhi, 1998): where wi ranges over V U *stop* and {wi-i wi-2, attri} is the history, where wi denotes the ith word in the phrase, and attri denotes the attributes that remain to be generated at position i in the phrase.
The h, where f3 (a, b) E {0,1}, are called features and capture any information in the history that might be useful for estimating P(wt iwi-1, wi-2, attri).
More specifically, the search procedure implements the recurrence: VVNa = top(N,{wlw E V}) 147.mi-1-1 = top(N,next(WN,i)) The set WN,i is the top N scoring sequences of length i, and the expression next(WN,i) returns all sequences w1 w,+1 such that wi w, E WN,, and wi-Fi E V U *stop*.
More specifically, the search procedure implements the recurrence: VVNa = top(N,{wlw E V}) 147.mi-1-1 = top(N,next(WN,i)) The set WN,i is the top N scoring sequences of length i, and the expression next(WN,i) returns all sequences w1 w,+1 such that wi w, E WN,, and wi-Fi E V U *stop*.
The expression top(N,next(WN,i)) finds the top N sequences in next(WN,i).
The expression chi(w) denotes the ith closest child to the headword w, par(w) denotes the parent of the headword w, dir E {left, right} denotes the direction of the child relative to the parent, and attr,i denotes the attributes that remain to be generated in the tree when headword w is predicting its ith child.
As in NLG2, NLG3 assumes the uniform distribution for the length probabilities Pr(# of left children = n) and Pr(# of right children = n) up to a certain maximum length M' = 10.
Bad: Words are missing or extraneous words are present No Output: The system failed to produce any output While there were a total 1946 attribute sets from the test examples, the judges only needed to evaluate the 190 unique attribute sets, e.g., the attribute set { $city-fr $city-to } occurs 741 times in the test data.
The weighted results in Tables 5 and 6 account for multiple occurrences of attribute sets, whereas the unweighted results in Tables 7 and 8 count each unique attribute set once, i.e., { $city-fr Scity-to } is counted 741 times in the weighted results but once in the unweighted results.
The unweighted results show that the baseline NLG1 does well on the common attribute sets, since it correctly generates only less than 50% of the unweighted cases but over 80% of the weighted cases.
