The system consists of the following six components which are applied in sequence to sentences containing a specific predicate in order to retrieve a set of subcategorization classes for that predicate: For example, building entries for attribute, and given that one of the sentences in our data was (la), the tagger and lemmatizer return (lb).
Patterns encode the value of the VSUBCAT feature from the VP rule and the head lemma(s) of each argument.
The grammar consists of 455 phrase structure rule schemata in the format accepted by the parser (a syntactic variant of a Definite Clause Grammar with iterative (Kleene) operators).
There are 29 distinct values for VSUBCAT and 10 for PSUBCAT; these are analysed in patterns along with specific closed-class head lemmas of arguments, such as it (dummy subjects), whether (wh-complements), and so forth, to classify patterns as evidence for one of the 160 subcategorization classes.
>Currently, the coverage of this grammar&#8212;the proportion of sentences for which at least one analysis is found&#8212;is 79% when applied to the Susanne corpus (Sampson, 1995), a 138K word treebanked and balanced subset of the Brown corpus.
The combined throughput of the parsing components on a Sun UltraSparc 1/140 is around 50 words per CPU second.
The extractor takes as input the ranked analyses from the probabilistic parser.
It locates the subanalyses around the predicate, finding the constituents identified as complements inside each subanalysis, and the subject clause preceding it.
The classifier filters out as unclassifiable around 15% of patterns found by the extractor when run on all the patternsets extracted from the Susanne corpus.
We estimate this probability more directly by first extracting the number of verbs which are members of each class in the ANLT dictionary (with intuitive estimates for the membership of the novel classes) and converting this to a probability of class membership by dividing by the total number of verbs in the dictionary; and secondly, by multiplying the complement of these probabilities by the probability of a pattern for class i, defined as the number of patterns for i extracted from the Susanne corpus divided by the total number of patterns.
Setting a threshold of less than or equal to 0.05 yields a 95% or better confidence that a high enough proportion of patterns for i have been observed for the verb to be in class i3.
At the moment, we exploit linguistic information about the syntactic type, obligatoriness and position of arguments, as well as the set of possible subcategorization classes, and combine this with statistical inference based on the probability of class membership and the frequency and reliability of patterns for classes.
In order to test the accuracy of our system (as developed so far) and to provide empirical feedback for further development, we took the Susanne, SEC (Taylor &amp; Knowles, 1988) and LOB corpora (Garside et at., 1987)&#8212;a total of 1.2 million words&#8212;and extracted all sentences containing an occurrence of one of fourteen verbs, up to a maximum of 1000 citations of each.
The sentences containing these verbs were tagged and parsed automatically, and the extractor, classifier and evaluator were applied to the resulting The binomial distribution gives the probability of an event with probability p happening exactly m times out of n attempts: successful analyses.
The citations from which entries were derived totaled approximately 70K words.
All classes for seem are exemplified in the corpus data, but for ask, for example, eight classes (out of a possible 27 in the merged entry) are not present, so comparison only to the merged entry would give an unreasonably low estimate of recall.
There are only 13 true negatives which the system failed to propose, each exemplified in the data by a mean of 4.5 examples.
On the other hand, there are 67 false negatives supported by an estimated mean of 7.1 examples which should, ideally, have been accepted by the filter, and 11 false positives which should have been rejected.
The experiment used the same probabilistic parser and tag sequence grammar as are present in the acquisition system (see references above)&#8212;although the experiment does not in any way rely on the parsers or grammars being the same.
We randomly selected a test set of 250 in-coverage sentences (of lengths 3-56 tokens, mean 18.2) from the Susanne treebank, retagged with possibly multiple tags per word, and measured the 'baseline' accuracy of the unlexicalized parser on the sentences using the now standard PARSEVAL/GEIG evaluation metrics of mean crossing brackets per sentence and (unlabelled) bracket recall and precision
We acquired subcategorization and associated frequency information from the citations, in the process successfully parsing 380K words.
The GEIG measures for the lexicalized parser show a 7% improvement in the crossing bracket score.
Over the existing test corpus this is not statistically significant at the 95% level (paired t-test, 1.21, 249 df, p = 0.11)&#8212;although if the pattern of differences were maintained over a larger test set of 470 sentences it would be significant
