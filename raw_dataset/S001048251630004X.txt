@&#MAIN-TITLE@&#Unsupervised entity and relation extraction from clinical records in Italian

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Extraction of domain relevant entities and relations from clinical records in Italian.


                        
                        
                           
                           Domain knowledge for the identification and classification of medical and pharmaceutical entities.


                        
                        
                           
                           Clustering to discover possible relations.


                        
                        
                           
                           Experimental testing on a fairly large dataset of clinical records.


                        
                        
                           
                           Solution evaluated from both a quantivative and a qualitative point of view.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Unsupervised learning

Relation clustering

Entity extraction

Medical information extraction

Entities relation discovery

@&#ABSTRACT@&#


                  This paper proposes and discusses the use of text mining techniques for the extraction of information from clinical records written in Italian. However, as it is very difficult and expensive to obtain annotated material for languages different from English, we only consider unsupervised approaches, where no annotated training set is necessary. We therefore propose a complete system that is structured in two steps. In the first one domain entities are extracted from the clinical records by means of a metathesaurus and standard natural language processing tools. The second step attempts to discover relations between the entity pairs extracted from the whole set of clinical records. For this last step we investigate the performance of unsupervised methods such as clustering in the space of entity pairs, represented by an ad hoc feature vector. The resulting clusters are then automatically labelled by using the most significant features. The system has been tested on a fairly large data set of clinical records in Italian, investigating the variation in the performance adopting different similarity measures in the feature space. The results of our experiments show that the unsupervised approach proposed is promising and well suited for a semi-automatic labelling of the extracted relations.
               

@&#INTRODUCTION@&#

Medical information processing systems are of paramount importance for patient care. Indeed, delivering the most appropriate piece of information in every moment and in the correct situation can represent a crucial support for clinical activities. Although the digitization of all medical documents in hospital information systems is now nearly completed, several problems still need to be solved for their effective and reliable automatic processing. A number of them, spanning from semantics-based indexing of documents for improved retrieval to more advanced query based information extraction, and to the application of ontology-based strategies for privacy protection, would take great advantage from improved approaches to extract relevant information from technical texts. Such information usually consists of relevant entities and relations connecting them [1]. Let us, for example, consider the following sentence:
                        
                           
                              
                                 Found
                                 
                                 subcapital
                                 
                                 fracture
                                 
                                 and
                                 
                                 dislocation
                                 
                                 of
                                 
                                 left
                                 
                                 shoulder
                                 
                                 and
                              
                              
                                 contusion
                                 
                                 of
                                 
                                 right
                                 
                                 hip
                                 
                                 caused
                                 
                                 by
                                 
                                 accidental
                                 
                                 fall
                                 
                                 at
                                 
                                 home
                              
                              .
                           
                        
                     We can identify the following five medical entities:
                        
                           
                              
                                 Found
                                 
                                 subcapital <ent1>fracture</ent1>and <ent2>dislocation</ent2>
                              
                                 of
                                 
                                 left <ent3>shoulder</ent3>and <ent4>contusion</ent4>of
                                 
                                 right
                              
                              <ent5>hip</ent5>caused
                                 
                                 by
                                 
                                 accidental
                                 
                                 fall
                                 
                                 at
                                 
                                 home
                              
                              .
                           
                        
                     There should be also three relations respectively connecting entities 4 and 5, 1 and 3, and 2 and 3; their label should refer to an illness of a body part.

Progresses in the field mainly regard general domains and produced reliable entity recognition systems including, for example, TextPro [2]. However, the availability in restricted domains of specific resources, such as dictionaries and ontologies, is known to represent an important opportunity for a burst in performance. This is surely the case for scientific documentation, including not only scientific papers, but also medical reports. In fact, the medical decision process is founded on both the availability of up-to-date results from the scientific community, and on the complete information about the current case, now available in digital form, then representing a not to be missed opportunity.

In this paper we propose a system to solve this problem, and describe it in detail, together with the adopted knowledge sources, including lexical resources and natural language processing tools. The system improves the one previously reported in [3], as here the general domain entity recognition step has been eliminated, while a final cluster labelling has been added. The final conclusion drawn in the preceding publication, that is the choice of the cosine similarity as the more effective similarity measure has been confirmed by the more extended experiments reported here. As a consequence, we choose a slightly different clustering algorithm, the spherical K-means, which is based on such measure. Further experiments have then been performed with this refined system on two larger data sets.

The paper is structured as follows. The next section is devoted to a deeper discussion of motivation of this work, while the following one considers the state of the art and Section 4 describes the information sources adopted by the system. Section 5 describes the system architecture focusing on entity recognition and relation clustering. The experimental assessment is discussed in Section 6, while Section 7 analyses the solution obtained both from a quantitative point of view and by considering the resulting cluster labelling. A final discussion concludes the paper in Section 8.

Given the availability of digitalized documents, it would be valuable to physicians and researchers being able to retrieve the clinical records for similar past cases, or the immediate availability of a different statistical analysis (e.g., correlation of breast cancer cases with the age). Rather than with the traditional keyword-based search, these tasks would be more effectively attained if a more structured search was possible, as for example looking for all cases involving two given entities involved in a certain relation. To achieve this goal, it is necessary to annotate the input text with entities and relations.

Such situation introduces a number of technical challenges. While scientific publications are written in English to favour international opinion exchanges, patient records are usually written in the hospital country language. In our case, we consider Italian, for the processing of which, as for most languages different from English, no rich literature is available. Another problem to take into account is the occurrence, in clinical records, of typos and not standard abbreviations, in addition to the most usual acronyms. Last but not least, moving from text to knowledge processing raises tricky privacy problems. In fact, especially but not only in small hospitals, obscuring the patient name is not sufficient to hide his/her identity as a precise profiling of the patient can often be obtained from a few of the medical characteristics reported in a record.


                     Ad hoc solutions are needed to tackle such problems. Beginning from the last one, more effective privacy protection can be based on ontological information [4]. However, the construction and population of the necessary ontologies require the identification of relevant information from medical reports. Furthermore, more in general, the identification of potentially dangerous information is again based on the extraction of domain entities and relations.

A great deal of effort to ease the porting of systems to languages different from English has been put in the development of lexical resources, which are now available also for Italian. However, even if such resources are valuable tools for the porting, they are not enough, because of the intrinsic linguistic differences between languages. It is therefore necessary to take into account the characteristics of the specific language in both entity and relation recognition. However, in this second step they are even more important, as relations strongly depend on the sentence structure, obviously language dependent.

Such problems are usually tackled adopting machine learning approaches, which try to extract information directly from data. Whenever annotated data are available, supervised strategies outperform unsupervised ones among machine learning approaches, at the price of an expensive annotation phase. In fact, domain experts are required to invest their time in a long and tedious annotation activity. In our case, this would imply persuading medical staff to invest part of their precious time to annotate data with information about the presence and the type of domain relevant entities and relations in records to be used for training. While this would be really difficult, things are much different if their competence is only required to check on an annotation which has been automatically produced. By following this idea, we propose a framework which integrates a knowledge-based and a text mining approaches: the expert intervention will be finally required to check on natural language labels associated to groups of relations.

The framework is composed by three phases. The first one is devoted to domain entity (i.e., medical and pharmaceutical entities) identification and classification, and exploits domain related lexical resources and standard natural language tools. The second one is based on an unsupervised machine learning approach, clustering, to avoid the necessity of annotating data. A potential relation is hypothesized among all pairs of the entities identified in the preceding phase. Clustering is then applied to group similar entity pairs. Small clusters indicate the lack of repetitive patterns and will therefore be considered as entity pairs which are not in relation to each other, while larger clusters will correspond to different relation types.

In order to compensate for the lack of annotation, we try and put as much knowledge as possible in the input representation. In particular, together with n-grams of words, we also include barrier features, an innovative type of features recently introduced for information extraction [5]. While such features are only based on standard natural language processing and do not require any manual annotation, they capture the context in which entity tokens appear in an efficient and effective way. The third and final phase of the proposed framework aims at associating a label to each cluster. It is based on the analysis of the n-grams of words which represent the only lexicalized features adopted in the clustering, and associates to each cluster the terms which best describe it.

@&#RELATED WORK@&#

Given the growing digitization of medical information processing around the world, several systems for information retrieval and extraction have been proposed, including [1,3,6–10]. Many of them are based on the extraction of entities and relations from free text parts of medical records. In the majority of these systems, entities and relations are considered in two separate steps. Some systems, such as [11,12], only consider general domain entities, while others [6–8,13–16] base the extraction of entities on domain specific external resources. In particular, most use the Unified Medical Language System (UMLS) [17]. Among these, [7] proposes a semantic framework for performing information retrieval in a collection composed by biomedical-chemistry patents and full-text articles. Queries are refined by means of biomedical entities previously extracted and tagged by using the UMLS metathesaurus.

A large part of the proposed approaches are based on machine learning. As an example we mention [8], where Conditional Random Fields (CRFs) are applied for identifying disease mentions; adopted features regard disease-specific contexts, word spelling, general linguistic characteristics, syntactic dependencies and dictionary entries. Experiments are based on the (English) Arizona Disease Corpus (AZDC) [18], which contains detailed annotations of diseases, including information taken from UMLS.


                     BANNER is another machine-learning system, also based on CRFs, described in [19], which also contains a wide survey of the features adopted for named entity recognition in biomedical texts. In fact, the main characteristics of this system is the use of features only based on word spelling and shallow parsing, rather than semantics or other kinds of knowledge-based processing. BANNER is widely adopted as a benchmark in this field, and it has also been used to build the AZDC corpus [18].

Many biomedical corpora are available in literature, but none of these are annotated with the types of entities and relationships that are relevant to the study of phenotype information. For these reason in [20] a new annotated corpus (PhenoCHF) has been created, focussed on the identification of phenotype information for a specific clinical sub-domain, i.e., congestive heart failure.

A number of supervised approaches have also been proposed. In [12] the authors present a knowledge and linguistic-based approach for the extraction of medical entities and the semantic relations between them. It recognizes medical entities using MetaMap [21], a tool mapping medical texts to the UMLS concepts, which are then used to construct linguistic patterns and to refine a question-answering system. Another example of application of supervised methods to a similar context is given by [22], which aims at extracting relations between events and the corresponding arguments from biomedical texts. Two different approaches are considered and assessed, the former based on Support Vector Machines (SVM), the latter on joint Markov logic networks. The system presented in [13] focusses on treatment relations between pairs of medical concepts mentioned in clinical notes. Lexical and semantic features have been extracted using the MEDication Indication (MEDI) resource [23] and SemRep [24].

All the supervised systems necessitate for a training phase for which a large quantity of manually annotated data must be available. Some recent works aim to create annotated corpora, as for example [25], where syntactic and semantic information is used to such end. In details, syntactic features are extracted by means of parsers and a schema for predicate-argument structures, while information is deduced from the input texts by means of UMLS. On the other hand, the costs in time and money required by the annotation justifies the proposal of unsupervised approaches for relation extraction [26–35]. Among the others, BioNoculars [28], a system aiming to extract protein–protein interactions by means of a statistical unsupervised system. An important feature of this approach is the construction of a graph to exploit data redundancy and construct a number of domain independent patterns used to extract the required interactions. The data set adopted for assessment, namely MEDLINE, is formed by abstracts of biomedical scientific papers and includes labelled interacting proteins and relations among them.

An interesting application of unsupervised approaches is included in IDEX [36], an interactive dynamic system in which the user formulates a topic description. The documents resulting from such initial query are then processed in an unsupervised fashion in order to automatically label relations occurring between entity pairs and use such labels to improve retrieval.

In [37] a semi-automatic approach is presented using a large number of medical records in Italian to build a training corpus annotated with medical entities (active ingredient, body part, sign or symptom, disease, syndrome, drug and treatment) and temporal expressions; the authors extend the corpus annotation, including expressions denoting physiological measures and the entity to which they refer, in order to leverage unlabelled data. This system is a combination of both supervised machine-learning techniques and unsupervised deep learning techniques. A visual tool for correlating the entities on a statistical basis has been provided, which graphically visualizes the correlations between signs and diseases with different degree of probability, helping doctors in formulating diagnoses.

Not only natural language processing requires general knowledge about the considered language, but it can also benefits from data about the involved context. In our case, the latter consists of the domain terminology and of relations which can possibly occur between two considered entities.

In this section, we review what tools we used to include both types of information in our system. We will start describing the Unified Medical Language System, conveying the medical domain specific terminology and a semantic network giving ontological knowledge about domain entities and relations, and the other domain information sources. We end this section presenting the natural language tools adopted.

We decided to include in our system the Unified Medical Language System
                           1
                        
                        
                           1
                           
                              http://www.nlm.nih.gov/research/umls/
                           
                         (UMLS) [17], developed by U.S. National Library of Medicine, because it is widely used in literature, as it was mentioned in Section 3, and includes a rich Italian vocabulary. It is composed by several health and biomedical vocabularies and standards to enable interoperability between computer systems.

UMLS is composed by three parts:
                           
                              1.
                              The Metathesaurus represented by different medical thesauri characterized by several different languages, including Italian.

The Semantic Network modelling all the concepts occurring in the Metathesaurus as a graph, where nodes correspond to semantic types and edges to the relations connecting them. In particular, the 133 semantic types considered in the network regard organisms, anatomical structures, biological functions, chemicals, physical objects, ideas or concepts. Among the 54 semantic relations, the most important is the isA one: it describes a hierarchy among the semantic types tying the more specific concept to the more general one.

The Specialist Lexicon only available for English.

The Italian language vocabularies included in the UMLS Metathesaurus are very rich from the medical point of view, but do not provide enough coverage of the pharmaceutical terms. This information is included in our system integrating the Pharmaceutical Reference Book
                           2
                        
                        
                           2
                           
                              https://farmaci.agenziafarmaco.gov.it/bancadatifarmaci/
                           
                         (PRB), officially maintained by the Agenzia Italiana del Farmaco, the Italian government agency in charge for the drugs administration.

The availability of clinical records in digital formats is one of the main effects of the introduction of computerized information systems in medical institutions. To take full advantage of the possibility to automatically process such documents, also a large quantity of legacy documentation has been digitized, and standards have been introduced. Among them, Health Level 7 (HL7)
                           3
                        
                        
                           3
                           
                              http://hl7.org/
                           
                         represents the standard de facto in medical and health information systems for representing medical, clinical, administrative and financial data.

Several tools have been produced to analyse standard English texts, characterized by different approaches and methodologies. They have also been ported on languages different from English, including Italian. Also in this case, however, optimal performance is obtained on a standard language, usually described by the one adopted in newspapers. The system described in this paper considers the suite TextPro [2], which has been designed and implemented for both Italian and English texts. Different modules are offered by this suite, aimed at different analysis levels; we only considered four of them: (1) TokenPro which divides the raw text in tokens, using some predefined language dependent rules; (2) SentencePro which splits such sequence of tokens in sentences; (3) TagPro which is a Part of Speech (PoS) tagger; and (4) LemmaPro, a lemmatizer which associates all lemmas corresponding to the considered token.

The system proposed in this paper is structured in several modules, and some of them include the tools described in the previous section. Its work-flow is depicted in Fig. 1
                     . The first module performs a preprocessing step on the raw text in input, while the second one extracts all the strings composed by one or more tokens referring to domain concepts. Such strings are referred to as domain entities. These modules are described in detail in the remainder of this section. We want to point out here that no machine learning approach has been employed for the entity extraction step.

On the other hand, we applied clustering to identify ordered pairs of entities corresponding to a relation. As usual in the machine learning field, we adopt the Vector Space Model 
                     [38] (VSM), where each input is represented by a point in a feature space the dimension of which is equal to the number of features.

The features adopted for this work, discussed in Section 5.3.1, are based on patterns occurring in the text. Therefore, a further step is performed on the output returned by the preprocessing to collect such patterns together with the associated counters. Only the features having a large number of occurrences in the data set are selected, as described in Section 5.3.2. At this point for each potential relation a feature vector is produced, combining the features of both the entities involved, and then the set of feature vectors is passed to the module for the relations discovery.

The text processed by our system is extracted from anonymized medical records, in the form of plain text encoded in UTF-8. The text includes a small set of special characters, used as delimiters and/or formatters. The largest part of these medical records has been produced by an HL7-compatible information system. At the end of each medical record, there is often an ICD9M (International Standard for Encoding and Classifying Diseases) disease code, which we disregard together with the rest of the structured part of the records.

The text is initially preprocessed for extracting textual parts from the medical records, and to get rid of non-textual characters. The plain text produced by this preprocessing step is passed to the natural language processing suite TextPro to perform the following four steps:
                           
                              
                                 Tokenization: the input is split into tokens, constituting the units for language processing.


                                 Sentence splitting: the input token sequence is split into sentences; this task is often not trivial, and we need an accurate sentence splitting because every pair of entities belonging to the same sentence is included in the list of potential relations.


                                 PoS tagging: a Part-of-Speech (PoS) tag is associated to each token, describing its syntactical role in the sentence; we need PoS tagging to extract the domain entities and to describe the patterns which correspond to some of the features employed to describe the input.


                                 Lemmatization: a lemma is associated to each input token; it is used for searching the term in the dictionary, and to give a less sparse representation of the input.

Entity extraction is crucial for our analysis, and a specific module has been implemented with the goal of extracting entities which are relevant for the application domain: biomedical and pharmaceutical entities in our case. The module follows a pattern matching approach by identifying each occurrence of the 19 PoS patterns shown in Table 1
                         in the input text as a candidate to be analysed further.

Afterwards, for each token occurring in the identified pattern, we search for matches of the corresponding lemma in the dictionaries. In case of multi-word expressions, when several patterns apply to overlapping strings of tokens, we apply a greedy approach by choosing the longest one matching the input.

The output is produced following the TextPro format, that is a line for each token, and a column for each analysis level. In our system these files are enriched by the information about Medical and Pharmaceutical entities obtained from the dictionaries provided by UMLS and PRB. These information are labelled as MED for the medical entities, and FAR for the pharmaceutical ones (the whole entity tag list is shown in the Table 2
                        ).

An example of the output produced by the entity extraction module for the following portion of text is shown in Fig. 2
                        , where the richer SUBCAT annotation is considered, as it also includes MEDCAT.

In details, the following paragraph describes a patient of the Cardiology Department suffering of torsade de pointes (a polymorphic ventricular tachycardia) caused by chronic kidney failure and the delivering of sotalol.
                           Paziente ricoverata presso la Cardiologia dal 9 al 19 febbraio per episodi di torsione di punta verosimilmente secondari ad insufficienza renale cronica e sotatolo.
                         An English translation of the text above is reported for clarity.
                           Patient admitted to the Cardiology department from the 9th to the 19th of February due to episodes of torsade de pointes probably incidental to chronic kidney failure and sotalol.
                        
                     

In addition to a label indicating whether the entity is medical (MED) or pharmaceutical (FAR), we also add to each medical entity annotation the sub-categories included in the UMLS database in correspondence to the dictionary entry. The list of sub-categories labels are summarized in Table 2. A side-effect of such sub-categorization is that the number of potential relations increases while it becomes possible to find more specific relations.

In this case, however, all medical entities have been labelled with sub-categories and the entity tagging follows an IOB-like notation where the prefix B stays for “beginning” (first token of entity mention), I for “inside” (labels all subsequent tokens) and O for “outside” (non-entities tokens). It can happen that several labels are associated to the same medical entity, as in the case of “torsione di punta” (torsade de pointes) or “insufficienza renale cronica” (chronic kidney failure), which can belong simultaneously to both MED and MAL classes, listed in Table 2.

It is worth mentioning that the Metathesaurus included in UMLS is very large, even when limited to Italian thesauri only. In order to prevent a deterioration of performance in the access to the resource, it is advisable to import its contents into a relational database. In our implementation we chose MySQL, as it is a very popular open source database management system.

As discussed above, we apply an unsupervised approach to identify groups of relations of the same type appearing in the data set. Each pair of entities occurring in the same sentence identifies a potential relation, therefore all possible entity pairs must be considered. We then apply a clustering algorithm to the set of all the potential relations identified.

The rationale underlying the method proposed is that relations of the same type should belong to the same cluster, while different kinds of relations should correspond to different clusters. Therefore, the choice of the similarity measure is crucial for the results. Other choices concern the clustering algorithm and the number of clusters, which must be chosen a priori for nearly all clustering algorithms and in particular in the ones we consider. On the other hand, pairs which are different from all the others are likely to correspond to unrelated entities. Following this assumption we will disregard all the clusters having a size smaller than a given threshold.

The input to machine learning applications is usually represented using the VSM where a vector is associated to each item, with an element corresponding to each feature. Vectors of size M correspond to points in an M-dimensional space: the main hypothesis underlying the VSM is that similar objects are represented by points which are close in the M-dimensional space. The choice of an appropriate feature space is clearly crucial for the results of the approach.

The feature vector representing the entity pairs is constructed by concatenating the features associated to the two entities. The chosen features are: type of the entity, unigrams, bigrams and trigrams of words, and barrier features. The type of the entity refers to the label associated to it by the extraction step.

We also consider all the words belonging to each entity (unigrams of words) and all the couples and triplets of adjacent words such that at least one of the words belongs to an entity (bigrams and trigrams of words).


                           Barrier features (BF) have been introduced in [5] in the context of entity and relation recognition from documents in English. BFs are based on a set of pairs of POS tags, (e,t), where for each occurrence of the trigger t the closest end-point e to its left is taken. If no token with the considered PoS appears in the prefix of the sentence, then the beginning of the sentence is chosen as e. The set of all POS tags occurring between them gives the value of the feature. These features aim to capture information about syntactical patterns inside the sentence, giving evidence in favour or against the occurrence of a relation. For example, given the following POS tagged sentence:
                              
                                 
                                    Paziente
                                    /
                                    SPN
                                    
                                    ricoverata
                                    /
                                    VSP
                                    
                                    presso
                                    /
                                    E
                                    
                                    la
                                    /
                                    RS
                                    
                                    〈
                                    ent
                                    1
                                    〉
                                    Cardiologia
                                    /
                                    SS
                                    〈
                                    ent
                                    1
                                    〉
                                    
                                    dal
                                    /
                                    ES
                                    
                                    9
                                    /
                                    N
                                    
                                    al
                                    /
                                    ES
                                    
                                    19
                                    /
                                    N
                                    
                                    febbraio
                                    /
                                    SS
                                    
                                    per
                                    /
                                    E
                                    
                                    episodi
                                    /
                                    SP
                                    
                                    di
                                    /
                                    E
                                    
                                    〈
                                    ent
                                    2
                                    〉
                                    torsione
                                    /
                                    SS
                                    
                                    di
                                    /
                                    E
                                    
                                    punta
                                    /
                                    SS
                                    〈
                                    ent
                                    2
                                    〉
                                    verosimilmente
                                    /
                                    B
                                    
                                    secondari
                                    /
                                    AP
                                    
                                    ad
                                    /
                                    E
                                    〈
                                    ent
                                    3
                                    〉
                                    insufficienza
                                    /
                                    SS
                                    
                                    renale
                                    /
                                    AS
                                    
                                    cronica
                                    /
                                    AS
                                    〈
                                    ent
                                    3
                                    〉
                                    
                                    e
                                    /
                                    C
                                    〈
                                    ent
                                    4
                                    〉
                                    sotatolo
                                    /
                                    SS
                                    〈
                                    ent
                                    4
                                    〉
                                 
                              
                           
                        

The BFs corresponding to the trigger “renale” are reported in Table 3
                           . Note that they will set to true all BFs with the same endpoint and trigger and having a value set including the one extracted from the data, so that several BFs can be activated for the same endpoint-trigger pair. In this way, simplified versions of the considered patterns are also included in the analysis. They provide information which is complementary with respect to n-grams of words, and all features together give an effective representation of the chunk of sentence containing the considered entities.

The construction of the BFs can be better explained considering the following sentence:


                           Il/RS paziente/SS vive/VI al/ES proprio/DS domicilio/SS con/E la/RS moglie/SS in/E 
                           
                              <
                              ent
                              1
                              >
                            
                           presenza/SS 
                           
                              〈
                              /
                              ent
                              1
                              〉
                            
                           effettivamente/B di/E 
                           
                              〈
                              ent
                              2
                              〉
                            
                           deficit/SP motori/AP e/C 
                           
                              cognitivi
                              /
                              AP
                              〈
                              /
                              ent
                              2
                              〉
                            (The patient lives at home with his wife showing cognitive and motor deficits)

In Table 4
                            we summarize all BFs associated to adjective (AP) “cognitivi”, which is then the trigger. The first row in the table shows the value of the BF corresponding to the endpoint AP, corresponding to the token “motori”: it only contains the tag C corresponding to the conjunction “e”. The value of the BF in the second row is empty, because the closest word tagged as C is the word immediately preceding the trigger. The value of third BF is {E,AP,C,SP}, that is the set of PoS tags between trigger AP (“cognitivi”) and endpoint B (“effettivamente”), deleting any multiple occurrence of the same PoS tags. The last row of the table has the value of BF {E,SP,VI,RS,C,AP,B,SS,DS,ES}, since the trigger is AP (“cognitivi”), but there is not the endpoint SPN in the sentence, so the value of the BF is given by the set of all PoS tags between the trigger and the beginning of the sentence.

In [5], the pairs (e,t) have been chosen on the basis of linguistic properties and applied to the English language. As they cannot be directly applied to Italian, we designed an automatic procedure to construct them. In the work-flow in Fig. 1, this step is named barrier features identification and it is based on statistics collected on a large general purpose text automatically labelled with POS tags. Our goal consists in finding POS pairs able to identify the most informative set of tags. We therefore assess them by entropy-based measures, choosing the best ones. While a complete and exhaustive experimentation is still in progress, in this work we adopted the pairs reported in Table 5
                           .

The BFs are eventually extracted from the text and combined with the other features in the feature vector.

With the only exception of the type of the entity, all the considered features are Boolean, labelled true if the corresponding pattern occurs in the input, false otherwise.

Although relations are defined as ordered pairs of entities, the order in which the two entities appear in the sentence is not taken into account, as it does not depend on the semantics of the sentence, but only on its linguistic realization. This is particularly true in our case as Italian is characterized by a relatively free order of words. As a last thing we mention the fact that, since no entity can be in relation with itself, the two entities in a pair must always be distinct.

As a very large number of different features can occur for both the n-grams of word and barrier features, feature selection becomes crucial for both accuracy and efficiency. In fact, as unigrams, bigrams and trigrams of words are lexicalized features, that is they directly depend on words, their quantity can be very large: in the worst case, the number of all possible trigrams could be cubic in the size of the dictionary. Even if such bound is never approached, the number of all possible triplets of words possible in natural language can be prohibitive. As long as BFs are concerned, each of them considers a set of tags which can be quite larger than for trigrams, resulting in a large number of possible instances of BFs.

The fact that the number of different features can be so large has an effect on the number of occurrences of each of them. Indeed, there is usually a relatively small number of features occurring very frequently, while most of them occur just a very small number of times. A usual strategy to prevent such problem is to put a threshold on the number of occurrences of the features we consider. Given the pattern qualitatively described above, a reasonable choice is to choose the median of the number of occurrences as value for the threshold. Given the different nature of word n-grams and BFs, we computed the medians separately for these two classes.

The complete identification of the process detailed in Section 5 requires a few choices which depend on the characteristics of the data sets used, described in Section 6.1.

Such decisions include the similarity measure employed by the clustering algorithm and the number of clusters, considered in Section 6.2, the clustering algorithm, in Section 6.3 and the number of clusters, in Section 6.4.

The input to the system consists of digital medical records extracted from the processing of the NoemaLife Galileo EMR system and written in Italian. They contain semi-structured and unstructured text parts: the former is usually composed by fields with information about the patient, the pathology or the therapy. The latter consists in the parts of the documents containing unstructured text: in the following we will concentrate on the processing of such text parts.

The whole dataset consists of 989 medical records, including 95,116 sentences and 1,344,636 tokens.

For the entity pairs we considered both cases of entity recognition described in Section 5.2, that is with and without sub-categorization. This results in two different collections of entity pairs:
                           
                              
                                 MEDFAR entities are only tagged as generic medical and pharmaceutical; in total, there are 156,999 entity pairs.


                                 SUBCAT entities are labelled with the medical sub-categories shown in Table 2 and the generic pharmaceutical label; their number is larger than the previous case, giving 448,046 entity pairs.

As explained in Section 5.3.2, the number of features before feature selection is very large: 318,007 features for MEDFAR and 336,881 for SUBCAT. We then apply a threshold on their number of occurrences, equal to the median. The threshold values result being the same for both n-grams and barrier features, that is 4 for MEDFAR and 9 for SUBCAT. In this way, the number of different features decreases (see the figures in Table 6
                        ).

The aim of this first set of experiments is determining the similarity measure which best suits our application. As discussed in Section 5.3.1, all the features are Boolean, with the only exception of the type of involved entities, which can take five different values. We therefore considered the following different measures [39], which are adequate to binary features. The corresponding expressions are given for two vectors x and y having size M.
                           
                              
                                 Manhattan, also known as city block or norm 1: 
                                    
                                       (1)
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                i
                                                =
                                                1
                                             
                                             
                                                M
                                             
                                          
                                          ∣
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          −
                                          
                                             
                                                y
                                             
                                             
                                                i
                                             
                                          
                                          ∣
                                       
                                    
                                 In the Boolean case, it corresponds to the number of elements for which the two vectors differ.


                                 Binary: this distance only applies to Boolean features and can be formalized as follows: 
                                    
                                       (2)
                                       
                                          1
                                          −
                                          
                                             
                                                
                                                   
                                                      b
                                                   
                                                   
                                                      11
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      b
                                                   
                                                   
                                                      11
                                                   
                                                
                                                +
                                                
                                                   
                                                      b
                                                   
                                                   
                                                      10
                                                   
                                                
                                                +
                                                
                                                   
                                                      b
                                                   
                                                   
                                                      01
                                                   
                                                
                                             
                                          
                                       
                                    
                                 where b
                                 11 and b
                                 00 are the numbers of features assuming respectively false and true values in both vectors and b
                                 10 and b
                                 01 represent the number of the elements assuming value true in exactly one of the two vectors.


                                 Cosine: the inverse cosine similarity: 
                                    
                                       (3)
                                       
                                          1
                                          −
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   
                                                      M
                                                   
                                                
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      i
                                                   
                                                
                                                ·
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      ‖
                                                      
                                                         x
                                                      
                                                      ‖
                                                   
                                                   
                                                      2
                                                   
                                                
                                                
                                                   
                                                      ‖
                                                      
                                                         y
                                                      
                                                      ‖
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              

For this experiment we used the R implementation of the K-means algorithm.

Because of size issues we ran the experiment on a reduced data set of 10,000 entity pairs, extracted from a set of randomly selected clinical records.

We set the value of K (number of clusters) to 10 and the maximum number of iterations also to 10. We aim at a solution where no cluster has extreme dimension, that is none of them is too small or too large.

The result of the K-means depends on K initial seeds randomly chosen among the data set points, therefore when running the experiment once it can happen that we obtain a very unlucky solution. In order to avoid such unlikely but possible extreme cases, it is a common practice to run the experiment several times and then to choose the solution which best scored on some internal measure. Henceforth, every time we consider K-means, we repeat each experiment 10 times and chose the solution which maximizes the sum of the intra-cluster similarities:
                           
                              (4)
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       K
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                u
                                                ,
                                                v
                                                ∈
                                                
                                                   
                                                      ω
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                       sim
                                       (
                                       u
                                       ,
                                       v
                                       )
                                    
                                 
                              
                           
                        where ω
                        
                           i
                         for 
                           1
                           ≤
                           i
                           ≤
                           K
                         denotes the i-th cluster. An analogous strategy is applied to spherical K-means.

The graph in Fig. 3
                         shows the size of each cluster for the SUBCAT data set, identified by the K-means for the different measures considered.

Analogously, the sizes of each cluster for the MEDFAR data set are reported in Fig. 4
                        . Note that in this case the scale is approximately three times the one in the other graph. The reason is that both Manhattan and Binary distances leads to a very unbalanced solution, where one cluster contains all or nearly all the 10,000 items.

We discard both Manhattan and Binary distances because of their behaviour on the SUBCAT data set, so we are left with the Cosine distance.

A second set of experiments aiming at the choice of the clustering algorithm was performed on the same reduced data set. We considered K-means, hierarchical and model-based clustering, which is sometimes called EM-clustering because it adopts the Expectation-Maximization algorithm to identify the parameters [38], as implemented by the C clustering library [40]. In particular, for the hierarchical algorithm we considered a complete link strategy [38] where the distance between clusters is computed as the longest distance between their elements. Results, reported in Fig. 5
                        , shows that the three different clustering algorithm perform similarly. Therefore, we chose the most computationally efficient method: K-means.

For determining the number of clusters we used all the entity pairs produced for both the data sets described above. Since we have shown that the cosine similarity is a good similarity measure, we decided to adopt a slight variation of the K-means algorithm exploiting the cosine similarity measure: spherical K-means [41]. In fact while classical K-means minimizes the mean squared error from the cluster centroid 
                           
                              
                                 
                                    
                                       1
                                    
                                    
                                       N
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          x
                                       
                                    
                                 
                                 ∥
                                 x
                                 −
                                 
                                    
                                       μ
                                    
                                    
                                       k
                                       (
                                       x
                                       )
                                    
                                 
                                 ∥
                              
                           
                        where N is the total number of feature vectors and 
                           
                              
                                 μ
                              
                              
                                 k
                                 (
                                 x
                                 )
                              
                           
                         is the most similar centroid, in spherical K-means the objective function is defined as 
                           
                              
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          x
                                       
                                    
                                 
                                 x
                                 ·
                                 
                                    
                                       μ
                                    
                                    
                                       k
                                       (
                                       x
                                       )
                                    
                                 
                              
                           
                        that is strictly related to the cosine similarity.

Since the quantity of all entity pairs is very large, we adopted the implementation of spherical K-means given in the CLUTO [42] toolbox, which is specifically designed for working with very large input data sets.

The convergence of K-means can be in some cases quite slow, even if in the last iterations only a few points change the assigned cluster. To obtain a faster algorithm while preserving the good characteristics of the solution, we set the maximum number of iterations to 12.

The size of the clusters for different values of K is shown in Fig. 6
                         for the MEDFAR data set and in Fig. 7
                         for the SUBCAT data set.

We assume that only significantly large clusters can correspond to some semantic relations. Given the disparity in dimensions of the two data sets, we respectively consider a threshold of 6000 and 10,000. A good balance between a well-distributed size of clusters (minimal number of peaks) and minimal number of cluster (avoiding fragmentation) can be obtained for the smallest K for which about one third of the clusters are significantly large, and this condition is fulfilled for a value of 30 for MEDFAR and 50 for SUBCAT.

In the preceding section we showed how to obtain a clustering of the entity pairs data set. We want now to focus on the interpretation of such solution. We will focus on the SUBCAT case, which is richer and more interesting. As mentioned above, we interpret the smaller data sets as unrelated entity pairs. For the other ones, we consider a quantitative analysis based on silhouette in Section 7.1, and a qualitative analysis aiming at a semantic interpretation of clusters in Section 7.2.

An assessment of the resulting clustering can only be based on internal measures, because we do not have any annotated data to use as ground truth. We will adopt the silhouette 
                        [43] because it takes into account two aspects relevant in judging a clustering: how similar to each other elements of the same cluster are, and how dissimilar elements belonging to two different clusters are. In other words, the first aspect gives an idea of how compact a cluster is, while the second one evaluates how distinct from the other it is.

Given a generic point i in the data set, let us call a(i) the average dissimilarity of the point with the elements of the same cluster. A small a(i) means that the point is quite close to all the other points in the cluster.

Analogously, we define b(i) as the smallest average dissimilarity between i and the elements of any cluster different from the one i belongs. In fact, in this case we do not consider the average, because all the points in all the other clusters are likely to be scattered, and the average would not capture a common behaviour. On the contrary, we estimate how far the current point is from the closest point not in the same cluster. Both a(i) and b(i) are not negative real numbers.

The silhouette s(i) of each point is then defined as:
                           
                              (5)
                              
                                 s
                                 (
                                 i
                                 )
                                 =
                                 
                                    
                                       b
                                       (
                                       i
                                       )
                                       −
                                       a
                                       (
                                       i
                                       )
                                    
                                    
                                       max
                                       (
                                       a
                                       (
                                       i
                                       )
                                       ,
                                       b
                                       (
                                       i
                                       )
                                       )
                                    
                                 
                              
                           
                        where the opposite of a(i) is considered so that its effect is in favour of compactness. Its value lies in the interval 
                           [
                           −
                           1
                           ,
                           1
                           ]
                        , and the larger the silhouette the better the assignment of that point to its cluster. The silhouette is negative whenever the other points in the cluster are, on average, further from the point i than the closest point outside of the cluster. We used the inverse cosine similarity as a measure of dissimilarity.

Silhouette can then be averaged on all points of a cluster to assess the compactness of a cluster with respect to the others. In this case, a negative number of silhouette means that the diameter of the cluster is larger than the distance from the closest point out of the cluster. This can usually be the case if the number of points in the cluster is large while the whole data set is compact.

In Fig. 8
                         we report the silhouette values for all the clusters in the MEDFAR final solution, having K=30. Nearly all clusters show a positive silhouette value, with a few exceptions where the value is very close to zero. Some of the values are definitely larger than zero and one of them arrives at 0.3.

In Fig. 9
                         we report the silhouette values for all the clusters in the SUBCAT final solution, having K=50. Evidently two of the clusters, 0 and 9, have a very large silhouette, probably corresponding to two bunches of points which are quite separated from all the others. Nearly all the other clusters have a value of silhouette in 
                           [
                           −
                           0.2
                           ,
                           −
                           0.1
                           ]
                        , with the only exception of a few of them, the silhouette of which is even better, larger than −0.1. The large number of clusters with a slightly negative value for the silhouette suggests that the data-set would require a larger number of clusters. However, considering what reported in Table 7
                        , the number of clusters we chose seems to be adequate for the task at hand.

For the semantic interpretation of the clustering we can exploit the fact that a part of the features, namely the n-grams, are lexicalized and therefore correspond to words. We can therefore try to identify the features which better characterize each cluster to obtain some clues about the type of the relation which can be associated to the cluster.

We analysed the contribution of the single features. In particular, the contribution of each feature respectively to the internal and external similarity of each cluster is exploited to assess their descriptive and discriminative score. In other words, we consider the loss in internal and external similarity obtained removing a single feature, and repeating the process for each feature [42]. Features with the best scores are considered descriptive and discriminative characteristics of the cluster. In general, the two classes of features largely overlap, and therefore we consider the union of the two to construct the label of each cluster. We delete from each one of these sets all out-of-domain terms,
                           4
                        
                        
                           4
                           Out-of-domain words are the ones not included in the considered dictionaries.
                         because they are not useful to the task. The English translation of resulting labels for the SUBCAT data set are reported in Table 7.

Let us consider for example the cluster 24 which is characterized by the following features: unigrams 
                           =
                           {
                           anca
                           
                           (
                           hip
                           )
                           ,
                           protesi
                           (
                           prostheses
                           )
                           ,
                           
                           contusione
                           
                           (
                           contusion
                           )
                           }
                         and bigrams 
                           =
                           {
                           ancasinistra
                           
                           (
                           lefthip
                           )
                           ,
                           
                           ancadx
                           
                           (
                           righthip
                           )
                           }
                        . In this case a possible label for the relation type associated to the considered cluster could be given by the union of all these terms, all of them related to the entity hip.

One of the elements of this cluster is given by the example we considered in the Introduction:
                           
                              
                                 
                                    Riscontrata <ent1>frattura</ent1>sottocapitata
                                    
                                    e <ent2>lussazione</ent2>
                                 
                                    della <ent3>spalla</ent3>sinistra
                                    
                                    e <ent4>contusione</ent4>di
                                 
                                 <ent5>anca</ent5>destra
                                    
                                    da
                                    
                                    caduta
                                    
                                    accidentale
                                    
                                    al
                                    
                                    proprio
                                    
                                    domicilio
                                 
                                 .
                              
                           
                        or, in English: 
                           
                              
                                 
                                    Found subcapital <ent1>fracture</ent1>and <ent2>dislocation</ent2>
                                 
                                    of
                                    
                                    left <ent3>shoulder</ent3>and <ent4>contusion</ent4>of
                                    
                                    right
                                 
                                 <ent5>hip</ent5>caused
                                    
                                    by
                                    
                                    accidental
                                    
                                    fall
                                    
                                    at
                                    
                                    her
                                    
                                    home
                                 
                                 .
                              
                           
                        A relation has been correctly identified between entities 4 and 5 and its label refers to “hip”, which indeed is a body part, even if no explicit mention is done to the concept of illness. Even if the labelling is not accurate enough for a completely automatic tool, labels like this would represent a useful clue for an annotator.

@&#CONCLUSIONS AND FUTURE WORK@&#

In this paper we presented a system for the extraction of information from clinical records in Italian. A first part of the system aims to extract domain relevant entities from medical reports by a pattern matching approach. A second part takes the output of the former step and applies a clustering approach to explore possible relations between such entities. An analysis of the solution is carried out by both a quantitative and a qualitative approach. With the latter, we propose a method to obtain a semi-automatic semantic labelling of the clusters.

There are several directions in which the approach could be improved, but an important way is the search for a more effective feature set. For example, the introduction of the syntactic analysis (parse tree) would provide useful information to the detection of relations and the reconstruction of their label. In fact, a more complete analysis of the sentence could give important cues about how the information is organized [26,44]. The idea could be to apply a constituency or dependency parser to the sentence and then to use the parse tree together with the feature vector to represent the input for the clustering task. An algorithm similar to the one presented in [45] can then be adopted for clustering.

@&#SUMMARY@&#

This paper proposes and discusses the use of unsupervised text mining techniques for the extraction of information from clinical records written in Italian. In particular, it aims at extracting domain relevant entities and relations connecting them. To this end a complete system structured in two steps is proposed, where the former considers entities, and the latter relations.

In the first step, domain knowledge is exploited for domain entity (i.e., medical and pharmaceutical entities) identification and classification. Among all possible resources, the Unified Medical Language System conveys the medical domain specific terminology and a semantic network giving ontological knowledge about domain entities and relations. On the other hand, text processing is based on generic natural language tools freely available for Italian.

The entities extracted in the first step are then passed to an unsupervised machine learning approach (clustering) which has been chosen to avoid the need of annotating data. Clustering is applied to all the entity pairs identified, to discover possible relations existing between them. Too small clusters are likely to correspond to unrelated entity pairs, while larger clusters will correspond to different kinds of relations.

Crucially, also during this step we try to put as much knowledge as possible into the system by constructing an appropriate input representation. This is achieved by including in the representation an innovative type of features, namely barrier features, together with more traditional ones, that is the type of the entities and n-grams of words. The main characteristic of the barrier features is that they consider patterns of Part-of-Speech (PoS) tags to describe the surroundings of each entity.

The system has been tested on a fairly large dataset of clinical records, consisting of nearly 1000 medical records, investigating the variation in the performance adopting different similarity measures in the feature space, different clustering algorithms and different number of clusters. The results of our experiments show that the unsupervised approach proposed is promising when paired with an appropriate similarity measure as the cosine distance and an appropriate number of clusters.

The final solution has been then evaluated from both a quantitative and a qualitative point of view. For the former we adopt an internal measure, namely silhouette, because it takes into account two aspects relevant in judging a clustering: how similar to each other elements of the same cluster and how dissimilar elements belonging to two different clusters are. For the semantic interpretation of the clustering we exploit the fact that a part of the features are lexicalized and therefore correspond to words. We therefore identify the features which most contribute to the internal and external similarity of each cluster to obtain some clues about the type of the relation which can be associated to the cluster. Indeed, we manage to label most of the clusters with domain relevant concepts.

None declared.

@&#ACKNOWLEDGEMENT@&#

This work has been partially supported by the Smart Health 2.0 Project – PON04a2_C, funded by the Italian Government (MIUR) in the program 
                  PON R&C 2007-2014.

@&#REFERENCES@&#

