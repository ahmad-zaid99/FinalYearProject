@&#MAIN-TITLE@&#Structural learning of Bayesian networks by bacterial foraging optimization

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Reproduction selects elite individuals and realizes information transmission.


                        
                        
                           
                           Chemotaxis and elimination-and-dispersal maintain a balance between exploitation and exploration.


                        
                        
                           
                           Four operators serve as candidate directions for each bacterium to select.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Bacterial foraging optimization

Bayesian networks

Structural learning

Swarm intelligence

@&#ABSTRACT@&#


                  Algorithms inspired by swarm intelligence have been used for many optimization problems and their effectiveness has been proven in many fields. We propose a new swarm intelligence algorithm for structural learning of Bayesian networks, BFO-B, based on bacterial foraging optimization. In the BFO-B algorithm, each bacterium corresponds to a candidate solution that represents a Bayesian network structure, and the algorithm operates under three principal mechanisms: chemotaxis, reproduction, and elimination and dispersal. The chemotaxis mechanism uses four operators to randomly and greedily optimize each solution in a bacterial population, then the reproduction mechanism simulates survival of the fittest to exploit superior solutions and speed convergence of the optimization. Finally, an elimination and dispersal mechanism controls the exploration processes and jumps out of a local optima with a certain probability. We tested the individual contributions of four algorithm operators and compared with two state of the art swarm intelligence based algorithms and seven other well-known algorithms on many benchmark networks. The experimental results verify that the proposed BFO-B algorithm is a viable alternative to learn the structures of Bayesian networks, and is also highly competitive compared to state of the art algorithms.
               

@&#INTRODUCTION@&#

A Bayesian network (BN) is one of the most effective theoretical models to represent uncertainty of knowledge in artificial intelligence. A BN uses a graphical model to depict conditional independence relations among random variables in a domain and encode the joint probability distribution of random variables [1]. Given a BN and observations of some variables, the values of other unobserved variables can be predicted by probabilistic inference. Therefore, systems successfully use this paradigm to model practical problems in many different areas, such as medical diagnosis, natural language processing, forecasting, biology, and control [2].

Learning a BN structure automatically from data has received much attention, and variety of learning algorithms have been proposed [3–29]. These algorithms all adopt either the dependency analysis or score and search approaches. Dependency analysis is a constraint satisfaction problem, and employs a statistical method to judge dependency and independency relationships among variables and thereby constructs a BN [19]. Score and search is an optimization problem, and employs a search method to probe the space of BN structures and a metric to constantly evaluate each candidate network structure until the best metric value is obtained [14]. Unfortunately, both approaches have fatal drawbacks. Dependency analysis needs to perform an exponential number of dependency tests that are usually complex and unreliable, and it is hard to ensure learning quality. In contrast, learning a BN structure by score and search becomes an NP-hard problem as the number of variables increases [30]. Once the space of candidate networks becomes large, nearly all exact searches are inappropriate for BN structural learning. Although some heuristic algorithms, such as iterated local search [3], K2 [25], and hill climbing [26,27] algorithms can address the problem of large search spaces, they often become trapped in local optima.

To solve these problems, several stochastic algorithms based on global optimization mechanisms have been introduced for structural learning of BNs in recent years. These algorithms can be divided into two categories [31]: 1) The evolutionary algorithm, which draws inspiration from evolution and natural genetics and includes evolutionary programming, genetic algorithm, evolution strategy, and genetic programming. Evolutionary programming [4,5] and genetic algorithm [8,9,17] based methods are effective ways with which a BN structure can be successfully learned. 2) The swarm intelligence algorithm, which is a nature inspired optimization technology that consists of particle swarm optimization (PSO) [32,33], ant colony optimization (ACO) [34,35], artificial bee colony optimization (ABC) [36], and bacterial foraging optimization (BFO) [37]. ACO, ABC, and PSO have proven their effectiveness at learning a BN structure from the data [6,10,23]. Their common feature is the use of a meta-heuristic search mechanism to explore the BN structural space while a scoring metric is applied to evaluate the fitness of candidate networks.

BFO is a swarm intelligence algorithm developed by Passino in 2002 [37,38], which simulates the foraging behavior of Escherichia coli bacteria. The basic principle is that bacteria move through either tumbling or swimming to maximize the energy consumed by eating as many nutrients as they can. As the smallest creatures on earth, bacteria contain many clever optimization mechanisms. Thus, BFO has unique good performance, and has been successful in a wide variety of optimization tasks since it was proposed [39–44]. However, to date this optimization technology has not been applied to learning BN structures.

Existing structural learning methods PSO-B, ACO-B, and ABC-B (based on the PSO, ACO, and ABC swarm intelligence algorithms, respectively) have some latent drawbacks. PSO-B keeps track of two types of optimal solutions, which makes it easily trapped in local optima. ACO-B and ABC-B employ pheromones to construct solutions. Although the positive feedback mechanism behind the pheromone can effectively guide the search for superior solutions, if the pheromone is over used, it may overpower a better solution, and the risk of the algorithms becoming trapped in local optima is high. However, BFO does not contain mechanisms that make an algorithm easily trapped, and has a high probability of escaping from local optima. Hence, we propose a new BN structural learning method, BFO-B, based on BFO.

In BFO-B, each bacterium constantly looks for a network structure with a better metric value using three optimization mechanisms: chemotaxis, reproduction, and elimination and dispersal. The chemotaxis locally optimizes each feasible solution, reproduction applies survival of the fittest to candidate solutions, and elimination and dispersal allows jump out of a local optima. The three mechanisms maintain a balance between exploitation and exploration and make it possible to obtain a global optimal or near optimal solution. To verify BFO-B performance, we conducted a series of experiments on many benchmark networks, investigating the effects of key parameters on the algorithm performance, contributions of different mechanisms to the algorithm performance, and performance comparisons with two swarm intelligence based algorithms and seven other algorithm types. The experimental outcomes verify that BFO-B is a promising approach to learn BN structures from data, highly competitive compared with two state of the art swarm intelligence based methods, and significantly superior to other methods.

Section 2 of this paper briefly introduces BNs, the K2 scoring metric of BNs, and BFO. Section 3 presents the details of the BFO-B algorithm, and the verification experiments are described and outcomes are discussed in Section 4. Section 5 summarizes our conclusions and possible future directions.

A BN, also known as a belief network or a causal network, is a directed acyclic graph (DAG), which qualitatively characterizes the dependent and independent relationships among random variables, and uses a set of probability parameters to quantify the strength of the dependencies between each node and its parent nodes. It can be denoted as 
                           G
                           =
                           (
                           X
                           ,
                           A
                           )
                        , where 
                           X
                           =
                           {
                           
                              
                                 X
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 X
                              
                              
                                 2
                              
                           
                           ,
                           ⋯
                           ,
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                           ,
                           ⋯
                           ,
                           
                              
                                 X
                              
                              
                                 n
                              
                           
                           }
                         is a set of nodes, 
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                         is a random variable, 
                           A
                           =
                           {
                           
                              
                                 a
                              
                              
                                 i
                                 j
                              
                           
                           }
                         is a set of arcs, and 
                           
                              
                                 a
                              
                              
                                 i
                                 j
                              
                           
                         describes a direct dependence relationship between 
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                         and 
                           
                              
                                 X
                              
                              
                                 j
                              
                           
                        . A set of conditional probability parameters is also associated with each non-root node, 
                           P
                           
                              (
                              
                                 
                                    X
                                 
                                 
                                    i
                                 
                              
                              |
                              ∏
                              (
                              
                                 
                                    X
                                 
                                 
                                    i
                                 
                              
                              )
                              )
                           
                        , where 
                           ∏
                           (
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                           )
                         is a parent set of 
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                        , which quantifies how much 
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                         depends on its parents. Thus, a BN can be uniquely encoded using the joint probability distribution of the variable set 
                           X
                           =
                           {
                           
                              
                                 X
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 X
                              
                              
                                 2
                              
                           
                           ,
                           ⋯
                           ,
                           
                              
                                 X
                              
                              
                                 n
                              
                           
                           }
                        ,
                           
                              (1)
                              
                                 
                                    P
                                    (
                                    
                                       
                                          X
                                       
                                       
                                          1
                                       
                                    
                                    ,
                                    
                                       
                                          X
                                       
                                       
                                          2
                                       
                                    
                                    ,
                                    ⋯
                                    ,
                                    
                                       
                                          X
                                       
                                       
                                          n
                                       
                                    
                                    )
                                    =
                                    
                                       ∏
                                       
                                          i
                                          =
                                          1
                                       
                                       n
                                    
                                    P
                                    (
                                    
                                       
                                          X
                                       
                                       
                                          i
                                       
                                    
                                    |
                                    ∏
                                    (
                                    
                                       
                                          X
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    )
                                    .
                                 
                              
                           
                        
                     

For the score and search approach, the problem of learning a BN can be described as follows: given a scoring metric and a training set 
                           D
                           =
                           {
                           
                              
                                 v
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 v
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 v
                              
                              
                                 m
                              
                           
                           }
                         with m cases, where each 
                           
                              
                                 v
                              
                              
                                 i
                              
                           
                           ∈
                           D
                         is a fully instantiated set of n random variables, a search method constantly uses the scoring metric with respect to the training set to evaluate candidate network structures, until it obtains the network structure that best matches D.

A key aspect of the score and search approach is the scoring metric. We use the K2 metric, one of the most well-known Bayesian scoring methods, first used in the K2 algorithm [25]. The initial expression of the K2 metric is
                           
                              (2)
                              
                                 
                                    P
                                    (
                                    G
                                    :
                                    D
                                    )
                                    =
                                    P
                                    (
                                    G
                                    )
                                    
                                       ∏
                                       
                                          i
                                          =
                                          1
                                       
                                       n
                                    
                                    
                                       ∏
                                       
                                          j
                                          =
                                          1
                                       
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                       
                                    
                                    
                                       
                                          (
                                          
                                             
                                                r
                                             
                                             
                                                i
                                             
                                          
                                          −
                                          1
                                          )
                                          !
                                       
                                       
                                          (
                                          
                                             
                                                N
                                             
                                             
                                                i
                                                j
                                             
                                          
                                          +
                                          
                                             
                                                r
                                             
                                             
                                                i
                                             
                                          
                                          −
                                          1
                                          )
                                          !
                                       
                                    
                                    
                                       ∏
                                       
                                          k
                                          =
                                          1
                                       
                                       
                                          
                                             r
                                          
                                          
                                             i
                                          
                                       
                                    
                                    
                                       
                                          N
                                       
                                       
                                          i
                                          j
                                          k
                                       
                                    
                                    !
                                    ,
                                 
                              
                           
                         where 
                           
                              
                                 r
                              
                              
                                 i
                              
                           
                         is the number of possible value assignments of 
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                        , 
                           
                              
                                 q
                              
                              
                                 i
                              
                           
                         is the number of possible instantiations for 
                           ∏
                           (
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                           )
                        , 
                           
                              
                                 N
                              
                              
                                 i
                                 j
                                 k
                              
                           
                         is the number of cases in D when 
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                           =
                           
                              
                                 v
                              
                              
                                 i
                                 k
                              
                           
                         and 
                           ∏
                           (
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                           )
                         are instantiated with the jth configuration, and 
                           
                              
                                 N
                              
                              
                                 i
                                 j
                              
                           
                           =
                           
                              
                                 ∑
                              
                              
                                 k
                                 =
                                 1
                              
                              
                                 
                                    
                                       r
                                    
                                    
                                       i
                                    
                                 
                              
                           
                           
                              
                                 N
                              
                              
                                 i
                                 j
                                 k
                              
                           
                        .

Decomposability is a very important characteristic for a scoring metric, by which the scoring of the whole network structure can be transformed into the summed score of the local structures of each node. When a local structure of a node in a BN is changed, we only need to recalculate the scoring of this node. Thus, a decomposable scoring metric can greatly reduce the number of repeated calculations. To obtain a decomposable K2 metric, we use the logarithm of 
                           P
                           (
                           G
                           :
                           D
                           )
                         and ignore the constant, 
                           
                              log
                           
                           P
                           (
                           G
                           )
                        , when assuming a uniform prior for P(G) [6]. We express the decomposable K2 metric as
                           
                              (3)
                              
                                 
                                    f
                                    (
                                    G
                                    :
                                    D
                                    )
                                    =
                                    
                                       log
                                    
                                    (
                                    P
                                    (
                                    G
                                    :
                                    D
                                    )
                                    )
                                    ≈
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       n
                                    
                                    f
                                    (
                                    
                                       
                                          X
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    ∏
                                    (
                                    
                                       
                                          X
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    )
                                    ,
                                 
                              
                           
                         where 
                           f
                           (
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                           ,
                           ∏
                           (
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                           )
                           )
                         represents the K2 score of each node and is formally defined as
                           
                              (4)
                              
                                 
                                    f
                                    (
                                    
                                       
                                          X
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    ∏
                                    (
                                    
                                       
                                          X
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    )
                                    =
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       
                                          
                                             q
                                          
                                          
                                             i
                                          
                                       
                                    
                                    
                                       (
                                       
                                          log
                                       
                                       
                                          (
                                          
                                             
                                                (
                                                
                                                   
                                                      r
                                                   
                                                   
                                                      i
                                                   
                                                
                                                −
                                                1
                                                )
                                                !
                                             
                                             
                                                (
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      i
                                                      j
                                                   
                                                
                                                +
                                                
                                                   
                                                      r
                                                   
                                                   
                                                      i
                                                   
                                                
                                                −
                                                1
                                                )
                                                !
                                             
                                          
                                          )
                                       
                                       +
                                       
                                          ∑
                                          
                                             k
                                             =
                                             1
                                          
                                          
                                             
                                                r
                                             
                                             
                                                i
                                             
                                          
                                       
                                       
                                          log
                                       
                                       (
                                       
                                          
                                             N
                                          
                                          
                                             i
                                             j
                                             k
                                          
                                       
                                       !
                                       )
                                       )
                                    
                                    .
                                 
                              
                           
                        
                     

Because the joint probability is less than 1, the decomposable K2 score using 
                           
                              log
                           
                           (
                           P
                           (
                           G
                           :
                           D
                           )
                           )
                         is always negative. Thus, the best BN structure is that with the highest K2 score regardless of which search algorithm is used.

Foraging strategies are used by animals and microbes to locate, handle, and ingest food. Natural selection tends to favor those species with good foraging strategies and eliminate those with poor foraging strategies. The species with poor foraging strategies cannot obtain enough food to enable them to reproduce, so after many generations, they are either eliminated or develop good foraging strategies. Inspired by this evolutionary principle, Passino developed a BFO algorithm based on the foraging behavior of E. coli bacteria present in the human gut [37,38]. The BFO algorithm has been applied to various real-world optimization problems such as data mining [39], harmonic estimation [41], edge detection [43], and RFID network planning [44] and has shown its effectiveness.

BFO is an iteration algorithm where each bacterium represents a feasible solution of the optimization problem. It starts with a population of bacteria, randomly generated during an initialization phase. Then the bacterial population tries to find an optimal solution by three nested loop mechanisms: chemotaxis, reproduction, and elimination and dispersal. BFO combines exploitation and exploration processes. In particular, each bacterium performs exploitation processes in the chemotactic steps and controls exploration processes with a certain probability in the elimination and dispersal steps. The reproduction steps, which select superior and eliminate inferior individuals, are directly analogous with the selection mechanism of classical evolutionary algorithms and can provide fast convergence of the bacterial population near the optima [45]. The BFO algorithm is summarized in Algorithm 1
                        , and a brief introduction to the three mechanisms is given below for the case of finding the maximum of an objective function.

Chemotaxis simulates the movement of E. coli through tumbling and swimming via flagella. A bacterium tumbles in a random direction, searching for food. If food is abundant in the selected direction, the bacterium will swim in this direction until the food supply worsens or the bacterium reaches the specified steps, which is called chemotaxis. Bacterium movement can be expressed as
                              
                                 (5)
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       (
                                       j
                                       +
                                       1
                                       ,
                                       k
                                       ,
                                       l
                                       )
                                       =
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       (
                                       j
                                       ,
                                       k
                                       ,
                                       l
                                       )
                                       +
                                       C
                                       (
                                       i
                                       )
                                       ϕ
                                       (
                                       i
                                       )
                                       ,
                                    
                                 
                              
                            where 
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              (
                              j
                              ,
                              k
                              ,
                              l
                              )
                            represents the position of the ith bacterium at the jth chemotaxis, kth reproduction, and lth elimination and dispersal step; 
                              C
                              (
                              i
                              )
                            is the step size in the random direction for bacterium i; and 
                              ϕ
                              (
                              i
                              )
                            is the unit length in the random direction. When 
                              J
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              (
                              j
                              +
                              1
                              ,
                              k
                              ,
                              l
                              )
                              )
                              >
                              J
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              (
                              j
                              ,
                              k
                              ,
                              l
                              )
                              )
                           , the ith bacterium will swim another step of size 
                              C
                              (
                              i
                              )
                            in the same direction according to Eq. (5). Swimming continues until the bacterium either reaches the maximum number of steps, 
                              
                                 
                                    N
                                 
                                 
                                    s
                                 
                              
                           , or the objective function value decreases.

The bacteria grow longer in accordance with the increase in the absorption of nutrients in the chemotactic steps. Under appropriate conditions, some will die, and others that have obtained adequate nutrients will divide to form two daughters. To model this phenomenon, let the number of chemotactic steps, 
                              
                                 
                                    N
                                 
                                 
                                    c
                                 
                              
                           , be the lifetime of the bacterium, S be the number of the bacterial population, and 
                              
                                 
                                    S
                                 
                                 
                                    r
                                 
                              
                              =
                              S
                              /
                              2
                            be the number of bacteria that have accumulated adequate nutrients to cope for themselves. After 
                              
                                 
                                    N
                                 
                                 
                                    c
                                 
                              
                            chemotactic steps, a reproduction step is instigated. The bacterial population is sorted in descending order according to the value of a health function. A larger health value indicates a healthier bacterium, so each of the 
                              
                                 
                                    S
                                 
                                 
                                    r
                                 
                              
                            healthiest bacteria split into two bacteria, which are placed at the same location, while the remaining 
                              
                                 
                                    S
                                 
                                 
                                    r
                                 
                              
                            bacteria die, to maintain a constant population size. The health function is used to compute the accumulated objective value of the ith bacterium over its lifetime, and is defined as
                              
                                 (6)
                                 
                                    
                                       
                                          
                                             J
                                          
                                          
                                             
                                                health
                                             
                                          
                                          
                                             i
                                          
                                       
                                       =
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          
                                             
                                                
                                                   N
                                                
                                                
                                                   c
                                                
                                             
                                             +
                                             1
                                          
                                       
                                       J
                                       (
                                       i
                                       ,
                                       j
                                       ,
                                       k
                                       ,
                                       l
                                       )
                                       ,
                                    
                                 
                              
                            where 
                              
                                 
                                    J
                                 
                                 
                                    
                                       health
                                    
                                 
                                 
                                    i
                                 
                              
                            represents the health value of the ith bacterium, 
                              J
                              (
                              i
                              ,
                              j
                              ,
                              k
                              ,
                              l
                              )
                              =
                              J
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              (
                              j
                              ,
                              k
                              ,
                              l
                              )
                              )
                           .

With changes to the local environment that a population of bacteria lives in, all of the bacteria may be killed or a group of bacteria may disperse into a new environment to find better food sources. To simulate this phenomenon, an elimination and dispersal step is taken after 
                              
                                 
                                    N
                                 
                                 
                                    r
                                    e
                                 
                              
                            reproduction steps. Each bacterium in the population may be eliminated or dispersed to a new location with probability 
                              
                                 
                                    P
                                 
                                 
                                    e
                                    d
                                 
                              
                           . The new location is randomly initialized over the solution space, so this may or may not place bacteria near good solutions. Let 
                              
                                 
                                    N
                                 
                                 
                                    e
                                    d
                                 
                              
                            be the number of elimination and dispersal steps. Generally, 
                              
                                 
                                    N
                                 
                                 
                                    c
                                 
                              
                              ≥
                              
                                 
                                    N
                                 
                                 
                                    r
                                    e
                                 
                              
                              ≥
                              
                                 
                                    N
                                 
                                 
                                    e
                                    d
                                 
                              
                           , i.e., a bacterial population will experience many chemotactic steps before a reproduction step, and several reproduction steps before an elimination and dispersal step.

We propose a new score and search algorithm for learning BNs using BFO. The proposed algorithm uses a bacterial population to search in the candidate network space and the K2 metric to evaluate the obtained networks until it finds the network with the highest K2 score.

To use the BFO algorithm to learn BNs, we must define some basic components.

The solution space for learning BNs is composed of all possible DAGs. Each bacterium in the population models a feasible solution and is initialized to a DAG with fewer arcs. The bacterial population then explores in the search space to find an optimal or near optimal network structure, as identified by the K2 metric. That is, the objective function is the K2 metric Eq. (3), and the search goal is to find a network structure with the highest K2 score.

Each initial solution is generated by an iterative process. Starting from an empty graph with no arcs (
                              
                                 
                                    G
                                 
                                 
                                    0
                                 
                              
                           ), arcs that are not already in the current graph are added one by one to the solution if and only if the score of the new solution is larger than the previous graph, and the new solution satisfies the DAG constraint. This process is repeated until the number of arcs reaches the number specified in advance. Thus, the solution initialization constructs a set of starting points in the search space. To save time, each solution is initialized to a simple graph with limited edges and subsequently optimized using the mechanisms discussed above.

Addition, deletion, reversion, and move operators are four candidate directions for each bacterium to select in the chemotactic process. A bacterium tries to perform each of the operators and selects the one that increases K2 score the most (equivalent to tumbling). The bacterium continues with the selected operator (equivalent to swimming) until the K2 score of the new solution no longer improves or the bacterium has performed the maximum steps, 
                              
                                 
                                    N
                                 
                                 
                                    s
                                 
                              
                           . Essentially, the operators are four different local optimization operators. Addition, deletion, and reversion are simple standard operators in this domain and only change one edge of a candidate solution each time, which offers a relatively small range of optimization around a solution. The move operator exchanges the parent set of two existing edges in a solution and can cause a relatively large change to a solution. Thus, if a solution is not improved using the three simple operators, it may improve with the move operator. Swimming is a driving force toward a better solution using the same local optimization operator, which becomes more frequent as a bacterium approaches a better solution. Tumbling controls the change among different local optimization operators, which becomes more frequent as a bacterium moves away from a solution to search for a better one. The chemotaxis mechanism is a complex and close combination of swimming and tumbling that keeps bacteria in these places with higher scores for BN structures and plays a crucial role in searching for the best BN structures.

As shown in Fig. 1
                           , a bacterium 
                              
                                 
                                    G
                                 
                                 
                                    h
                                 
                              
                           , which represents a DAG with h arcs, attempts deletion, addition, reversion, and move and obtains new solutions 
                              
                                 
                                    G
                                 
                                 
                                    h
                                    −
                                    1
                                 
                              
                           , 
                              
                                 
                                    G
                                 
                                 
                                    h
                                    +
                                    1
                                 
                              
                           , 
                              
                                 
                                    G
                                 
                                 
                                    h
                                 
                                 
                                    ′
                                 
                              
                           , and 
                              
                                 
                                    G
                                 
                                 
                                    h
                                 
                                 
                                    ″
                                 
                              
                           , respectively. Assuming the K2 score of 
                              
                                 
                                    G
                                 
                                 
                                    h
                                    +
                                    1
                                 
                              
                            is the highest, then this bacterium will pick 
                              
                                 
                                    G
                                 
                                 
                                    h
                                    +
                                    1
                                 
                              
                           , and continue to test the same operator to obtain new solution, 
                              
                                 
                                    G
                                 
                                 
                                    h
                                    +
                                    2
                                 
                              
                           . If the K2 score of 
                              
                                 
                                    G
                                 
                                 
                                    h
                                    +
                                    2
                                 
                              
                            is still larger than for 
                              
                                 
                                    G
                                 
                                 
                                    h
                                    +
                                    1
                                 
                              
                           , it will continue to perform the same operator. This process repeats until the K2 score no longer increases or 
                              
                                 
                                    N
                                 
                                 
                                    s
                                 
                              
                            addition operators have been performed, i.e., 
                              m
                              =
                              
                                 
                                    N
                                 
                                 
                                    s
                                 
                              
                           .

We summarize the chemotactic process of a bacterium into function Chemotaxis_Process(
                           i
                           ), where 
                              K
                              2
                              (
                              i
                              ,
                              j
                              ,
                              k
                              ,
                              l
                              )
                            represents the score when the ith bacterium is at the jth chemotaxis, kth reproduction, and lth elimination and dispersal step:
                              
                                 
                              
                           
                        

The four operators are
                              
                                 •
                                 Addition: Randomly selects two nodes, 
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                     and 
                                       
                                          
                                             x
                                          
                                          
                                             j
                                          
                                       
                                    , where 
                                       i
                                       ≠
                                       j
                                     and 
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       ∉
                                       ∏
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             j
                                          
                                       
                                       )
                                    . If adding an arc 
                                       
                                          
                                             a
                                          
                                          
                                             i
                                             j
                                          
                                       
                                       =
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       →
                                       
                                          
                                             x
                                          
                                          
                                             j
                                          
                                       
                                     in 
                                       
                                          
                                             G
                                          
                                          
                                             h
                                          
                                       
                                     it does not generate a directed cycle, and a new solution 
                                       
                                          
                                             G
                                          
                                          
                                             h
                                             +
                                             1
                                          
                                       
                                       =
                                       
                                          
                                             G
                                          
                                          
                                             h
                                          
                                       
                                       ⋃
                                       
                                          
                                             a
                                          
                                          
                                             i
                                             j
                                          
                                       
                                     is obtained.

Deletion: Randomly selects an arc, 
                                       
                                          
                                             a
                                          
                                          
                                             i
                                             j
                                          
                                       
                                    , from node 
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                     to 
                                       
                                          
                                             x
                                          
                                          
                                             j
                                          
                                       
                                     in 
                                       
                                          
                                             G
                                          
                                          
                                             h
                                          
                                       
                                    , and deletes it. A new solution, 
                                       
                                          
                                             G
                                          
                                          
                                             h
                                             −
                                             1
                                          
                                       
                                       =
                                       
                                          
                                             G
                                          
                                          
                                             h
                                          
                                       
                                       ∖
                                       
                                          
                                             a
                                          
                                          
                                             i
                                             j
                                          
                                       
                                    , is obtained.

Reversion: Randomly selects an arc, 
                                       
                                          
                                             a
                                          
                                          
                                             i
                                             j
                                          
                                       
                                    , from node 
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                     to 
                                       
                                          
                                             x
                                          
                                          
                                             j
                                          
                                       
                                     in 
                                       
                                          
                                             G
                                          
                                          
                                             h
                                          
                                       
                                    . If reversing the direction of the arc 
                                       
                                          
                                             a
                                          
                                          
                                             i
                                             j
                                          
                                       
                                     still satisfies the DAG constraint, then 
                                       
                                          
                                             G
                                          
                                          
                                             h
                                          
                                       
                                       =
                                       
                                          
                                             G
                                          
                                          
                                             h
                                          
                                       
                                       ∖
                                       
                                          
                                             a
                                          
                                          
                                             i
                                             j
                                          
                                       
                                       ⋃
                                       
                                          
                                             a
                                          
                                          
                                             j
                                             i
                                          
                                       
                                    .

Move: For two nodes 
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                     and 
                                       
                                          
                                             x
                                          
                                          
                                             j
                                          
                                       
                                     with non-empty parent sets, this operator selects a parent node for each of the two nodes, 
                                       
                                          
                                             x
                                          
                                          
                                             k
                                          
                                       
                                       ∈
                                       ∏
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                       )
                                     and 
                                       
                                          
                                             x
                                          
                                          
                                             l
                                          
                                       
                                       ∈
                                       ∏
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             j
                                          
                                       
                                       )
                                     
                                    
                                       (
                                       k
                                       ≠
                                       l
                                       )
                                    , then exchanges 
                                       
                                          
                                             x
                                          
                                          
                                             k
                                          
                                       
                                     with 
                                       
                                          
                                             x
                                          
                                          
                                             l
                                          
                                       
                                     if 
                                       
                                          
                                             x
                                          
                                          
                                             l
                                          
                                       
                                       ∈
                                       
                                          (
                                          X
                                          ∖
                                          
                                             (
                                             ∏
                                             (
                                             
                                                
                                                   x
                                                
                                                
                                                   i
                                                
                                             
                                             )
                                             ⋃
                                             
                                                
                                                   x
                                                
                                                
                                                   i
                                                
                                             
                                             )
                                          
                                          )
                                       
                                    , 
                                       
                                          
                                             x
                                          
                                          
                                             k
                                          
                                       
                                       ∈
                                       
                                          (
                                          X
                                          ∖
                                          
                                             (
                                             ∏
                                             (
                                             
                                                
                                                   x
                                                
                                                
                                                   j
                                                
                                             
                                             )
                                             ⋃
                                             
                                                
                                                   x
                                                
                                                
                                                   j
                                                
                                             
                                             )
                                          
                                          )
                                       
                                    . This move operation satisfies the DAG constraint, i.e., the move operator simultaneously modifies the parent sets of two nodes.

Reproduction employs a health function to calculate the accumulated K2 score of each bacterium over 
                              
                                 
                                    N
                                 
                                 
                                    c
                                 
                              
                            chemotaxis steps, then sorts the bacterial population in descending order by their health value and picks the 
                              
                                 
                                    S
                                 
                                 
                                    r
                                 
                              
                            healthiest bacteria to reproduce. Each of the healthiest bacteria splits into two bacteria with the same network structures. The remaining 
                              
                                 
                                    S
                                 
                                 
                                    r
                                 
                              
                            least healthy bacteria are abandoned. Essentially, this mechanism provides information transmission among individuals, which is a basis to implement swarm intelligence methods. It represents a fairly abstract model of Darwinian evolution and biological genetics in genetic algorithms and acquires the elite information for delivery among swarm agents. The elite selection criterion in this mechanism is based on the fitness sum over the whole life of a bacterium, i.e., the average fitness of a bacterium in the chemotaxis process. This selection method may lead to the case where a bacterium in the last step of its life found the best solution so far, yet it still dies in the reproduction phase. However, from another perspective, it also has the effect of preventing the population from falling into local optima. Unlike the hill climbing algorithm, this selection method picks superior individuals according to multiple chemotaxis optimization processes (i.e., the average fitness) rather than picking the best every time, making it easier to escape from local optima.

The health function is
                              
                                 (7)
                                 
                                    
                                       K
                                       
                                          
                                             2
                                          
                                          
                                             
                                                health
                                             
                                          
                                          
                                             i
                                          
                                       
                                       =
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          
                                             
                                                
                                                   N
                                                
                                                
                                                   c
                                                
                                             
                                             +
                                             1
                                          
                                       
                                       K
                                       2
                                       (
                                       i
                                       ,
                                       j
                                       ,
                                       k
                                       ,
                                       l
                                       )
                                       ,
                                    
                                 
                              
                            where 
                              K
                              
                                 
                                    2
                                 
                                 
                                    
                                       health
                                    
                                 
                                 
                                    i
                                 
                              
                            is the health value of the ith bacterium. A larger health value indicates that the corresponding bacterium has achieved larger K2 scores during its 
                              
                                 
                                    N
                                 
                                 
                                    c
                                 
                              
                            chemotactic steps and hence is more likely to reproduce.

To explain the reproduction step clearly, let us take a population of four bacteria 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          i
                                       
                                       
                                          1
                                       
                                    
                                 
                              
                           , 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          i
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                           , 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          i
                                       
                                       
                                          3
                                       
                                    
                                 
                              
                           , and 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          i
                                       
                                       
                                          4
                                       
                                    
                                 
                              
                            as an example. The process is shown in Fig. 2
                           . First, the bacteria are sorted in descending order on the basis of their health value, obtaining 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          i
                                       
                                       
                                          3
                                       
                                    
                                 
                              
                           , 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          i
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                           , 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          i
                                       
                                       
                                          4
                                       
                                    
                                 
                              
                           , and 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          i
                                       
                                       
                                          1
                                       
                                    
                                 
                              
                           , i.e., 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          i
                                       
                                       
                                          3
                                       
                                    
                                 
                              
                            and 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          i
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                            are the healthiest bacteria, 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          i
                                       
                                       
                                          4
                                       
                                    
                                 
                              
                            and 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          i
                                       
                                       
                                          1
                                       
                                    
                                 
                              
                            the least healthy. 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          i
                                       
                                       
                                          4
                                       
                                    
                                 
                              
                            and 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          i
                                       
                                       
                                          1
                                       
                                    
                                 
                              
                            are discarded, and 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          i
                                       
                                       
                                          3
                                       
                                    
                                 
                              
                            and 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          i
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                            reproduce themselves to create two 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          i
                                       
                                       
                                          3
                                       
                                    
                                 
                              
                            and two 
                              
                                 
                                    G
                                 
                                 
                                    
                                       
                                          i
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                           .

The elimination and dispersal mechanism is invoked after 
                              
                                 
                                    N
                                 
                                 
                                    r
                                    e
                                 
                              
                            reproduction steps. Each bacterium in the population is subjected to an elimination and dispersal step with probability 
                              
                                 
                                    P
                                 
                                 
                                    e
                                    d
                                 
                              
                            
                           
                              (
                              0
                              <
                              
                                 
                                    P
                                 
                                 
                                    e
                                    d
                                 
                              
                              <
                              1
                              )
                            and the rule is
                              
                                 (8)
                                 
                                    
                                       G
                                       =
                                       
                                          {
                                          
                                             
                                                
                                                   
                                                      
                                                         G
                                                      
                                                      
                                                         ′
                                                      
                                                   
                                                
                                                
                                                   
                                                      if
                                                   
                                                   
                                                   q
                                                   <
                                                   
                                                      
                                                         P
                                                      
                                                      
                                                         e
                                                         d
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   G
                                                
                                                
                                                   
                                                      otherwise
                                                   
                                                
                                             
                                          
                                       
                                       ,
                                    
                                 
                              
                            where q is a random number uniformly distributed in 
                              [
                              0
                              ,
                              1
                              ]
                           , G is the current solution associated with a bacterium and 
                              
                                 
                                    G
                                 
                                 
                                    ′
                                 
                              
                            is a new solution obtained by solution initialization. For each bacterium, if the random number is smaller than 
                              
                                 
                                    P
                                 
                                 
                                    e
                                    d
                                 
                              
                           , it moves to a new initial solution; otherwise, the solution remains unchanged. This mechanism generates new solutions for some bacteria and makes these bacteria search from new starting points, which ensures new search regions are explored across the search space. Thus, the mechanism helps the bacterial population jump out of local optima.

The proposed BFO-B algorithm is shown in Algorithm 2
                        . It starts with an initial population of DAGs, randomly generated by solution initialization, and iteratively performs the three principal mechanisms (chemotaxis, reproduction, and elimination and dispersal) to search for networks with higher scores. BFO-B uses chemotaxis and elimination and dispersal to balance between exploitation and exploration. It performs exploitation processes to local solutions in the chemotaxis phase and exploration processes in the elimination and dispersal phase. Chemotaxis is a major driving force that provides local optimization, where the bacteria try attain larger K2 scores, and can be viewed as a biased random walk or stochastic hill climbing. Elimination and dispersal places some bacteria in new regions, which causes a certain amount of destruction of the accumulating chemotactic progress, but it also has the positive effect of assisting chemotaxis because the dispersal may help bacteria jump out of local optimal solutions and obtain a global optima. Therefore, elimination and dispersal is consider to be a chemotactic mobile behavior at the population level. Reproduction provides information transmission among the whole population and picks the elite individuals with higher scores. This process can accelerate convergence, and the special selection method based on average fitness will, to some extent, avoid the algorithm converging to the local best solutions.

We conducted experiments to study the performance of the BFO-B algorithm and compared it with nine well-known algorithms on many benchmark networks.

An algorithm for learning BNs is generally evaluated by testing it on datasets generated from benchmark networks by probabilistic logic sampling. In our experiments, we use twelve benchmark networks of different sizes, as detailed in Table 1
                        , including their source and domain descriptions. Alarm is the most common network used in the literature for learning BN structures from data. Insurance, Child, and Asia are also relatively common networks. We also used two tiled networks, Alarm3 and Child3, which are separately composed of two common networks, Alarm and Child, by tiling three copies of themselves [15]. The tiling is performed in a way that maintains the structural and probabilistic properties of the original network in the tiled network. The aim of using tiled networks is to check the performance of an algorithm as the number of variables increases, while the difficulty of learning the network remains the same. All of the datasets used in the experiments are generated from the networks by probabilistic logic sampling, as shown in Table 2
                        , where the datasets (D), the original BNs (G) that generate the datasets, the number of cases in D, the number of nodes in G, the number of arcs in G, and the K2 scores for the true network structures are listed for reference. Some datasets originate from the same network but have different data volume. For example, eight datasets are generated from the most common Alarm network. Using datasets from the same network, we can check the algorithm performance as data volume increases. The experimental platform was a PC with Core 2, 2.13 GHz CPU, 2.99 GB RAM, and Windows XP. The proposed BFO-B algorithm was implemented in the Java language.

The score and search approach can evaluate the learned results based on the scores (e.g. the K2 metric), and/or structural difference, i.e., the number of different arcs in the learned network compared with the true network. The higher the K2 score or the smaller the structural difference implies a better outcome. However, the network with the highest K2 score is not necessarily the same network that has the smallest structural difference. Here, we are primarily concerned with researching search algorithms, so we consider the K2 metric as the primary way to evaluate the learned results.

In the experiments, we first analyzed the BFO-B parameter influences by empirical testing and chose appropriate values for each parameter. To probe the BFO-B algorithm details, we investigated the contributions of different mechanisms to algorithm performance. BFO-B was then run on all the datasets in Table 2 to test its performance. Finally, using various metrics, we compared BFO-B with nine well-known methods, including six score and search algorithms, two dependency analysis algorithms, and a hybrid method combing score and search and dependency analysis.

We used statistical analysis [46] to provide levels of confidence in our comparisons. Kolmogorov–Smirnov tests showed that our results do not follow a Gaussian distribution. Therefore, we used Kruskal–Wallis tests to perform nonparametric analysis of the results. The Friedman test also provides nonparametric analysis between more than two samples. However, the Friedman test is only valid for correlated samples, whereas the Kruskal–Wallis test may be applied to independent samples. In the experiments, the BFO-B algorithm and each comparison algorithm were independently run, so there is no correlation. Hence, we selected the Kruskal–Wallis test. To demonstrate the differences between the proposed algorithm and any of the comparison algorithms, we present all the statistical results whether the difference is significant or not. We apply the Kruskal–Wallis test to perform paired tests with the confidence level 
                           95
                           %
                        , i.e., the probability of producing the difference by chance is not greater than 
                           5
                           %
                        . If the p-value obtained from the test is less than 
                           5
                           %
                        , we consider that a significant difference exists in the corresponding experimental results.

There are six parameters, S, 
                           
                              
                                 N
                              
                              
                                 s
                              
                           
                        , 
                           
                              
                                 N
                              
                              
                                 c
                              
                           
                        , 
                           
                              
                                 N
                              
                              
                                 r
                                 e
                              
                           
                        , 
                           
                              
                                 N
                              
                              
                                 e
                                 d
                              
                           
                        , and 
                           
                              
                                 P
                              
                              
                                 e
                                 d
                              
                           
                         for the bacterial population size, number of swimming steps, number of chemotactic steps, number of reproduction steps, number of elimination and dispersal steps, and the elimination and dispersal probability, respectively. Fortunately, some BFO-B parameters are easily determined. For example, the number of swimming along the same operator in the chemotaxis phase is always less than five, and the algorithm performance is best when 
                           
                              
                                 N
                              
                              
                                 s
                              
                           
                           =
                           4
                         (from test experiments). Since 
                           
                              
                                 N
                              
                              
                                 c
                              
                           
                           ≥
                           
                              
                                 N
                              
                              
                                 r
                                 e
                              
                           
                           ≥
                           
                              
                                 N
                              
                              
                                 e
                                 d
                              
                           
                        , 
                           
                              
                                 N
                              
                              
                                 r
                                 e
                              
                           
                         and 
                           
                              
                                 N
                              
                              
                                 e
                                 d
                              
                           
                         also have constrained values. Thus, for our experiments, we set 
                           
                              
                                 N
                              
                              
                                 s
                              
                           
                           =
                           4
                        , 
                           
                              
                                 N
                              
                              
                                 r
                                 e
                              
                           
                           =
                           4
                        , and 
                           
                              
                                 N
                              
                              
                                 e
                                 d
                              
                           
                           =
                           3
                        .

In general, swarm intelligence based algorithms have good robustness, and are less sensitive to the parameters. Hence, we select only one dataset (Alarm-2000), which is generated from the most common Alarm network, as an example of the process of determining the remaining three parameters: S, 
                           
                              
                                 N
                              
                              
                                 e
                                 d
                              
                           
                        , and 
                           
                              
                                 P
                              
                              
                                 e
                                 d
                              
                           
                        . Ten candidate values of each parameter were chosen. The value of each parameter was changed while keeping the other parameters fixed and 10 independent trials performed for each set of parameters. The evaluation metrics used are
                           
                              •
                              HKS: the Highest K2 Score over all trials. The higher the corresponding value, the better the learned network.

SSS: the K2 Scoring of the learned network with the Smallest Structural difference over all trials.

LKS: the Lowest K2 Score over all trials. The higher the corresponding mean value, the better the solution performance of the algorithm.

AKS: the Average K2 Score over all trials. The higher the corresponding mean value, the better the solution performance of the algorithm.

AET: the Average Execution Time over all trials. The smaller the corresponding mean value, the better the time performance of the algorithm.


                        (A) Size of the bacterial population (
                        S
                        )
                         The size of a population plays an important role in the search process of a swarm intelligence algorithm. To probe the effect of this parameter on our algorithm performance and select an appropriate value, we performed BFO-B 10 times using each candidate value of S, as summarized in Table 3
                        . AKS for 
                           S
                           ≥
                           80
                         is significantly superior to 
                           S
                           <
                           80
                        . Moreover, HKS, SSS, and LKS show that BFO-B can not only consistently find networks with the highest K2 scores (−9717.46), but also all the scores are between the highest score (−9717.46) and the score of the network with the smallest structural difference (−9720.09) when 
                           S
                           ≥
                           80
                        . Thus, larger S is better for the algorithm, because when there are more bacteria to search for the best network in a search space, it is more likely that the optimal or near optimal network structure will be found. To differentiate the most appropriate value from candidate values 
                           S
                           ≥
                           80
                        , i.e., 80, 90, 100, 110, and 120, we performed pair-wise Kruskal–Wallis tests on the learning results, as shown in Table 4
                        , in the form of p-values, where 
                           p
                           >
                           0.05
                         indicates that the two values of S have no statistically significant different effect on the performance and vice versa. All p-values are greater than 0.05, indicating that there are no statistically significant differences between the five values. However, running time increases as S increases, as shown in Table 3, and so we would prefer a smaller value. Thus, balancing between finding a better network and spending less time, we set size of the bacterial population to 80.


                        (B) Chemotactic steps (
                        
                           
                              
                                 N
                              
                              
                                 c
                              
                           
                        
                        )
                         Chemotaxis is a key process, and so 
                           
                              
                                 N
                              
                              
                                 c
                              
                           
                         is an important parameter for BFO-B. We tested BFO-B with 10 different candidate values of 
                           
                              
                                 N
                              
                              
                                 c
                              
                           
                        , as shown in Table 5
                        . From HKS, the BFO-B algorithm generally finds the network with the highest score (−9717.46) when 
                           
                              
                                 N
                              
                              
                                 c
                              
                           
                           ≥
                           20
                        ; SSS shows that BFO-B finds the network with the smallest structural difference (−9720.09) when 
                           
                              
                                 N
                              
                              
                                 c
                              
                           
                           =
                           25
                           ,
                           30
                        , or ≥40; and LKS shows that BFO-B obtains the highest value (−9720.09) when 
                           
                              
                                 N
                              
                              
                                 c
                              
                           
                           =
                           30
                           ,
                           45
                        , and 60. Thus, the BFO-B algorithm shows good performance when 
                           
                              
                                 N
                              
                              
                                 c
                              
                           
                           =
                           30
                           ,
                           45
                        , or 60 for all three statistics. To identify the most appropriate value, we selected five candidate values of 
                           30
                           ,
                           40
                           ,
                           45
                           ,
                           50
                        , and 60 and performed pair-wise Kruskal–Wallis tests, as shown in Table 6
                        . All p-values are greater than 0.05, indicating there are no statistically significant differences between the five values. However, AET shows that running time generally increases as 
                           
                              
                                 N
                              
                              
                                 c
                              
                           
                         increases, and so we set 
                           
                              
                                 N
                              
                              
                                 c
                              
                           
                           =
                           30
                        .


                        (C) Elimination and dispersal probability (
                        
                           
                              
                                 P
                              
                              
                                 e
                                 d
                              
                           
                        
                        )
                         
                        
                           
                              
                                 P
                              
                              
                                 e
                                 d
                              
                           
                         controls the exploratory search in the elimination and dispersal process. To select an appropriate value for 
                           
                              
                                 P
                              
                              
                                 e
                                 d
                              
                           
                        , we tested BFO-B with 10 different candidate values, as shown in Table 7
                        . AKS shows that the performance when 
                           
                              
                                 P
                              
                              
                                 e
                                 d
                              
                           
                           =
                           0.10
                           ,
                           0.12
                           ,
                           0.14
                        , and 0.15 is superior to the other cases. HKS, SSS, and LKS show that the BFO-B algorithm not only finds the network with the highest score (−9717.46) and that with the smallest structural difference (−9720.09), but also all of the scores of the 10 runs are between the highest score (−9717.46) and the score of the network with the smallest structural difference (−9720.09) for those 
                           
                              
                                 P
                              
                              
                                 e
                                 d
                              
                           
                         cases. Although 
                           
                              
                                 P
                              
                              
                                 e
                                 d
                              
                           
                           =
                           0.18
                         returned 9 of 10 runs with the highest K2 score (−9717.46), which is better than the four cases above, the average score is inferior. Thus, 
                           
                              
                                 P
                              
                              
                                 e
                                 d
                              
                           
                         either too large or too small adversely affects BFO-B performance. If 
                           
                              
                                 P
                              
                              
                                 e
                                 d
                              
                           
                         is too small, there will be fewer bacteria to perform the elimination and dispersal step, making it more difficult to overcome a local optima. On the other hand, if 
                           
                              
                                 P
                              
                              
                                 e
                                 d
                              
                           
                         is too large, while more bacteria will restart to search for the optimal solution, there is also increasing risk of missing the optimal solution. To further select the most appropriate value from the five candidates (
                           0.10
                           ,
                           0.12
                           ,
                           0.14
                           ,
                           0.15
                        , and 0.18), we performed pair-wise Kruskal–Wallis tests, as shown in Table 8
                        . All p-values are greater than 0.05, indicating that there are no statistically significant differences between the five candidate values. Since 
                           
                              
                                 P
                              
                              
                                 e
                                 d
                              
                           
                           =
                           0.10
                         shows the best time performance among the five cases, we selected 
                           
                              
                                 P
                              
                              
                                 e
                                 d
                              
                           
                           =
                           0.10
                        .

Summarizing, the BFO-B parameters used in our experiments were: 
                           
                              
                                 N
                              
                              
                                 s
                              
                           
                           =
                           4
                        , 
                           
                              
                                 N
                              
                              
                                 r
                                 e
                              
                           
                           =
                           4
                        , 
                           
                              
                                 N
                              
                              
                                 e
                                 d
                              
                           
                           =
                           3
                        , 
                           S
                           =
                           80
                        , 
                           
                              
                                 N
                              
                              
                                 c
                              
                           
                           =
                           30
                        , and 
                           
                              
                                 P
                              
                              
                                 e
                                 d
                              
                           
                           =
                           0.1
                        .

The proposed BFO-B algorithm combines several different components that work together to obtain the final outcomes. To gain additional insight into the inner workings of the algorithm, we probed individual contributions of the different components, i.e., the inherent characteristics of BFO-B. Three datasets generated from different networks were chosen to validate the effects of different components. When choosing the datasets, we considered popular networks of medium size: Alarm, Insurance, and Child. Thus, we selected three datasets generated from Alarm, Insurance, and Child: Alarm-2000, Insurance-3000, and Child-2000, respectively. Chemotaxis is a critical driving force for the BFO algorithm, without which BFO cannot work properly. Therefore, we used the chemotaxis mechanism as the essential mechanism of the algorithm and explored the individual contributions of the other four operators. Specifically, the four variations are
                           
                              •
                              BFO-B1: remove the reproduction process from BFO-B.

BFO-B2: remove the elimination and dispersal process from BFO-B.

BFO-B3: remove the move operator from BFO-B.

BFO-B4: remove the swimming process from BFO-B.

SSD: the Smallest Structural Difference over all trials, i.e., the smallest number of arcs wrongly added, deleted and reversed over all trials. The smaller the corresponding value, the better the learned network.

BSD: the Biggest Structural Difference over all trials.

ASD: the Average overall Structural Difference over all trials.

Both AKS and ASD show that the BFO-B variants achieve significantly inferior results than the whole BFO-B algorithm on all three datasets. Thus, the corresponding four components are all effective in assisting the BFO-B algorithm to learn a better solution. The reasons for their effectiveness can be concluded as follows: (1) Reproduction takes charge of information transmission among individuals, which is a key factor for a swarm intelligence method to obtain a better solution. (2) Elimination and dispersal makes some individuals search for a better solution in new regions, which plays a main role in global search, and is important for the algorithm to maintain a balance between global exploration and local exploitation. (3) Compared with the three standard operators (addition, deletion, and reversion), move makes a bigger change on a candidate solution, which offers more chances to assist the algorithm in escaping from a local optima. (4) Under a certain number of chemotaxis steps, the swimming process makes it possible to perform more explorations around a candidate solution and thereby is useful for the algorithm. From Table 9, we also observe that BFO-B1 and BFO-B2 have less difference from the whole BFO-B algorithm for AKS and ASD, which indicates that although reproduction and elimination and dispersal have positive roles in enhancing the performance of the algorithm, chemotaxis is the most essential component for the BFO-B algorithm. BFO-B3 and BFO-B4 show larger differences from the whole BFO-B algorithm. Since the move operator and swimming process are two key strategies in chemotaxis, removing either of them is equivalent to damaging the chemotaxis process. Hence the size of these differences provides further evidence of the importance of chemotaxis to the proposed BFO-B algorithm.

Thus, we conclude that reproduction, elimination and dispersal, move, and swimming processes are essential components for BFO-B to efficiently learn a BN structure from data.

To fully test the performance of the proposed BFO-B algorithm, we ran 20 independent trials on all the datasets in Table 2, as summarized in Table 10
                         for K2 score and time performance, and Table 11
                         for structural difference between the learned and the true networks. The new evaluation metrics used in Tables 10 and 11 are defined below, and the meanings of the other items are as in Sections 4.2 and 4.3.
                           
                              •
                              KLQ: the Lower Quartile of the K2 scoring over all trials.

SLQ: the Lower Quartile of overall Structural difference over all trials sorted.

AAD: the Average Difference of arcs incorrectly Added over all trials, i.e., the average number of the arcs wrongly added over all trials.

ADD: the Average Difference of arcs incorrectly Deleted over all trials.

ARD: the Average Difference of arcs incorrectly Reversed over all trials.

The difference between HKS and LKS is small (Table 10), which indicates that the BFO-B algorithm is stable for all of the datasets. Also, KLQ is equal to the corresponding HKS for Alarm-2000, Alarm-5000, Child-2000, Child-5000, Credit-3000, Asia-1000, Asia-5000, EngineFuelSystem-10000, and Studfarm-10000, which means that the proposed BFO-B algorithm returns the best networks on at least 15 of the 20 runs on these datasets. HKS and LKS are the same for Asia-5000, EngineFuelSystem-10000, and Studfarm-10000, which means that BFO-B obtains exactly the same results over 20 runs. Thus, the proposed BFO-B algorithm performs well for relatively smaller networks. HKS, KLQ, and AKS for relatively large networks (Child3-5000, Alarm3-5000, Brain-10000, Win95pts-50000, and Hepar II-50000) are higher than for the corresponding true networks (Table 2), which shows that even for larger networks, the BFO-B algorithm is capable of finding networks with higher K2 scores (i.e., the networks which most match the datasets), provided they exist in the search space.

Mean and standard deviations of AAD, ADD, ARD and ASD (Table 11) are relatively small for datasets generated from the nine smaller networks (Alarm, Insurance, Child, Credit, Asia, EngineFuelSystem, Boeralge92, Studfarm, and Tank). The best values (numbers in parentheses) are not more than 3, except for Alarm-1000, which does not contain enough cases to correctly learn a BN structure. It is interesting that all of the statistics on Studfarm-10000 are 0, indicating that BFO-B always found the true network over 20 runs. Thus, the proposed BFO-B algorithm finds networks with structures very similar to the true structures for smaller networks. For datasets Child3-5000, Alarm3-5000, Brain-10000, Win95pts-50000, and Hepar II-50000, generated from larger networks, the BFO-B algorithm does not perform well, but still attains relatively good results on Child3-5000, Alarm3-5000 and Brain-10000. For Win95pts-50000 and Hepar II-50000, the BFO-B algorithm obtains networks with more incorrect arcs compared to the true networks, because the Win95pts and Hepar II networks have much more complex network structures (some nodes have more parent nodes or child nodes), and 50000 available cases does not fully reflect their network characteristics. From the Alarm, Insurance, Child, and Credit datasets, BFO-B outcomes improve as data volume increases. This illustrates that larger data volumes are more conducive to correctly learning the underlying network structures. The tiled networks, Alarm and Child, show that BFO-B performance decreases as the number of variables increases while the difficulty of learning the network remains constant.

Execution times for the BFO-B algorithm on different datasets generated from the Alarm network is shown in Fig. 3
                         in the form of a box plot, where the top and the bottom of each box indicate the 75th and 25th percentiles, respectively; the line in each box indicates the 50th percentile; the whisker bars below and above each box indicate the 10th and 90th percentiles, respectively; and the squares and asterisks in each box indicate the mean and outliers, respectively. The mean values are all close to 50th percentile for all datasets except Alarm-2000 and Alarm-5000, and execution time fluctuates weakly on each dataset, which indicates that the BFO-B algorithm runs stably. Execution time grows relatively slowly with increasing sample capacity, suggesting that the BFO-B algorithm is capable of handling relatively large datasets.

Considering K2 scores, structural differences and execution time, we conclude that the proposed BFO-B algorithm can find network structures with higher K2 scores and smaller structural differences, and relatively stable runtime efficiency. Thus, the proposed BFO-B algorithm appears to be a more promising algorithm for learning BNs in the field of big data.

We compared the proposed BFO-B algorithm with the nine well-known algorithms: ABC-B [10], ACO-B [6], GA-B [9], HCST-B [26], SC [21], GES [28], PC [29], TPDA [19] and MMHC [15]. These algorithms incorporate almost all the different classes of methods for learning BN structures from data and are all well-known methods in the domain of BN structural learning:
                           
                              •
                              ACO-B and ABC-B are state of the art swarm intelligence based methods recently developed that simulate foraging behavior of real ants and honeybees, respectively.

GA-B is a classic evolution based algorithm, which makes use of a genetic algorithm to search for node ordering of the system variables and then evaluates the quality of the variable ordering with the K2 algorithm.

HCST-B is a simple and common heuristic based algorithm that uses a hill climbing algorithm to construct network structures.

SC constrains the search space by restricting the parents of each variable to belong to a small subset of candidates and also makes use of a hill climbing algorithm to learn network structures.

GES provides a new implementation of the search space, using equivalence classes as states in a greedy search.

PC is a prototypical dependency analysis algorithm.

TPDA employs the concept of mutual information to test for conditional independencies.

MMHC is a hybrid method combing dependency analysis and score and search, which uses conditional independence tests to identify the skeleton of the network structure and then orients the skeleton using a greedy hill climbing search.

To ensure fair comparison, the population sizes of the three population based algorithms (ABC-B, ACO-B, and GA-B) were set to 80. The other specific parameters of these algorithms and the HCST-B algorithm conformed to the best settings as reported in their original papers, which were tuned by a series of experiments. We obtained software implementations of the SC, PC, TPDA, and MMHC algorithms in the Causal Explorer system,
                           1
                        
                        
                           1
                           Available at http://www.dsl-lab.org/causal_explorer/.
                         and software implementation of the GES algorithm from the TETRAD Project.
                           2
                        
                        
                           2
                           Available at http://www.phil.cmu.edu/projects/tetrad/index.html.
                         We used the default values in the software implementations for the parameters of these algorithms. Each algorithm was executed for 20 independent runs over each dataset, as summarized in Tables 12 and 13
                        
                         for K2 score and time aspects, along with the detailed parameter values for some of the algorithms. Since PC and TPDA are based on independency analysis, which do not use the scoring mechanism in the learning process, but to uniformly compare the different algorithms, we computed the scores of the final networks learned by them. Tables 14 and 15
                        
                         show the structural difference outcomes, where the numbers in parentheses represent the best result over 20 runs. The meanings of all of the metrics in Tables 12–15 are as described variously above, and the best value for each metric except AET is shown in bold. We also performed pair-wise Kruskal–Wallis tests for the BFO-B algorithm against each of the other nine algorithms on each dataset to determine significant differences, as shown in Fig. 4(a) and 4(b)
                         for K2 scores and overall structural difference, respectively. The red line is the benchmark, 
                           p
                           =
                           0.05
                        . When the results of two algorithms are exactly the same over all runs, p is infinite, and we denote this case with “NA” in Fig. 4.


                        (A) K2 scores
                         The three swarm intelligence based algorithms, BFO-B, ABC-B, and ACO-B, obtain the highest scores (in bold) with respect to the four metrics (HKS, LKS, KLQ, and AKS) on all eight datasets (Tables 12 and 13), which illustrates the superiority of swarm intelligence based approaches over score and search approaches used by the other algorithms. A scoring metric was used to measure the degree of matching between the learned network and the dataset. Thus, in spite of not using the scoring mechanism for the PC and TPDA algorithms, we are still able to measure their outcomes as inferior to the three swarm intelligence algorithms. The reasons for the inferior obtained by the GA-B, HCST-B, SC, GES, MMHC, PC, and TPDA algorithms are
                           
                              •
                              Although GA-B is a global optimization method, it searches the node ordering using a genetic algorithm, which is easily trapped into local optima, especially when the node number is large.

Although the HCST-B, SC, GES and MMHC algorithms have different concrete implementations, they all look for better networks using a greedy hill climbing search, which is a local optimization technology, and can only ensure these algorithms find local optimal solutions.

PC and TPDA make use of statistical or information theory measures to judge whether certain arcs between variables exist. This type of method is unreliable for finding better networks.


                        Fig. 4(a) shows the BFO-B algorithm is significantly superior to the seven non-swarm intelligence algorithms (GA-B, HCST-B, SC, GES, PC, TPDA, and MMHC) on all eight datasets (except GA-B on Child-2000), with higher K2 scores and 
                           p
                           <
                           0.05
                         (except for BFO-B and GA-B on Child-2000). BFO-B performs significantly better than ABC-B on Insurance-3000 and ACO-B on Alarm-6000 and Insurance-3000 (
                           p
                           <
                           0.05
                        ). However, there are no significant differences between BFO-B and either of ABC-B and ACO-B for the other cases.

Hence, the proposed BFO-B algorithm can guarantee the discovery of better quality networks, with higher scores than the seven non-swarm intelligence algorithms (GA-B, HCST-B, SC, GES, PC, TPDA, and MMHC), and are not lower than the two swarm intelligence algorithms, ABC-B and ACO-B.


                        (B) Structure differences 
                         The results in terms of structure difference are similar to those in terms of scores (Tables 14 and 15). The swarm intelligence algorithms (BFO-B, ABC-B, and ACO-B) have smaller structural differences than the other seven algorithms, GA-B, HCST-B, SC, GES, PC, TPDA, and MMHC, according to the structural difference metrics (AAD, ADD, ARD, ASD, and SLQ) in the majority of situations. The proposed BFO-B algorithm obtains the smallest ASD on six of the eight datasets (Alarm-6000, Insurance-3000, Studfarm-10000, Boerlage92-10000, Brain-10000, and Hepar II-50000).

There are several situations where the structural differences are too large. For example, the MMHC and PC algorithms are respectively 140 and 790 for the Brain-10000 data, and the SC, PC and TPDA algorithms are respectively 155, 119, and 117 for the Hepar II-50000 data. These results indicate that the corresponding algorithms completely failed to learn the network structures from the datasets. Two factors may be the cause of such poor outcomes: the default parameters are potentially unsuitable for the corresponding algorithms on these datasets (Brain-10000 and Hepar II-50000); and/or the networks (Brain and Hepar II) may be more complex, and the number of cases (10000 and 50000) are not large enough for the corresponding algorithms to correctly learn better network structures.


                        Fig. 4(b) shows that the BFO-B algorithm is significantly superior to HCST, SC, GES, PC, TPDA, and MMHC, on all eight datasets (except GES on Asia-5000 and HCST-B on Hepar II-50000), achieving the smallest ASD, and the corresponding p-values are all less than 0.05. BFO-B also performs significantly better than GA-B on half of the datasets (Alarm-6000, Insurance-3000, Studfarm-10000, and Boerlage92-10000), ABC-B also on (a different) half of the datasets (Alarm-6000, Insurance-3000, Brain-10000, and Hepar II-50000), and ACO-B on three datasets (Alarm-6000, Insurance-3000, and Boerlage92-10000). For the other cases, there are no significant differences between BFO-B and GA-B, ABO-B, and ACO-B. Hence, the proposed BFO-B algorithm can obtain networks with smaller structural differences compared with the other algorithms.


                        (C) Time performance 
                         
                        Tables 12 and 13 show that the four population based algorithms BFO-B, ABC-B, ACO-B, and GA-B generally take more time than the non-population based algorithms (HCST-B, SC, GES, PC, TPDA, and MMHC), because these four algorithms have many candidate solutions to be optimized. The GA-B algorithm also needs more time than the three swarm intelligence algorithms BFO-B, ABC-B, and ACO-B, because it performs both genetic and K2 algorithms each iteration.

To further study the time performance of BFO-B among swarm intelligence based algorithms, we compared the three swarm intelligence algorithms on eight datasets generated from the Alarm network, as shown in Fig. 5
                        , where the execution time is an average of 20 independent runs. BFO-B performs the worst among the three algorithms on Alarm-2000 and Alarm-3000. This is because the solution initialization and optimization phases are separated, in contrast to ABC-B and ACO-B, which merge optimization processes into their solution construction phases. Thus, when the sample capacity is small, BFO-B spends a relatively high proportion of its time on initialization, which leads to an overall longer running time. However, as the sample capacity increases, BFO-B spends most of its time on optimization, and initialization time becomes trivial, as shown in Fig. 5. BFO-B performance improves, and time performance stays between that of ABC-B and ACO-B when the sample capacity equals or exceeds 3000. However, because ABC-B has two information exchange mechanisms among artificial bees, it can quickly perform solution optimization and obtains the best time performance.

Summarizing, we conclude that the proposed BFO-B algorithm is capable of finding networks that is superior to those obtained by the non-swarm intelligence algorithms (GA-B, HCST-B, SC, GES, PC, TPDA, and MMHC) and is highly competitive compared with networks obtained by the two state of the art swarm intelligence based algorithms (ABC-B and ACO-B). Therefore, the proposed BFO-B algorithm appears to be a promising method for learning BN structures from data.

@&#CONCLUSIONS@&#

We proposed a new swarm intelligence algorithm, BFO-B, to learn the structures of BNs. The proposed algorithm uses BFO to search the optimal network structure in the network space and the K2 metric to guide the search process. The novelty of this method is its application of basic BFO to the structural learning of BNs, which not only enriches the application of the BFO-B algorithm but also provides a new way to learn BN structure in uncertain artificial intelligence. We performed a series of experiments to demonstrate the performance of the BFO-B algorithm in many domains and compared the algorithm with nine representative methods: ACO-B, ABC-B, GA-B, HCST-B, SC, GES, PC, TPDA, and MMHC. The proposed algorithm BFO-B algorithm is an effective algorithm to accurately learn a BN structure from the data.

In future work, we will continue to study the optimization mechanisms of BFO to improve BFO-B time performance, and extend our study to more complex optimization problems, such as dynamic BN learning problems and BN learning problems with incomplete data or large scale nodes.

@&#ACKNOWLEDGEMENTS@&#

This work was partly supported by the NSFC Research Program (61375059, 61332016), the National “973” Key Basic Research Program of China (2014CB744601), the Specialized Research Fund for the Doctoral Program of Higher Education (20121103110031), and the Beijing Municipal Education Research Plan key project (Beijing Municipal Fund Class B) (KZ201410005004).

@&#REFERENCES@&#

