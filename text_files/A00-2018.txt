The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for &quot;tag&quot;), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c).
Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g., whether it is a noun phrase (np), verb-phrase, etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.
Much of the interesting work is determining what goes into H (c).
Whenever it is clear to which constituent we are referring we omit the (c) in, e.g., h(c).
In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.
In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.
The method that gives the best results, however, uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6,10,15].
The method we use follows that of [10].
In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence of one or more such symbols.
(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)
For us the non-terminal symbols are those of the tree-bank, augmented by the symbols aux and auxg, which have been assigned deterministically to certain auxiliary verbs such as &quot;have&quot; or &quot;having&quot;.
For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).
M(c) is the constituent from which the head lexical item h is obtained according to deterministic rules that pick the head of a constituent from among the heads of its children.
To the left of M is a sequence of one or more left labels Li (c) including the special termination symbol A, which indicates that there are no more symbols to the left, and similarly for the labels to the right, Ri(c).
Thus an expansion e(c) looks like: The expansion is generated by guessing first M, then in order L1 through L,„.+1 (= A), and similarly for RI through In a pure Markov PCFG we are given the left-hand side label 1 and then probabilistically generate the right-hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right-hand side itself.
In the simplest of such models, a zeroorder Markov grammar, each label on the righthand side is generated conditioned only on / — that is, according to the distributions p(Li j1), p(M I 1), and p(Ri I 1).
More generally, one can condition on the m previously generated labels, thereby obtaining an mth-order Markov grammar.
So, for example, in a second-order Markov PCFG, L2 would be conditioned on L1 and M. In our complete model, of course, the probability of each label in the expansions is also conditioned on other material as specified in Equation 1, e.g., p(e t, h, H).
Thus we would use p(L2 I L1, M, 1, t, h, H).
Note that the As on both ends of the expansion in Expression 2 are conditioned just like any other label in the expansion.
The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model, and how to smooth the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning.
For example, in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1, M,1, t, h, H).
Also, remember that H is a placeholder for any other information beyond the constituent c that may be useful in assigning c a probability.
In the past few years the maximum entropy, or log-linear, approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1,17].
A complete review of log-linear models is beyond the scope of this paper.
Rather, we concentrate on the aspects of these models that most directly influenced the model presented here.
To compute a probability in a log-linear model one first defines a set of &quot;features&quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.
In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.
In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.
For example, in computing the probability of the head's pre-terminal t we might want a feature schema f (t, 1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1, and zero otherwise.
This feature is obviously composed of two sub-features, one recognizing t, the other 1.
If both return 1, then the feature returns 1.
Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability, the higher the absolute value of the associated A.
The function Z(H), called the partition function, is a normalizing constant (for fixed H), so the probabilities over all a sum to one.
Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a, H) for 0 < i < j: Here go(a, H) = 11Z (H) and gi(a, H) = eAi(a,H) fi(°,11).
The intuitive idea is that each factor gi is larger than one if the feature in question makes the probability more likely, one if the feature has no effect, and smaller than one if it makes the probability less likely.
Maximum-entropy models have two benefits for a parser builder.
First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable — just change the set of features used.
This point is emphasized by Ratnaparkhi in discussing his parser [17).
Second, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another.
This is useful if one is using a loglinear model for smoothing.
That is, suppose we want to compute a conditional probability p(a b, c), but we are not sure that we have enough examples of the conditioning event b, c in the training corpus to ensure that the empirically obtained probability P (a I b, c) is accurate.
The traditional way to handle this is also to compute P(a b), and perhaps P(a c) as well, and take some combination of these values as one's best estimate for p(a I b, c).
This method is known as &quot;deleted interpolation&quot; smoothing.
In max-entropy models one can simply include features for all three events f1 (a, b, c), f2 (a, b), and f3(a, c) and combine them in the model according to Equation 3, or equivalently, Equation 4.
The fact that the features are very far from independent is not a concern.
Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.
As it stands, this last equation is pretty much content-free.
But let us look at how it works for a particular case in our parsing scheme.
Consider the probability distribution for choosing the pre-terminal for the head of a constituent.
In Equation 1 we wrote this as p(t I 1, H).
As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &quot;before&quot;), and the label of the grandparent of c (la).
That is, we wish to compute p(t 1, lp, tp, lb, lg, hp).
We can now rewrite this in the form of Equation 5 as follows: Here we have sequentially conditioned on steadily increasing portions of c's history.
In many cases this is clearly warranted.
For example, it does not seem to make much sense to condition on, say, hp without first conditioning on ti,.
In other cases, however, we seem to be conditioning on apples and oranges, so to speak.
For example, one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling, or the grandparent label.
One way to do this is to modify the simple version shown in Equation 6 to allow this: Note the changes to the last three terms in Equation 7.
Rather than conditioning each term on the previous ones, they are now conditioned only on those aspects of the history that seem most relevant.
The hope is that by doing this we will have less difficulty with the splitting of conditioning events, and thus somewhat less difficulty with sparse data.
We make one more point on the connection of Equation 7 to a maximum entropy formulation.
Suppose we were, in fact, going to compute a true maximum entropy model based upon the features used in Equation 7, Ii (t,1), f2(t,1,1p), f3(t,1,lp) .
...
This requires finding the appropriate Ais for Equation 3, which is accomplished using an algorithm such as iterative scaling [II] in which values for the Ai are initially &quot;guessed&quot; and then modified until they converge on stable values.
With no prior knowledge of values for the Ai one traditionally starts with Ai = 0, this being a neutral assumption that the feature has neither a positive nor negative impact on the probability in question.
With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.
We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.
(Our experience is that rather than requiring 50 or so iterations, three suffice.)
Now we observe that if we were to use a maximum-entropy approach but run iterative scaling zero times, we would, in fact, just have Equation 7.
The major advantage of using Equation 7 is that one can generally get away without computing the partition function Z(H).
In the simple (content-free) form (Equation 6), it is clear that Z(H) = 1.
In the more interesting version, Equation 7, this is not true in general, but one would not expect it to differ much from one, and we assume that as long as we are not publishing the raw probabilities (as we would be doing, for example, in publishing perplexity results) the difference from one should be unimportant.
As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly.
Naturally, the distributions required by Equation 7 cannot be used without smoothing.
In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17].
While we could have smoothed in the same fashion, we choose instead to use standard deleted interpolation.
(Actually, we use a minor variant described in [4].)
We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.
As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.
For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.
This allows the second pass to see expansions not present in the training corpus.
We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.
We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.
As noted above, the probability model uses five smoothed probability distributions, one each for Li, M,Ri,t, and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7.
The other four equations can be found in a longer version of this paper available on the author's website (www.cs.brown.eduhiec).
L and R are conditioned on three previous labels so we are using a third-order Markov grammar.
Also, the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.
This is due to the importance of this factor in parsing, as noted in, e.g., [14].
In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning).
Performance on the test corpus is measured using the standard measures from [5,9,10,17].
In particular, we measure labeled precision (LP) and recall (LR), average number of crossbrackets per sentence (CB), percentage of sentences with zero cross brackets (OCB), and percentage of sentences with < 2 cross brackets (2CB).
Again as standard, we take separate measurements for all sentences of length < 40 and all sentences of length < 100.
Note that the definitions of labeled precision and recall are those given in [9] and used in all of the previous work.
As noted in [5], these definitions typically give results about 0.4% higher than the more obvious ones.
The results for the new parser as well as for the previous top-three individual parsers on this corpus are given in Figure 1.
As is typical, all of the standard measures tell pretty much the same story, with the new parser outperforming the other three parsers.
Looking in particular at the precision and recall figures, the new parser's give us a 13% error reduction over the best of the previous work, Co1199 [9].
In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach, the aspect of the parser that is most novel.
However, we do not think this aspect is the sole or even the most important reason for its comparative success.
Here we list what we believe to be the most significant contributions and give some experimental results on how well the program behaves without them.
We take as our starting point the parser labled Char97 in Figure 1 [5], as that is the program from which our current parser derives.
That parser, as stated in Figure 1, achieves an average precision/recall of 87.5.
As noted in [5], that system is based upon a &quot;tree-bank grammar&quot; - a grammar read directly off the training corpus.
This is as opposed to the &quot;Markovgrammar&quot; approach used in the current parser.
Also, the earlier parser uses two techniques not employed in the current parser.
First, it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads.
(It is &quot;soft&quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)
Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &quot;correct&quot;, and statistics were collected on the resulting parses.
Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.
In this section we evaluate the effects of the various changes we have made by running various versions of our current program.
To avoid repeated evaluations based upon the testing corpus, here our evaluation is based upon sentences of length < 40 from the development corpus.
We note here that this corpus is somewhat more difficult than the &quot;official&quot; test corpus.
For example, the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.
This is indicated in Figure 2, where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.
This is in accord with our experience that developmentcorpus results are from 0.3% to 0.5% lower than those obtained on the test corpus.
The model labeled &quot;Old&quot; attempts to recreate the Char97 system using the current program.
It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments), it does not guess the pre-terminal before guessing the lexical head, and it uses a tree-bank grammar rather than a Markov grammar.
This parser achieves an average precision/recall of 86.2%.
This is consistent with the average precision/recall of 86.6% for [5] mentioned above, as the latter was on the test corpus and the former on the development corpus.
Between the Old model and the Best model, Figure 2 gives precision/recall measurements for several different versions of our parser.
One of the first and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three.
As already noted, Char97 first guesses the lexical head of a constituent and then, given the head, guesses the PCFG rule used to expand the constituent in question.
In contrast, the current parser first guesses the head's pre-terminal, then the head, and then the expansion.
It turns out that usefulness of this process had already been discovered by Collins [10], who in turn notes (personal communication) that it was previously used by Eisner [12].
However, Collins in [10] does not stress the decision to guess the head's pre-terminal first, and it might be lost on the casual reader.
Indeed, it was lost on the present author until he went back after the fact and found it there.
In Figure 2 we show that this one factor improves performance by nearly 2%.
It may not be obvious why this should make so great a difference, since most words are effectively unambiguous.
(For example, part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].)
We believe that two factors contribute to this performance gain.
The first is simply that if we first guess the pre-terminal, when we go to guess the head the first thing we can condition upon is the pre-terminal, i.e., we compute p(h I t).
This quantity is a relatively intuitive one (as, for example, it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it, in effect, as the unsmoothed probability upon which all smoothing of p(h) is based.
This one &quot;fix&quot; makes slightly over a percent difference in the results.
The second major reason why first guessing the pre-terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion.
For example, when we first guess the lexical head we can move from computing p(r I 1,1p, h) to p(r I /, t, /p, h).
So, e.g., even if the word &quot;conflating&quot; does not appear in the training corpus (and it does not), the &quot;ng&quot; ending allows our program to guess with relative security that the word has the vbg pre-terminal, and thus the probability of various rule expansions can be considerable sharpened.
For example, the tree-bank PCFG probability of the rule &quot;VP --+ vbg np&quot; is 0.0145, whereas once we condition on the fact that the lexical head is a vbg we get a probability of 0.214.
The second modification is the explicit marking of noun and verb-phrase coordination.
We have already noted the importance of conditioning on the parent label /p.
So, for example, information about an np is conditioned on the parent — e.g., an s, vp, pp, etc.
Note that when an np is part of an np coordinate structure the parent will itself be an np, and similarly for a vp.
But nps and vps can occur with np and vp parents in non-coordinate structures as well.
For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.
Note that the subordinate vp has a vp parent.
Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.
A vp coordinate structure is defined here as a constituent with two or more vp children, one or more of the constituents comma, cc, conjp (conjunctive phrase), and nothing else; coordinate np phrases are defined similarly.
Something very much like this is done in [15].
As shown in Figure 2, conditioning on this information gives a 0.6% improvement.
We believe that this is mostly due to improvements in guessing the sub-constituent's pre-terminal and head.
Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.
Next we add the less obvious conditioning events noted in our previous discussion of the final model — grandparent label lg and left sibling label /b.
When we do so using our maximum-entropy-inspired conditioning, we get another 0.45% improvement in average precision/recall, as indicated in Figure 2 on the line labeled &quot;MaxEnt-Inspired'.
Note that we also tried including this information using a standard deleted-interpolation model.
The results here are shown in the line &quot;Standard Interpolation&quot;.
Including this information within a standard deleted-interpolation model causes a 0.6% decrease from the results using the less conventional model.
Indeed, the resulting performance is worse than not using this information at all.
Up to this point all the models considered in this section are tree-bank grammar models.
That is, the PCFG grammar rules are read directly off the training corpus.
As already noted, our best model uses a Markov-grammar approach.
As one can see in Figure 2, a firstorder Markov grammar (with all the aforementioned improvements) performs slightly worse than the equivalent tree-bank-grammar parser.
However, a second-order grammar does slightly better and a third-order grammar does significantly better than the tree-bank parser.
