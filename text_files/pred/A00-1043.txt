 Human professionals also often reuse the input documents to generate summaries; however, rather than simply extracting sentences and stringing them together, as most current summarizers do, humans often &quot;edit&quot; the extracted sentences in some way so that the resulting summary is concise and coherent. We analyzed a set of articles and identified six major operations that can be used for editing the extracted sentences, including removing extraneous phrases from an extracted sentence, combining a reduced sentence with other sentences, syntactic transformation, substituting phrases in an extracted sentence with their paraphrases, substituting phrases with more general or specific descriptions, and reordering the extracted sentences (Jing and McKeown, 1999; Jing and McKeown, 2000). We use the term &quot;phrase&quot; here to refer to any of the above components that can be removed in reduction. The following example shows an original sentence and its reduced form written by a human professional: Original sentence: When it arrives sometime next year in new TV sets, the V-chip will give parents a new and potentially revolutionary device to block out programs they don't want their children to see. The reduction system uses multiple sources of knowledge to make reduction decisions, including syntactic knowledge, context, and statistics computed from a training corpus. The program achieved a success rate of 81.3%, meaning that 81.3% of reduction decisions made by the system agreed with those of humans. Finally, we The goal of sentence reduction is to &quot;reduce without major loss&quot;; that is, we want to remove as many extraneous phrases as possible from an extracted sentence so that it can be concise, but without detracting from the main idea the sentence conveys. The program, called the decomposition program, matches phrases in a human-written summary sentence to phrases in the original document (Jing and McKeown, 1999). The human-written abstracts were collected from the free daily news service &quot;Communicationsrelated headlines&quot;, provided by the Benton Foundation (http://www.benton.org). We use the English Slot Grammar(ESG) parser developed at IBM (McCord, 1990) to analyze the syntactic structure of an input sentence and produce a sentence parse tree. The ESG parser not only annotates the syntactic category of a phrase (e.g., &quot;np&quot; or &quot;vp&quot;), it also annotates the thematic role of a phrase (e.g., &quot;subject&quot; or &quot;object&quot;). Each following step annotates each node in the parse tree with additional information, such as syntactic or context importance, which are used later to determine which phrases (they are represented as subtrees in a parse tree) can be considered extraneous and thus removed. For instance, for a sentence, the main verb, the subject, and the object(s) are essential if they exist, but a prepositional phrase is not; for a noun phrase, the head noun is essential, but an adjective modifier of the head noun is not. For example, for the verb &quot;convince&quot;, the lexicon has the following entry: This entry indicates that the verb &quot;convince&quot; can be followed by a noun phrase and a prepositional phrase starting with the preposition &quot;of' (e.g., he convinced me of his innocence). It can also be followed by a noun phrase and a to-infinitive phrase (e.g., he convinced me to go to the party). For example, whether a determiner is obligatory is relative to the noun phrase it is in; whether a prepositional phrase is obligatory is relative to the sentence or the phrase it is in. After an importance score is computed for each word, each phrase in the 'sentence gets a score by adding up the scores of its children nodes in the parse tree. It then marked which subtrees in these parse trees (i.e., phrases in the sentences) were removed by humans. For example, we can compute the probability that the &quot;when&quot; temporal clause is removed when the main verb is &quot;give&quot;, represented as Prob(&quot;when-clause is removed&quot; I &quot;v=give&quot;), or the probability that the to-infinitive modifier of the head noun &quot;device&quot; is removed, represented as Prob(&quot;to-infinitive modifier is removed&quot; I&quot;n=device&quot;). For example, the probability that the &quot;when&quot; temporal clause is removed when the main verb is &quot;give&quot;, Prob(&quot;when-clause is removed&quot; I &quot;v=give&quot;), is computed as the product of Prob( &quot;v=give&quot; I &quot;when-clause is removed&quot;) (i.e., the probability that the main verb is &quot;give&quot; when the &quot;when&quot; clause is removed) and Prob(&quot;when-clause is removed&quot;) (i.e., the probability that the &quot;when&quot; clause is removed), divided by Prob(&quot;v=give&quot;) (i.e., the probability that the main verb is &quot;give&quot;). Besides computing the probability that a phrase is removed, we also compute two other types of probabilities: the probability that a phrase is reduced (i.e., the phrase is not removed as a whole, but some components in the phrase are removed), and the probability that a phrase is unchanged at all (i.e., neither removed nor reduced). For example, for sentences like &quot;The agency reported that ...&quot; , &quot;The other source says that ...&quot; , &quot;The new study suggests that ...&quot; , the thatclause following the say-verb (i.e., report, say, and suggest) in each sentence is very rarely changed at all by professionals. To decide which phrases to remove, the system traverses the sentence parse tree, which now have been annotated with different types of information from earlier steps, in the top-down order and decides which subtrees should be removed, reduced or unchanged. A subtree (i.e., a phrase) is removed only if it is not grammatically obligatory, not the focus of the local context (indicated by a low importance score), and has a reasonable probability of being removed by humans. Suppose we have an input sentence (ABCDEFGH), which has a parse tree shown in Figure 2. Suppose a human reduces the sentence to (ABDGH), which can be translated to a series of decisions made along edges in the sentence parse tree as shown in Figure 3. The symbol &quot;y&quot; along an edge means the node it points to will be kept, and &quot;n&quot; means the node will be removed. Suppose the program reduces the sentence to (BCD), which can be translated similarly to the annotated tree shown in Figure 4. We can see that along five edges (they are D-)T, D-*G, B-4A, B->C), both the human and the program made decisions. Two out of the five decisions agree (they are D--B and D->E), so the success rate is 2/5 (40%). The success rate is defined as: # of edges along which the human and the program have made the same decision success rate = the total # of edges along which both the human and the progam have made decisions Note that the edges along which only the human or the program has made a decision (e.g., G--F and G->F in Figure 3 and Figure 4) are not considered in the computation of success rate, since there is no agreement issue in such cases. In the evaluation, we used 400 sentences in the corpus to compute the probabilities that a phrase is removed, reduced, or unchanged. Using five-fold validation (i.e., chose different 100 sentences for testing each time and repeating the experiment five times), The program achieved an average success rate of 81.3%. For the decisions on removing or keeping a clause, the system has a success rate of 78.1%; for the decisions on removing or keeping a to-infinitive, the system has a success rate of 85.2%. One reason for this is that our probability model can hardly capture the dependencies between a particular adjective and the head noun since the training corpus is not large enough, while the other sources of information, including grammar or context information, provide little evidence on whether an adjective or an adverb should be removed. Given that whether or not an adjective or an adverb is removed does not affect the conciseness of the sentence significantly and the system lacks of reliability in making such decisions, we decide not to remove adjectives and adverbs. On average, the system reduced the length of the 500 sentence by 32.7% (based on the number of words), while humans reduced it by 41.8%. Therefore, we take this into account when marking the obligatory components using subcategorization knowledge from the lexicon (step 2) - we not only look at the PPs that are attached to a verb phrase, but also PPs that are next to the verb phrase but not attached, in case it is part of the verb phrase. For example, given a sentence &quot;The spokesperson of the University said that ...&quot;, although that-clause in the sentence may have a complicated structure and the parser gets it wrong, the reduction system is not necessarily affected since it may decide in this case to keep that-clause as it is, as humans often do, so the parsing errors will not matter in this example.