 According to this table, classification of one word requires  ?s dot products with 228,306 support vectors in 33 classifiers. Inother domains such as character recognition, dimen 3http://cl.aist-nara.ac.jp/?taku-ku/software/TinySVM sion ` is usually fixed. Z?? fi 0 ? 1) G?Z?? fi 0 ? 1) G?Z?? Z???] ? We can rewrite +-) as follows.fi 0 ? ? Z???]?G[Z???]fi.? Z?? * ? B@]? * ? Z?? Z?B@]_ For binary vectors, it can be simplified as +-) .*? Z??? ? * ? ? Z??? B@]? * ? ? Z???] ? Z?? ]?G[Z?B@] . ? For instance,TinySVM took 11,490.26 seconds (3.2 hours) in to tal for applying OTHER?s classifier to all vectors in the training data. Its initialization phase took 2.13 seconds and all vectors in the training data were classified in 11,488.13 ( *=#9#%X?%Q??9????x?#p? ) ? / ?1) ? ? / ?1) ? ? ? We simplify ( ) by removing all features ? Z?? ? ? This approximation slightly degraded GENERAL?s F-measure from 88.31% to 88.03%.Table 2 shows the reduction of features that ap pear in support vectors. Accoring to the binary search, its performance did notchange even when the number of features was reduced to 21,852 at ?*KQ?Qr?9?r?%? Table 1: Reduction of CPU time (in seconds) by XQK word class  TinySVM (init) XQK (init) speed up SVM-Light OTHER 64,970 11,488.13 (2.13) 51.11 (174.17) 224.8 29,986.52 ARTIFACT-MIDDLE 14,171 1,372.85 (0.51) 41.32 (14.98) 33.2 6,666.26 LOCATION-SINGLE 13,019 1,209.29 (0.47) 38.24 (11.41) 31.6 6,100.54 ORGANIZ..-MIDDLE 12,050 987.39 (0.44) 37.93 (11.70) 26.0 5,570.82 : : : : : : TOTAL 228,306 21,754.23 (9.83) 1,019.20 (281.28) 21.3 104,466.31 Table 2: Reduction of features by XQK-FS word class number of features number of non-zero weights seconds OTHER 56,220 ? 21,852 (38.9%) 1,512,827 ? 892,228 (59.0%) 42.31 ARTIFIFACT-MIDDLE 22,090 ? 4,410 (20.0%) 473,923 ? 164,632 (34.7%) 30.47 LOCATION-SINGLE 17,169 ? 3,382 (19.7%) 366,961 ? 123,808 (33.7%) 27.72 ORGANIZ..-MIDDLE 17,123 ? 9,959 (58.2%) 372,784 ? 263,695 (70.7%) 31.02 ORGANIZ..-END 15,214 ? 3,073 (20.2%) 324,514 ? 112,307 (34.6%) 26.87 : : : : TOTAL 307,721 ? 75,455 (24.5%) 6,669,664 ? However, simple re duction of infrequent features without consideringweights damages the system?s performance. As mentioned above, we conducted an experiment for the cubic kernel ( F *??) by using all features.When we trained the cubic kernel classifiers by us ing only features selected by XQK-FS, TinySVM?s classification time was reduced by 40% because  was reduced by 38%. GENERAL?s F-measure was slightly improved from 87.04% to 87.10%. Onthe other hand, when we trained the cubic ker nel classifiers by using only features that appeared three times or more (without considering weights), TinySVM?s classification time was reduced by only 14% and the F-measure was slightly degraded to86.85%. By analyzing TinySVM?s classifier, we found that they can be calculated more efficiently. However,  is common to all dot products in B?D7   BD 7/ . In addition, counters for ?D%7  p ?D%7 / are prepared because dot products of binary vectors are integers. Then, for each non-zero G[Z??
] , the counters are incremented for all 7   fi2si Z???] Therefore, TinySVM?s clas sifier is faster than other classifiers. Z???] Yamada (Yamada et al, 2001) also reports that F*?? Since we followedIsozaki?s implementation (Isozaki, 2001), our system is different from Yamada?s system in the fol lowing points: 1) adjustment of word boundaries, 2)ChaSen?s parameters for unknown words, 3) char acter types, 4) use of the Viterbi search. For efficient classification, Burges and Scho?lkopf (1997) propose an approximation method that uses ?reduced set vectors?