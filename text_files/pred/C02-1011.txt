 In translation selection, we find out possible translation(s) from the translation candidates. information ->
age - (how old somebody is)  (historical era)  (legal adult hood) 3. Chinese:
obtain the document frequencies of them (i.e., numbers of documents containing them):
10000
10
0 5. Basic Algorithm Let e~ denote the Base NP to be translated and C~ the set of its translation candidates (phrases). Input: e~ , C~ , contexts containing e~ , contexts containing all Cc ~~ ? ; 1.
create a frequency vector )),(,),(),(( 21 mefefef L ),,1(, miEei L=? using contexts containing e~ ; transforming the vector into )),(,),(),(( 21 nEEE cfcfcf L ),,1(, niCci L=? , using a translation dictionary and the EM algorithm; 2.
for each ( Cc ~~ ? ){ estimate with Maximum Likelihood Estimation the prior probability )~(cP using contexts containing all Cc ~~ ? ; create a frequency vector )),(,),(),(( 21 ncfcfcf L ),,1(, niCci L=? using contexts containing c~ ; normalize the frequency vector , yielding ),,1(,)),~|(,),~|(),~|(( 21 niCcccPccPccP in LL =? ; calculate the posterior probability )|~( DcP with EM-NBC (generally EM-NBC-Ensemble), where ),,1(,)),(,),(),(( 21 niCccfcfcf inEEE LL =?=D 3. in descending order of )|~( DcP ;. Actually, the contexts containing the candidates are obtained at the same time when we conduct translation candidate collection (Step 4 in Figure 1). We further define }),(|{ Rceec ?=? At Step 1, we assume that all the instances in ))(),..,(),(( 21 mefefef are independently generated according to the distribution defined as: ? = Cc cePcPeP )|()()( (1) We estimate the parameters of the distribution by using the Expectation and Maximization (EM) Algorithm (Dempster et al, 1977). || 1)( C cP = , ?? ?= c c c e e ceP if,0 if,|| 1 )|( Next, we estimate the parameters by iteratively updating them, until they converge (cf., Figure 3). Finally, we calculate )(cf E for all Cc ? = Ee E efcPcf )()()( (2) In this way, we can transform the frequency vector in English ))(),..,(),(( 21 mefefef into a vector in Chinese ))(),..,(),(( 21 nEEE cfcfcf=D . Prior Probability Estimation At Step 2, we approximately estimate the prior probability )~(cP by using the document frequencies of the translation candidates. The data are obtained when we conduct candidate collection (Step 4 in Figure 1). Ee Ee Cc ecPef ecPef ceP ecPefcP cePcP cePcP ecP )|()( )|()()|( )|()()(StepM )|()( )|()()|(StepE Figure 3. EM Algorithm EM-NBC At Step 2, we use an EM-based Na?ve Bayesian Classifier (EM-NBC) to select the candidates c~ whose posterior probabilities are the largest: ?? )~|(log)()~(logmaxarg )|~(maxarg ~ ~ ~ ~ ccPcfcP cP Cc E Cc Cc D (3) Equation (3) is based on Bayes?rule and the assumption that the data in D are independently generated from CcccP ? ),~|( . If we ignore the first term in Equation (4), then the use of one EM-NBC turns out to select the candidate whose frequency vector is the closest to the transformed vector D in terms of KL divergence (cf., Cover and Tomas 1991). EM-NBC-Ensemble To further improve performance, we use an ensemble (i.e., a linear combination) of EM-NBCs (EM-NBC-Ensemble), while the classifiers are constructed on the basis of the data in different contexts with different window sizes. More specifically, we calculate where s),1,(i, L=iD denotes the data in different contexts. The idf value of a Chinese word c is calculated in advance and as )/)(log()( Fcdfcidf ?= (6) where )cdf( denotes the document frequency of c and F the total document frequency. Input: e~ , C~ , contexts containing e~ , contexts containing all Cc ~~ ? , Cc),cidf( ? ; 1.
create a frequency vector )),(,),(),(( 21 mefefef L ),,1(, miEei L=?using contexts containing e~ ; transforming the vector into 21 )),c(f,),c(f),c(f( nEEE L ),,1(, niCci L=?, using a translation dictionary and the EM algorithm; create a TF-IDF vector 11 )),cidf())c(f,),cidf()c(f( nnEE L=A ),,1(, niCci L=? ){ create a frequency vector )),(,),(),(( 21 ncfcfcf L ),,1(, niCci L=?using contexts containing c~ ; create a TF-IDF vector 11 ))cidf())c(f,),cidf()c(f( nnL=B ),,1(, niCc i L=? ; calculate ),cos()c~tfidf( BA= ; } 3. If for each English word we only retain the link connecting to the Chinese translation with the largest frequency (a link represented as a solid line) to establish a many-to-one mapping and transform vector A from English to Chinese, we obtain vector B. Specifically, EM can split the frequency of a word in English and distribute them into its translations in Chinese in a theoretically sound way (cf., the distributed frequencies of ?internet?). = = s i icP s cP 1 )|~(1)|~( DD (5) relationship, then the use of EM turns out to be equivalent to that of Major Translation. We extracted Base NPs (noun-noun pairs) from the Encarta 1 English corpus using the tool developed by Xun et al(2000). As a web search engine, we used Google (http://www.google.com). Best translation result for each method Accuracy (%) Top 1 Top 3 Coverage (%) EM-NBC-Ensemble 61.7 80.3 Prior 57.6 77.6 MT-NBC-Ensemble 59.9 78.1 EM-KL-Ensemble 45.9 72.3 EM-NBC 60.8 78.9 EM-TF-IDF 61.9 80.8 MT-TF-IDF 58.2 77.6 EM-TF 55.8 77.8 89.9 Table 1 shows the results in terms of coverage and top n accuracy. !in (4) to be 5 on the basis of our preliminary experimental results. and obtain documents as follows (i.e., using partial parallel corpora):
!#$ %()*#+ information asymmetry , 3. The key components are (1) distance calculation by KL divergence (2) EM, (3) prior probability, and (4) ensemble. The variants, thus, respectively make use of (1) the baseline method ?Prior?, (2) an ensemble of Na?ve Bayesian Classifiers based on Major Translation (MT-NBC-Ensemble), (3) an ensemble of EM-based KL divergence calculations (EM-KL-Ensemble), and (4) EM-NBC. The key components are (1) idf value and (2) EM. The variants, thus, respectively make use of (1) EM-based frequency vectors (EM-TF), (2) the baseline method MT-TF-IDF. Comparing the results between MT-NBC-Ensemble and EM-NBC-Ensemble and the results between MT-TF-IDF and EM-TF-IDF, we see that the uses of the EM Algorithm can indeed help to improve translation accuracies. Sample of translation outputs Base NP Translation calcium ion
adventure tale      lung cancer aircraft carrier * adult literacy * * Table 2 shows translations of five Base NPs as output by EM-NBC-Ensemble, in which the translations marked with * were judged incorrect by human experts. We analyzed the reasons for incorrect translations and found that the incorrect translations were due to: (1) no existence of dictionary entry (19%), (2) non-compositional translation (13%), (3) ranking error (68%). Translation results Accuracy (%) Top 1 Top 3 Coverage (%) Our Method 61.7 80.3 89.9 Nagata et als 72.0 76.0 10.5 We next used Nagata et als method to perform translation. Translation results Accuracy% Top 1 Top 3 Coverage % Back-off (Ensemble) 62.9 79.7 Back-off (TF-IDF) 62.2 79.8 91.4 helps to further improve the results whether EM-NBC-Ensemble or EM-TF-IDF is used. The data comprised of the Wall Street Journal corpus in English (1987-1992, 500MB) and the People?s Daily corpus in Chinese (1982-1998, 700MB). Translation results Accuracy%Data Top 1 Top 3 Coverage % Web (EM-NBC-Ensemble) 62.9 79.7 91.4 Non-web (EM-NBC-Ensemble) 56.9 74.7 79.3 Web (EM-IF-IDF) 62.2 79.8 91.4 Non-web (EM-TF-IDF) 51.5 71.4 78.5 The results in Table 5 show that the use of web data can yield better results than non-use of it, although the sizes of the non-web data we used were considerably large in practice.