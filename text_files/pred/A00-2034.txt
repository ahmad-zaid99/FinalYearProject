 Korhonen (1997) uses them in subcategorization frame (scF) acquisition to improve the performance of a statistical filter which determines whether a SCF observed for a particular verb is genuine or not. They have also been suggested for the recovery of predicate argument structure, necessary for SCF acquisition (Briscoe and Carroll, 1997; Boguraev and Briscoe, 1987). And Ribas (1995) showed that selectional preferences acquired using alternations performed better on a word sense disambiguation task compared to preferences acquired without alternations. These subtle changes of meaning are important in natural language generation (Stede, 1998). Alternations also provide a means of generalizing patterns of behaviour over groups of verbs, typically the group members are semantically related. Levin (1993) provides a classification of over 3000 verbs according to their participation in alternations involving NP and PP constituents. Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998). Dorr and Jones (1996) extend the classification by using grammatical information in LDOCE alongside semantic information in WordNet. Additionally, the frequency information in corpora is helpful for estimating alternation productivity (Lapata, 1999). Estimations of productivity have been suggested for controlling the application of alternations (Briscoe and Copestake, 1996). Secondly, selectional preference models are acquired for the argument heads associated with a specific slot in a specific SCF of a verb. We use the SCF acquisition system of Briscoe and Carroll (1997), with a probabilistic LR parser (Inui et al., 1997) for syntactic processing. To ensure the TCM covers all the word senses in WordNet, we modify Li and Abe's original scheme by creating hyponym leaf classes below all WordNet's hypernym (internal) classes. The minimum description length principle (MDL) (Rissanen, 1978) is used to find the best TCM by considering the cost (in bits) of describing both the model and the argument head data encoded in the model. The cost of describing each argument head (n) is calculated using the log of the probability estimate for the classes on the TCM that n belongs to (ca). Since the probability distributions may be at different levels of WordNet, we map the Tcms at the target slots to a common tree cut, a &quot;base cut&quot;. The UBC is at the classes B, C and D. To quantify the similarity between the probability distributions for the target slots we use the a-skew divergence (asp) proposed by Lee (1999). We experiment with a SCF lexicon produced from 19.3 million words of parsed text from the BNC (Leech, 1992). We used the causative and conative alternations, since these have enough candidates in our lexicon for experimentation. The SCF acquisition system has been evaluated elsewhere (Briscoe and Carroll, 1997). The kappa statistic (Siegel and Castellan, 1988) was calculated to ensure that there was significant agreement between judges for the initial set of candidates. The numbers of false negative (FN) and false positive (FP) errors for the mean and median thresholds are displayed in table 2, along with the threshold and accuracy. The outcomes for each individual verb for the experiment using the RBC and the mean threshold are as follows: add admit answer believe borrow cost declare demand expect feel imagine know notice pay perform practise proclaim read remember sing survive understand win write accelerate bang bend boil break burn change close cook cool crack decrease drop dry end expand fly improve increase match melt open ring rip rock roll shatter shut slam smash snap spill split spread start stop stretch swing tilt turn wake ask attack catch choose climb drink eat help kick knit miss outline pack paint plan prescribe pull remain steal suck warn wash The results for the UBC experiment are very similar. For both base cuts, there are a larger number of false positives than false negatives when the mean is used. There verbs have a high probability mass (around 0.7) under the entity class in both target slots, since both people and types of food occur under this class. The number of senses (according to WordNet) was used to indicate the polysemy of a verb. In this experiment, we used a similarity score on the argument heads directly, instead of generalizing the argument heads to WordNet classes. The lemma based similarity measure is termed lemma overlap (LO) and is given in equation 3, where A and B represent the target slots. LO is the size of the intersection of the multisets of argument heads at the target slots, divided by the size of the smaller of the two multisets. For example, if one slot contained the argument heads (person, person, person, child, man, spokeswoman), and the other slot contained {person, person, child, chair, collection}, then the intersection would be {person, person, child}, and LO would be t . The outcome for the individual verbs using the mean as a threshold was:add admit answer borrow choose climb cost declare demand drink eat feel imagine notice outline pack paint perform plan practise prescribe proclaim read remain sing steal suck survive understand wash win write accelerate bang break cook crack decrease drop expand flood land march repeat rip rock shatter Interestingly, the errors for the LO measure tend to be false negatives, rather than false positives.