 The cosine measure (Salton and McGill, 1983) returns the cosine of the angle between two vectors. The Jensen-Shannon (JS) divergence measure (Rao, 1983) and the ?-skew divergence measure (Lee, 1999) are based on the Kullback-Leibler (KL) divergence measure. The KL divergence,or relative entropy, D(p||q), between two prob ability distribution functions p and q is defined (Cover and Thomas, 1991) as the ?inefficiency of assuming that the distribution is q when the true distribution is p? c p log p q .However, D(p||q) = ? Thus,this measure cannot be used directly on maxi mum likelihood estimate (MLE) probabilities.One possible solution is to use the JS diver gence measure, which measures the cost of usingthe average distribution in place of each individual distribution. = 0.99 since this provides a close approximation to the KL divergence and has been shown to provide good results in previous research (Lee, 2001). The confusion probability (Sugawara et al, 1985) is an estimate of the probability that one word can be substituted for another. Jaccard?s coefficient (Salton and McGill,1983) calculates the proportion of features be longing to either word that are shared by both words. simja+mi is a variant (Lin, 1998) in which the features of a word are thosecontexts for which the pointwise mutual infor mation (MI) between the word and the context is positive, where MI can be calculated usingI(c, w) = log P (c|w)P (c) . The related Dice Coeffi cient (Frakes and Baeza-Yates, 1992) is omitted here since it has been shown (van Rijsbergen, 1979) that Dice and Jaccard?s Coefficients are monotonic in each other. Lin?s Measure (Lin, 1998) is based on his information-theoretic similarity theorem, whichstates, ?the similarity between A and B is measured by the ratio between the amount of in formation needed to state the commonality of A and B and the information needed to fully describe what A and B are.? The final three measures are settings in the additive MI-based Co-occurrence Retrieval Model (AMCRM) (Weeds and Weir, 2003; Weeds, 2003). We can measure the precisionand the recall of a potential neighbour?s re trieval of the co-occurrences of the target word,where the sets of required and retrieved co occurrences (F (w1) and F (w2) respectively) are those co-occurrences for which MI is positive. Neighbours with both high precision and high recall retrieval can be obtained by computing Measure Function cosine simcm(w2, w1) = ? c P (c|w1).P (c|w2) ?? distjs(w2, w1) = 12 ( D ( p||p+q2 ) +D ( q||p+q2 )) where p = P (c|w1) and q = P (c|w2) ?-skew dist? (w2, w1) = D (p||(?.q + (1? ).p)) where p = P (c|w1) and q = P (c|w2) conf. c P (w1|c).P (w2|c).P (c) P (w1) Jaccard?s simja(w2, w1) = |F (w1)?F (w2)| |F (w1)?F (w2)| where F (w) = {c : P (c|v) > 0} Jacc.+MI simja+mi(w2,W1) = |F (w1)?F (w2)| |F (w1)?F (w2)| where F (w) = {c : I(c, w) > 0} Lin?s simlin(w2, w1) = ? F (w1)?F (w2) (I(c,w1)+I(c,w2)) ? F (w1) I(c,w1)+ ? F (w2) I(c,w2) where F (w) = {c : I(c, w) > 0} precision simP(w2, w1) = ? F (w1)?F (w2) I(c,w2) ? F (w2) I(c,w2) where F (w) = {c : I(c, w) > 0} recall simR(w2, w1) = ? F (w1)?F (w2) I(c,w1) ? F (w1) I(c,w1) where F (w) = {c : I(c, w) > 0} harm. mean simhm(w2, w1) = 2.simP (w2,w1).simR(w2,w1) simP (w2,w1)+simR(w2,w1) where F (w) = {c : I(c, w) > 0} Table 1: Ten distributional similarity measures their harmonic mean (or F-score). We do this by calculating the overlap between neighbour setsfor 2000 nouns generated using different mea sures from direct-object data extracted from the British National Corpus (BNC). The data from which sets of nearest neighbours are derived is direct-object data for 2000 nouns extracted from the BNC using a robust accurate statistical parser (RASP) (Briscoe and Carroll, 2002). The complete set of 2000 nouns (WScomp) is the union of two sets WShigh and WSlow for which nouns were selected on the basis of frequency: WShigh contains the 1000 most frequently occurring nouns (frequency > 500), and WSlow contains the nouns ranked 3001-4000 (frequency ? We then generated ranked sets of nearest neighbours (of size k = 200 and where a word is excluded from being a neighbour of itself) for each word and each measure.For a given word, we compute the overlap between neighbour sets using a comparison tech nique adapted from Lin (1998). If NS(w,m) is the vector of such scores for word w and measure m, then theoverlap, C(NS(w,m1),NS(w,m2)), of two neigh bour sets is the cosine between the two vectors: C(NS(w,m1),NS(w,m2)) = ? rm2(w ?, w) ?k i=1 i2 The overlap score indicates the extent to which sets share members and the extent to whichthey are in the same order. cp ja ja+mi lin cm 1.0(0.0) 0.69(0.12) 0.53(0.15) 0.33(0.09) 0.26(0.12) 0.28(0.15) 0.32(0.15) js 0.69(0.12) 1.0(0.0) 0.81(0.10) 0.46(0.31) 0.48(0.18) 0.49(0.20) 0.55(0.16) ? 0.53(0.15) 0.81(0.10) 1.0(0.0) 0.61(0.08) 0.4(0.27) 0.39(0.25) 0.48(0.19) cp 0.33(0.09) 0.46(0.31) 0.61(0.08) 1.0(0.0) 0.24(0.24) 0.20(0.18) 0.29(0.15) ja 0.26(0.12) 0.48(0.18) 0.4(0.27) 0.24(0.24) 1.0(0.0) 0.81(0.08) 0.69(0.09) ja+mi 0.28(0.15) 0.49(0.20) 0.39(0.25) 0.20(0.18) 0.81(0.08) 1.0(0.0) 0.81(0.10) lin 0.32(0.15) 0.55(0.16) 0.48(0.19) 0.29(0.15) 0.69(0.09) 0.81(0.10) 1.0(0.0) Table 2: Cross-comparison of first seven similarity measures in terms of mean overlap of neighbour sets and corresponding standard deviations. P R hm cm 0.18(0.10) 0.31(0.13) 0.30(0.14) js 0.19(0.12) 0.55(0.18) 0.51(0.18) ? 0.08(0.08) 0.74(0.14) 0.41(0.23) cp 0.03(0.04) 0.57(0.10) 0.25(0.18) ja 0.36(0.30) 0.38(0.30) 0.74(0.14) ja+mi 0.42(0.30) 0.40(0.31) 0.86(0.07) lin 0.46(0.25) 0.52(0.22) 0.95(0.039)Table 3: Mean overlap scores for seven simi larity measures with precision, recall and the harmonic mean in the AMCRM. Although overlap between most pairs of measures is greater than expected if sets of 200 neighbours were generated randomly from WScomp (in this case, average overlap would be 0.08 and only the overlap between the pairs (?,P) and (cp,P) is not significantly greaterthan this at the 1% level), there are substantial differences between the neighbour sets gen erated by different measures. If NSwordset is the vector of neighbours (and associated rank scores) for a given word, w, andsimilarity measure, m, and generated considering just the words in wordset as potential neigh bours, then the overlap between two neighboursets can be computed using a cosine (as be fore). If Chigh = C(NScomp,NShigh) and Clow =C(NScomp,NSlow), then we compute the bias towards high frequency neighbours for word w us ing measure m as: biashighm(w) = Chigh Chigh+Clow The value of this normalised score lies in the range [0,1] where 1 indicates a neighbour set completely made up of high frequency words, 0 indicates a neighbour set completely made up oflow frequency words and 0.5 indicates a neighbour set with no biases towards high or low fre quency words. The standard deviations (not shown) all lie in the range [0,0.2]. Further, there appears to be three classes of mea sures: those that select high frequency nouns as neighbours regardless of the frequency of thetarget noun (cm, js, ?, cp andR); those that select low frequency nouns as neighbours regard less of the frequency of the target noun (P); and those that select nouns of a similar frequency to the target noun (ja, ja+mi, lin and hm).This can also be considered in terms of distri butional generality. For example, animal is an (indirect1) hypernym of dog and conversely dog is an (indi rect) hyponym of animal. Thus, if n1 and n2 are related and P(n2, n1) >R(n2, n1), we might expect that n2 is a hy ponym of n1 and vice versa. In order to test these hypotheses, we ex tracted all of the possible hyponym-hypernym pairs (20, 415 pairs in total) from our list of 2000 nouns (using WordNet 1.6). Baldwin et al (2003) explore empiricalmodels of compositionality for noun-noun com pounds and verb-particle constructions. Based on the observation (Haspelmath, 2002) that compositional collocations tend to be hyponyms of their head constituent, they propose a model which considers the semantic similarity between a collocation and its constituent words.McCarthy et al (2003) also investigate sev eral tests for compositionality including one (simplexscore) based on the observation that compositional collocations tend to be similar inmeaning to their constituent parts. The value obtained (0.0525) is dis appointing since it is not statistically significant(the probability of this value under the null hy pothesis of ?no correlation? is 0.3).2 However, Haspelmath (2002) notes that a compositional collocation is not just similar to one of its constituents ? and ?to2Other tests for compositionality investigated by Mc Carthy et al (2003) do much better. Measure rs P (rs) under H0 simlin 0.0525 0.2946 precision -0.160 0.0475 recall 0.219 0.0110 harmonic mean 0.011 0.4562 Table 5: Correlation with compositionality for different similarity measures rip up? Thus, we hypothesised that a distributional measure which tends to select more generalterms as neighbours of the phrasal verb (e.g. re call) would do better than measures that tend to select more specific terms (e.g. precision) or measures that tend to select terms of a similar specificity (e.g simlin or the harmonic mean of precision and recall). Table 5 shows the results of using different similarity measures with the simplexscore test and data of McCarthy et al (2003). Finally, we obtained a very similar result (0.217) by ranking phrasals according to their inverse relative frequency with their simplex constituent (i.e., freq(simplex)freq(phrasal) ).