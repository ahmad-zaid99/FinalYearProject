 For training from a hand-tagged corpus, the model is estimated by counting the number of transitions from each tag i to each tag j, the total occurrence of each tag i, and the total occurrence of word w with tag i. For example, CLAWS (Garside et al., 1987) normalises the lexical probabilities by the total frequency of the word rather than of the tag. Various methods of quoting accuracy have been used in the literature, the most common being the proport ion of words (tokens) receiving the correct tag. A model was trained from a handtagged corpus in the manner described above, and then degraded in various ways to simulate the effect of poorer training, as follows:
DO Un-degraded lexical probabilities, calculated from f (i, w) / f (i). The Xerox experiments (Cutting et a/., 1992) correspond to something between D1 and D2, and between TO and Ti, in that there is some initial biasing of the probabilities. Corpus LOBB-J was used to train the model, and LOB-B, LOBL and LOB-B-G were passed through thirty iterations of the BW algorithm as untagged data. In each case, the best accuracy (on ambiguous words, as usual) from the FB algorithm was noted. A possible explanation is that in this case the test data does not overlap with the training data, and hence the good quality lexicons (DO and D1) have less of an influence. It follows that if BaumWelch re-estimation is to be an effective technique, the initial data must have either biasing in the transitions (the TO cases) or in the lexical probabilities (cases DO+T1 and D1-FT1), but it is not necessary to have both (D2/D3+TO and DO/Did-T1). Secondly, training from a hand-tagged corpus (case DO+TO) always does best, even when the test data is from a different source to the training data, as it is for LOB-L. Early maximum Rising accuracy for a small number of iterations (2-4), and then falling as in initial maximum. Clearly, if the expected pattern is initial maximum, we should not use BW at all, if early maximum, we should halt the process after a few iterations, and if classical, we should halt the process in a &quot; standard&quot; way, such as comparing the perplexity of successive models. The tests were conducted in a similar manner to those of the first experiment, by building a lexicon and transitions from a hand tagged training corpus, and then applying them to a test corpus with varying degrees of degradation. Two tests were conducted with each combination of the degradation and similarity, using different corpora (from the Penn treebank) ranging in size from approximately 50000 words to 500000 words. The results appear in table 2, showing the best accuracy achieved (on ambiguous words). the iteration at which it occurred, and the pattern of re-estimation (I = initial maximum, E = early maximum, C = classical). With a good lexicon but either degraded transitions or a test corpus differing from the training corpus, the pattern tends to be early maximum. In the case of early maximum, the few iterations where the accuracy is improving correspond to the creation of entries for unknown words and the fine tuning of ones for known ones, and these changes outweigh those produced by the re-estimation. Similar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions. In addition, although Merialdo does not highlight the point, BW re-estimation starting from less than 5000 words of hand-tagged text shows early maximum behaviour. The step forward taken in the work here is to show that there are three patterns of reestimation behaviour, with differing guidelines for how to use BW effectively, and that to obtain a good starting point when a hand-tagged corpus is not available or is too small, either the lexicon or the transitions must be biased. While these may be useful heuristics from a practical point of view, the next step forward is to look for an automatic way of predicting the accuracy of the tagging process given a corpus and a model. Perhaps what is needed is a &quot;similarity measure&quot; between two models M and M', such that if a corpus were tagged with model M, M' is the model obtained by training from the output corpus from the tagger as if it were a hand-tagged corpus.