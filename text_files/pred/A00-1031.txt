 To be explicit, we calculate for a given sequence of words w1 of length T. t1 tr are elements of the tagset, the additional tags t_1, to, and t7-,+-1 are beginning-of-sequence and end-of-sequence markers. As a first step, we use the maximum likelihood probabilities P which are derived from the relative frequencies: for all t1, t2, t3 in the tagset and w3 in the lexicon. As a second step, contextual frequencies are smoothed and lexical frequences are completed by handling words that are not in the lexicon (see below). Therefore, we estimate a trigram probability as follows: P are maximum likelihood estimates of the probabilities, and A1 + A2 +- A3 = 1, SO P again represent probability distributions. The tested groupings included a) one set of As for each frequency value and b) two classes (low and high frequency) on the two ends of the scale, as well as several groupings in between and several settings for partitioning the classes. Currently, the method of handling unknown words that seems to work best for inflected languages is a suffix analysis as proposed in (Samuelsson, 1993). The suffix is a strong predictor for word classes, e.g., words in the Wall Street Journal part of the Penn Treebank ending in able are adjectives (.11) in 98% of the cases (e.g. This calculates the probability of a tag t given the last m letters i of an n letter word: P(t1/7,+1,,..ln). The sequence of increasingly more general contexts omits more and more characters of the suffix, such that P(tlin-m+2, P(tlin_m+3, ,i), , P(t) are used for smoothing. The recursion formula is set A = A2 = A3 = 0 foreach trigram t1,t2,t3 with f (ti,t2,t3) >0 depending on the maximum of the following three values: for i = m ... 0, using the maximum likelihood estimates P from frequencies in the lexicon, weights Oi and the initialization For the Markov model, we need the inverse conditional probabilities P(1,2_1+1, ... /Tilt) which are obtained by Bayesian inversion. A theoretical motivated argumentation uses the standard deviation of the maximum likelihood probabilities for the weights 0, (Samuelsson, 1993). We use the longest suffix that we can find in the training set (i.e., for which the frequency is greater than or equal to 1), but at most 10 characters. It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m - 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10. 3) We use different estimates for uppercase and lowercase words, i.e., we maintain two different suffix tries depending on the capitalization of the word. Accepting that unknown words are most probably infrequent, one can argue that using suffixes of infrequent words in the lexicon is a better approximation for unknown words than using suffixes of frequent words. Tags are usually not informative about capitalization, but probability distributions of tags around capitalized words are different from those not capitalized. The processing time of the Viterbi algorithm (Rabiner, 1989) can be reduced by introducing a beam search. Empirically, a value of 0 = 1000 turned out to approximately double the speed of the tagger without affecting the accuracy. The tagger currently tags between 30,000 and 60,000 tokens per second (including file I/O) on a Pentium 500 running Linux. The overall accuracy, as well as separate accuracies for known and unknown words are measured. Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set). The tests are performed on partitions of the corpora that use 90% as training set and 10% as test set, so that the test data is guaranteed to be unseen during training. Using the round-robin procedure, parts of an article are already seen, which significantly reduces the percentage of unknown words. In the following, accuracy denotes the number of correctly assigned tags divided by the number of tokens in the corpus processed. We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens. The latter are interesting, since usually unknown tokens are much more difficult to process than known tokens, for which a list of valid tags can be found in the lexicon. The German NEGRA corpus consists of 20,000 sentences (355,000 tokens) of newspaper texts (Frankfurter Rundschau) that are annotated with parts-ofspeech and predicate-argument structures (Skut et al., 1997). Figure 3 shows the learning curve of the tagger, i.e., the accuracy depending on the amount of training data. Training length is the number of tokens used for training. Each training length was tested ten times, training and test sets were randomly chosen and disjoint, results were averaged. The training length is given on a logarithmic scale. We exploit the fact that the tagger not only determines tags, but also assigns probabilities. So, the distance of the probabilities of a best tag tbest and an alternative tag tau is expressed by p(tbest)/P(talt)7 which is some value greater or equal to 1 since the best tag assignment has the highest probability. acc. acc. The table shows the percentage of unknown tokens, separate accuracies and standard deviations for known and unknown tokens, as well as the overall accuracy. acc. acc. This distinction is, e.g., useful for annotation projects during the cleaning process, or during pre-processing, so the tagger can emit multiple tags if the best tag is classified as unreliable. The annotation consists of four parts: 1) a context-free structure augmented with traces to mark movement and discontinuous constituents, 2) phrasal categories that are annotated as node labels, 3) a small set of grammatical functions that are annotated as extensions to the node labels, and 4) part-of-speech tags (Marcus et al., 1993). 50,000 sentences (1.2 million tokens). Figure 6 shows the learning curve of the tagger, i.e., the accuracy depending on the amount of training data. Training length is the number of tokens used for training. The training length is given on a logarithmic scale. As for the NEGRA corpus, tagging accuracy is very high for known tokens even with small amounts of training data. We exploit the fact that the tagger not only determines tags, but also assigns probabilities. Average part-of-speech tagging accuracy is between 96% and 97%, depending on language and tagset, which is at least on a par with state-of-the-art results found in the literature, possibly better. For the Penn Treebank, (Ratnaparkhi, 1996) reports an accuracy of 96.6% using the Maximum Entropy approach, our much simpler and therefore faster HMM approach delivers 96.7%. For the German newspaper data, results are 8.7% better when the word was seen before and therefore is in the lexicon, than when it was not seen before (97.7% vs. 89.0%).