 First, a predicate that can have multiple arguments may still occur with only one of its arguments.For example, in ?The acquisition of TCA was successful?, ?TCA? For example, modifiers, such as ?the elected X?, or intransitive verbs. In addition, it appears more tractable to learn all variations for each argument of a predicate separately than to learn them for combinations of argument pairs.For these reasons, it seems that unary rule learn ing should be addressed in addition to binary rule learning. We are further motivated by the fact thatsome (mostly supervised) works in IE found learn ing unary templates useful for recognizing relevant named entities (Riloff, 1996; Sudo et al, 2003; Shinyama and Sekine, 2006), though they did notattempt to learn generic knowledge bases of entail ment rules.This paper investigates acquisition of unary entailment rules from regular non-comparable cor pora. The second approach,which was also mentioned in (Iftene and Balahur Dobrescu, 2007), derives unary rules from learned binary rules. Texts are represented by dependency parse trees (using the Minipar parser (Lin, 1998b)) and templates by parse sub-trees. Given a dependency parse tree, any sub-tree can be a candidate template, setting some of its nodesas variables (Sudo et al, 2003). In the binary rule learning litera ture, the main solution for exhaustively learning allrules between any pair of templates in a given corpus is to restrict the structure of templates. Typi cally, a template is restricted to be a path in a parse tree between two variable nodes (Lin and Pantel, 2001; Ibrahim et al, 2003). Following this approach, we chose the structure of unary templates to be paths as well, where oneend of the path is the template?s variable. How ever, paths with one variable have more expressive power than paths between two variables, since the combination of two unary paths may generate a binary template that is not a path. is the template ?X call Y indictable?, which is not a path between X and Y . Whenever a candidate pred icate is encountered (any noun, adjective or verb) the path from that node to v is taken as a template. ?X help preventing?, ?Xmake noise?, whenever a verb is encountered we generate templates that are paths between v and the verb?s modifiers, either ob jects, prepositional complements or infinite or gerund verb forms (paths ending at stop words, e.g. ?the losingX?, we extract template paths between v and each of its modifiers, nouns or adjectives, that are derived from a verb. We use the Catvar database to identify verb derivations (Habash and Dorr, 2003). Each argument instantiation of template t in the corpus is taken as a feature f , and the pmi between t and f is used for the feature?s weight. We first adaptedDIRT for unary templates (unary-DIRT, apply ing Lin-similarity to the single feature vector), as well as its output filtering by LEDIR. After initial analysis, we found that given a right hand side template r, symmetric measures such as Lin (in DIRT) generally tend to prefer (score higher) relations ?l, r? On the other hand, directional measures such as Weeds Precision tend to prefer directional rules inwhich the entailing template is infrequent. If an in frequent template has common instantiations with another template, the coverage of its features istypically high, whether or not an entailment relation exists between the two templates. Based on this analysis, we propose a new measure that balances the two behaviors, termed 1We applied the best performing parameter values pre sented in (Bhagat et al, 2007) and (Weeds and Weir, 2003).Balanced-Inclusion (BInc). BInc identifies entail ing templates based on a directional measure but penalizes infrequent templates using a symmetric measure: BInc(l, r) = ? Precision(l, r) 3.4 Deriving Unary Rules From Binary Rules. First, for each binary rule, we generate all possible unary rules that are part of that rule (each unary template is extracted following the same procedure describedin Section 3.2). employ Y for Z?, having a different score from each original binary rule. Three aggregationfunctions were tested: sum (Derived-Sum), aver age (Derived-Avg) and maximum (Derived-Max). We want to evaluate learned unary and binary rule bases by their utility for NLP applications throughassessing the validity of inferences that are per formed in practice using the rule base.To perform such experiments, we need a test set of seed templates, which correspond to a set of target predicates, and a corpus annotated with allargument mentions of each predicate. The evaluation assesses the correctness of all argument ex tractions, which are obtained by matching in the corpus either the seed templates or templates that entail them according to the rule-base (the latter corresponds to rule-application). Following (Szpektor et al, 2008), we found the ACE 2005 event training set 2useful for this pur pose. For instance, some of the Injure event arguments are Agent, Victim and Time.To utilize the ACE dataset for evaluating entail ment rule applications, we manually represented each ACE event predicate by unary seed templates. Templatesare matched using a syntactic matcher that han dles simple morpho-syntactic phenomena, as in (Szpektor and Dagan, 2007). For testing binary rule-bases, we automatically generated binary seed templates from any twounary seeds that share the same predicate. First, our evaluation aims at assessing the correctness of inferring a specific target semantic meaning, which is denoted by a specific predicate, using rules. We also omitted 3 events with less than 10mentions, and were left with 26 events (6380 argu ment mentions). We implemented the unary rule learning algo rithms described in Section 3 and the binary DIRT algorithm (Lin and Pantel, 2001). We executed each method over the Reuters RCV1 corpus 3 , learning for each template r in the corpus the top100 rules in which r is entailed by another tem plate l, ?l? All rules were learned in canonical form (Szpektor and Dagan, 2007). No threshold setting mechanism is suggested inthe literature for the scores of the different algo rithms, especially since rules for different right hand side templates have different score ranges. Thus, we follow common evaluation practice (Lin and Pantel, 2001; Geffet and Dagan, 2005) and test each learned rule-set by taking the top K rules for each seed template, whereK ranges from 0 to 100.WhenK=0, no rules are used and mentions are ex tracted only by direct matching of seed templates. Our rule application setting provides a rather simplistic IE system (for example, no named entity recognition or approximate template matching). It is thus useful for comparing different rule-bases,though the absolute extraction figures do not re flect the full potential of the rules. In this section we focus on the best performing variations of each algorithm type: binary DIRT, unary DIRT, unary Weeds Harmonic, BInc and Derived-Avg. We omitted the results of methods that were clearly inferior to others: (a) WeedsA, WeedsD and Weeds-Precision did not increase 3 http://about.reuters.com/researchandstandards/corpus/ 853Recall over not using rules because rules with in frequent templates scored highest and arithmetic averaging could not balance well these high scores; (b) out of the methods for deriving unary rules from binary rule-bases, Derived-Avg performed best; (c) filtering with (the directional) LEDIR did not improve the performance of unary DIRT. First, we observe that even when matching only the seed tem plates (K=0), unary seeds outperform the binary seeds in terms of both Precision and Recall. The inferior behavior of binary DIRT is analyzed in Section 5.2.The graphs show that symmetric unary approaches substantially increase recall, but dramati cally decrease precision already at the top 10 rules. BInc?s re call increases moderately compared to other unarylearning approaches, but it is still substantially bet ter than not using rules (a relative recall increase of 50% already at K=10). We found that many of the correct mentions missed by BInc but identified by other methods are due to occasional extractions of incorrect frequent rules, such as partial templates (see Section 5.2). X marry?.When only one argument is annotated for a specific event mention (28% of ACE predicate mentions, which account for 15% of all annotated arFigure 1: Average Precision, Recall and F1 at dif ferent top K rule cutoff points. guments), binary rules either miss that mention, orextract both the correct argument and another in correct one. Binary-DIRT We analyzed incorrect rules both for binary-DIRT and BInc by randomly sampling,for each algorithm, 200 rules that extracted incor rect mentions. as either: (a) Correct - the rule is valid insome contexts of the event but extracted some in correct mentions; (b) Partial Template - l is only apart of a correct template that entails r. For exam ple, learning ?X decide? ; (e) Incorrect - other incorrect rules, e.g. First, rela tive to BInc, it tends to learn incorrect rules for high frequency templates, and therefore extractedmany more incorrect mentions for the same num ber of incorrect rules. Second, a large percentage of incorrect mentions extracted are due to partial templates at the rule left-hand-side. As explained in Section 3.2 the unary template structure we use is more expressive, enabling to learn the correct rules. System Level Analysis We manually analyzedthe reasons for false positives (incorrect extrac tions) and false negatives (missed extractions) of BInc, at its best performing cutoff point (K=20), by sampling 200 extractions of each type. The main reasons for learning such rules are those discussed in Section 3.3: (a) related templates that are not entailing; (b) infrequent templates. Apart from incorrectly learned rules, incorrect template matching (e.g. Binary DIRT Balanced Inclusion Correct 16 (70) 38 (91) Partial Template 27 (2665) 6 (81) Incorrect 157 (2584) 156 (787) Total 200 (5319) 200 (959) Table 1: Rule type distribution of a sample of 200rules that extracted incorrect mentions. Reason % mentions Incorrect Rule learned 39.0 Context mismatch 27.0 Match error 19.0 Annotation problem 15.0 Table 2: Distribution of reasons for false positives (incorrect argument extractions) by BInc at K=20. Reason % mentions Rule not learned 61.5 Match error 25.0 Discourse analysis needed 12.0 Argument is predicative 1.5 Table 3: Distribution of reasons for false negatives (missed argument mentions) by BInc at K=20. First, we note that 12% of the arguments cannotbe extracted by rules alone, due to necessary discourse analysis. How ever, 25% of the mentions were missed because of incorrect syntactic matching of correctly learned rules. By assuming correct matches in these cases we isolate the recall of the rule-set (along with the seeds), which yields 39% recall.