 Finally, the system combines them to produce the holder?s sentiment for the whole sentence. The basic approach is to assemble a small amount of seed words by hand, sorted by polarity into two lists?positive and negative?and then to grow this by adding words obtained from WordNet (Miller et al 1993; Fellbaum et al 1993). We assume synonyms of positive words are mostly positive and antonyms mostly negative, e.g., the positive word ?good? To start the seed lists we selected verbs (23 positive and 21 negative) and adjectives (15 positive and 19 negative), adding nouns later. Using these expanded lists, we extracted an additional cycle of words from WordNet, to obtain finally 5880 positive adjectives, 6233 negative adjectives, 2840 positive verbs, and 3239 negative verbs. This indicated the need to develop a measure of strength of sentiment polarity (the alternative was simply to discard such ambiguous words)?to determine how strongly a word is positive and also how strongly it is negative. That is, we compute (1) ).....,|(maxarg )|(maxarg 21 n c c synsynsyncP wcP ? where c is a sentiment category (positive or negative), w is the unseen word, and synn are the WordNet synonyms of w. To compute Equation (1), we tried two different models: (2) )|()(maxarg )|()(maxarg )|()(maxarg)|(maxarg 1 ))(,( ...3 2 1 ? = = = = m k wsynsetfcount k c n c cc kcfPcP csynsynsynsynPcP cwPcPwcP where fk is the kth feature (list word) of sentiment class c which is also a member of the synonym set of w, and count(fk,synset(w)) is the total number of occurrences of fk in the synonym set of w. P(c) is the number of words in class c divided by the total number of words considered. == = To compute the probability P(w|c) of word w given a sentiment class c, we count the occurrence of w?s synonyms in the list of c. The intuition is that the more synonyms occuring in c, the more likely the word belongs. abysmal : NEGATIVE [+ : 0.3811][- : 0.6188] adequate : POSITIVE [+ : 0.9999][- : 0.0484e-11] afraid : NEGATIVE [+ : 0.0212e-04][- : 0.9999] ailing : NEGATIVE [+ : 0.0467e-8][- : 0.9999] amusing : POSITIVE [+ : 0.9999][- : 0.0593e-07] answerable : POSITIVE [+ : 0.8655][- : 0.1344] apprehensible: POSITIVE [+ : 0.9999][- : 0.0227e-07] averse : NEGATIVE [+ : 0.0454e-05][- : 0.9999] blame : NEGATIVE [+ : 0.2530][- : 0.7469] Table 2: Sample output of word sentiment classifier. We therefore included in the algorithm steps to identify the Topic (through direct matching, since we took it as given) and any likely opinion Holders (see Section 2.2.1). Near each Holder we then identified a region in which sentiments would be considered; any sentiments outside such a region we take to be of undetermined origin and ignore (Section 2.2.2). We then defined several models for combining the sentiments expressed within a region (Section 2.2.3). 2.2.2 Sentiment Region Lacking a parse of the sentence, we were faced with a dilemma: How large should a region be? We therefore defined the sentiment region in various ways (see Table 3) and experimented with their effectiveness, as reported in Section 3. 2.2.3 Classification Models We built three models to assign a sentiment category to a given sentence, each combining the individual sentiments of sentiment-bearing words, as described above, in a different way. Model 1 is the harmonic mean (average) of the sentiment strengths in the region: Model 1: cwcp wcp cn scP ij n i i = = ? = )|(argmax if ,)|( )( 1)|( j 1 Here n(c) is the number of words in the region whose sentiment category is c. If a region contains more and stronger positive than negative words, the sentiment will be positive. )|(argmax ,)|(10)|( j 1 1)( 2.2.4 Examples The following are two example outputs. TOPIC : illegal alien HOLDER : U.S. Senate OPINION REGION: vote/NN Thursday/NNP to/TO exclude/VB illegal/JJ aliens/NNS from/IN the/DT 1990/CD census,/NN SENTIMENT_POLARITY: negative For that reason and others, the Constitutional Convention unanimously rejected term limits and the First Congress soundly defeated two subsequent term-limit proposals. TOPIC : term limit HOLDER : First Congress OPINION REGION: soundly/RB defeated/VBD two/CD subsequent/JJ term-limit/JJ proposals./NN SENTIMENT_POLARITY: negative
The first experiment examines the two word sentiment classifier models and the second the three sentence sentiment classifier models. The classification task is defined as assigning each word to one of three categories: positive, negative, and neutral. 3.1.1 Human?Human Agreement Adjectives Verbs Human1 : Human2 Human1 : Human3 Strict 76.19% 62.35% Lenient 88.96% 85.06% Table 4: Inter-human classification agreement. The strict measure is defined over all three categories, whereas the lenient measure is taken over only two categories, where positive and neutral have been merged, should we choose to focus only on differentiating words of negative sentiment. 3.1.2 Human?Machine Agreement Table 5 shows results, using Equation (2) of Section 2.1.1, compared against a baseline that randomly assigns a sentiment category to each word (averaged over 10 iterations). Of the test data, the algorithm classified 93.07% of adjectives and 83.27% of verbs as either positive and negative. In Table 5, the seed list included just a few manually selected seed words (23 positive and 21 negative verbs and 15 and 19 adjectives, repectively). After collecting the annotated data, we added half of it (231 adjectives and 251 verbs) to the training set, retaining the other half for the test. Adjective (Train: 231 Test : 231) Verb (Train: 251 Test : 251) Lenient agreement Lenient agreement H1:M H2:M recall H1:M H3:M recall 75.66% 77.88% 97.84% 81.20% 79.06% 93.23% Table 6: Results including manual data. 3.2.1 Data 100 sentences were selected from the DUC 2001 corpus with the topics ?illegal alien?, ?term limits?, ?gun control?, and ?NAFTA?. Two humans annotated the 100 sentences with three categories (positive, negative, and N/A). To measure the agreement between humans, we used the Kappa statistic (Siegel and Castellan Jr. 1988). 3.2.2 Test on Human Annotated Data We experimented on Section 2.2.3?s 3 models of sentiment classifiers, using the 4 different window definitions and 4 variations of word-level classifiers (the two word sentiment equations introduced in Section 2.1.1, first with and then without normalization, to compare performance). The models are numbered as follows: m0 through m4 represent 4 sentence classifier models, Table 5. Adjective (test: 231 adjectives) Verb (test : 251 verbs) Lenient agreement Lenient agreement H1:M H2:M recall H1:M H3:M recall Random selection (average of 10 iterations) 59.35% 57.81% 100% 59.02% 56.59% 100% Basic method 68.37% 68.60% 93.07% 75.84% 72.72% 83.27% p1/p2 and p3/p4 represent the word classifier models in Equation (2) and Equation (3) with normalization and without normalization respectively. 0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 ac cu ra cy Window 1 Window 2 Window 3 Window 4 0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 ac cu rac y Window 1 Window 2 Window 3 Window 4 Human 1 : Machine Human 2 : Machine Figure 2: Results with manually annotated Holder. 0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 ac cu rac y Window 1 Window 2 Window 3 Window 4 0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 ac cu rac y Window 1 Window 2 Window 3 Window 4 Human 1 : Machine Human 2 : Machine Figure 3: Results with automatic Holder detection. Since human1 classified 33 sentences positive and 33 negative, random classification gives 33 out of 66 sentences. Similarly, since human2 classified 29 positive and 34 negative, random classification gives 34 out of 63 when the system blindly marks all sentences as negative and 29 out of 63 when it marks all as positive. 3.3.1 Word Sentiment Classification As mentioned, some words have both strong positive and negative sentiment. Second, a unigram model is not sufficient: common words without much sentiment alone can combine to produce reliable sentiment. 3.3.2 Sentence Sentiment Classification Even in a single sentence, a holder might express two different opinions. expresses a positive opinion about term limits but the absence of adjective, verb, and noun sentiment-words prevents a classification. With manually identified topic and holder, the region window4 (from Holder to sentence end) performs better than other regions. positive negative total Human1 5.394 1.667 7.060 Human2 4.984 1.714 6.698 Table 7: Average difference between manual and automatic holder detection.