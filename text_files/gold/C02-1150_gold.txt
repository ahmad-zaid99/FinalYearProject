The approach we adopted is a multi-level learning approach: some of our fea tures rely on finer analysis of the questions that are outcomes of learned classifiers; the QC module then applies learning with these as input features.
The hierarchy con tains 6 coarse classes (ABBREVIATION, ENTITY,DESCRIPTION, HUMAN, LOCATION and NU MERIC VALUE) and 50 fine classes, Table 1 showsthe distribution of these classes in the 500 ques tions of TREC 10.
9 description 7 abb 1 manner 2 exp 8 reason 6 ENTITY 94 HUMAN 65 animal 16 group 6 body 2 individual 55 color 10 title 1 creative 0 description 3 currency 6 LOCATION 81 dis.med.
2 city 18 event 2 country 3 food 4 mountain 3 instrument 1 other 50 lang 2 state 7 letter 0 NUMERIC 113 other 12 code 0 plant 5 count 9 product 4 date 47 religion 0 distance 16 sport 1 money 3 substance 15 order 0 symbol 0 other 12 technique 1 period 8 term 7 percent 3 vehicle 4 speed 6 word 0 temp 5 DESCRIPTION 138 size 0 definition 123 weight 4 Table 1: The distribution of 500 TREC 10 questions over the question hierarchy.
Map coarse classes to fine classes C0 C1 C2 C3 abb,def animal,food all possible subsets of C0 wih size = 5 all possible subsets of C2 with size =5 Figure 1: The hierarchical classifier that its candidate labels are generated by expanding the set of retained coarse classes from the first into a set of fine classes; this set is then treated as the confusion set for the second classifier.Figure 1 shows the basic structure of the hierar chical classifier.
If we treat pi as the probability that a question belongs to Class i, the decision model yields a reasonable probabilistic interpretation.
We use T = 0:95 in the experiments.
Data are collected from four sources: 4,500 English questions published by USC (Hovy et al, 2001), about 500 manually constructed questions for a few rare classes, 894 TREC 8 and TREC 9 questions, and also 500 questions from TREC 10 which serves as our test set3.These questions were manually labeled accord ing to our question hierarchy.
In training, we divide the 5,500 questions from the first three sources randomly into 5 training sets of 1,000, 2,000, 3,000, 4,000 and 5,500 questions.
All 500 TREC 10 questions are used as the test set.
In this paper, we count the number of correctly clas sified questions by two different precision standards P 1 and P 5 . Suppose k. ilabels are output for the i th question (k i  5) and are ranked in a decreasing order according to their density values.
Overall, we get a98.80% precision for coarse classes with all the fea tures and 95% for the fine classes.
P =5 Word Pos Chunk NE Head RelWord Coarse 92.00 96.60 97.00 97.00 97.80 98.80 Fine 86.00 86.60 87.60 88.60 89.40 95.00Table 2: Classification results of the hierarchical clas sifier on 500 TREC 10 questions.
Training is done on 5,500 questions.
No. Train Test P 1 P =5 1 1000 500 83.80 95.60 2 2000 500 84.80 96.40 3 3000 500 91.00 98.00 4 4000 500 90.80 98.00 Table 3: Classification accuracy for coarse classes ondifferent training sets using the feature set RelWord.
Train Test P 1 P =5 1 1000 500 71.00 83.80 2 2000 500 77.80 88.20 3 3000 500 79.80 90.60 4 4000 500 80.00 91.20Table 4: Classification accuracy for fine classes on different training sets using the feature set RelWord.
We define the tendency of Class i to be confused with Class j as follows: D ij = Err ij  2=(N i + N j ); (3) where (when using P 1 ), Err ij is the number ofquestions in Class i that are misclassified as belong P 1 Word Pos Chunk NE Head RelWord h 77.60 78.20 77.40 78.80 78.80 84.20 f 52.40 77.20 77.00 78.40 76.80 84.00 P =5 Word Pos Chunk NE Head RelWord h 86.00 86.60 87.60 88.60 89.40 95.00 f 83.20 86.80 86.60 88.40 89.80 95.60 Table 5: Comparing accuracy of the hierarchical (h) and flat (f) classifiers on 500 TREC 10 question; training is done on 5,500 questions.
Results are shown for different feature sets using P 1 and P 5 . Fine Classes 1?50 Fi ne C la ss es 1 ?5 0 2 24 28 32 37 50 2 24 28 32 37 50 Figure 2: The gray?scale map of the matrix D[n,n].
