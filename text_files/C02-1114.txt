PoS-tagged Corpus In this section we describe how a graph ? a collection of nodes and links ? was built to represent the relationships between nouns.
Themodel was built using the British National Cor pus which is automatically tagged for parts of speech.
Initially, grammatical relations between pairsof words were extracted.
The relationships ex tracted were the following: ? Noun (assumed to be subject) Verb ? Verb Noun (assumed to be object) ? Adjective Noun?
Noun Noun (often the first noun is modify ing the second) ? Noun and/or Noun The last of these relationships often occurs when the pair of nouns is part of a list.
Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998).
In this paper we too focus on nouns co-occurring in lists.
This is be cause the noun and/or noun relationship is the only symmetric relationship in our model, andsymmetric relationships are much easier to ma nipulate than asymmetric ones.
Our full graph contains many directed links between words of different parts of speech.
Initial experiments with this model show considerable promise but are at too early a stage to be reported upon yet.Thus the graph used in most of this paper repre sents only nouns.
Each node represents a noun and two nodes have a link between them if they co-occur separated by the conjunctions and or or, and each link is weighted according to the number of times the co-occurrence is observed.Various cutoff functions were used to deter mine how many times a relationship must be observed to be counted as a link in the graph.
A well-behaved option was to take the top nneighbours of each word, where n could be determined by the user.
In this way the link weighting scheme was reduced to a link-ranking scheme.
One consequence of this decision was that links to more common words were preferred over links to rarer words.
This decision may have effectively boosted precision at the expense of recall, because the preferred links are to fairlycommon and (probably) more stable words.
Re search is need to reveal theoretically motivatedor experimentally optimal techniques for select ing the importance to assign to each link ? the choices made in this area so far are often of an ad hoc nature.
The graph used in the experiments described has 99,454 nodes (nouns) and 587,475 links.
There were roughly 400,000 different types tagged as nouns in the corpus, so the graph model represents about one quarter of these nouns, including most of the more common ones.
Extracting Categories of Similar Words In this section we describe a new algorithm for adding the ?most similar node?
to an existingcollection of nodes in a way which incremen tally builds a stable cluster.
We rely entirelyupon the graph to deduce the relative importance of relationships.
In particular, our algo rithm is designed to reduce so-called ?infections?(Roark and Charniak, 1998, ?3) where the inclu sion of an out-of-category word which happens to co-occur with one of the category words can significantly distort the final list.
Here is the process we use to select and add the ?most similar node?
to a set of nodes: Definition 1 Let A be a set of nodes and let N(A), the neighbours of A, be the nodes which are linked to any a ? A.
(So N(A) = ? a?AN(a).)
The best new node is taken to be the node b ? N(A)\A with the highest proportion of links to N(A).
More precisely, for each u ? N(A)\A, let the affinity between u and A be given by the ratio |N(u) ?N(A)| |N(u)| . The best new node b ? N(A) \ A is the node which maximises this affinity score.
This algorithm has been built into an on-line demonstration where the user inputs a givenseed word and can then see the cluster of re lated words being gradually assembled.
The algorithm is particularly effective atavoiding infections arising from spurious co occurrences and from ambiguity.
Consider, forexample, the graph built around the word ap ple in Figure 6.
Suppose that we start with the seed-list apple, orange, banana.
However many times the string ?Apple and Novell?
occurs in the corpus, the novell node will not be addedto this list because it doesn?t have a link to or ange, banana or any of their neighbours except for apple.
One way to summarise the effect of this decision is that the algorithm adds words to clusters depending on type frequency rather than token frequency.
This avoids spurious links due to (for example) particular idioms rather than geniune semantic similarity.
In this section we give examples of lexical cat egories extracted by our method and evaluatethem against the corresponding classes in Word Net.
5.1 Methodology.
Our methodology is as follows.
Consider an intuitive category of objects such as musical instruments.
Define the ?WordNet class?
or ?WordNet category?
of musical instruments tobe the collection of synsets subsumed in Word Net by the musical instruments synset.
Take a ?protypical example?
of a musical instrument, such as piano.
The algorithm defined in (1) gives a way of finding the n nodes deemed to be most closely related to the piano node.
Thesecan then be checked to see if they are members of the WordNet class of musical instru ments.
This method is easier to implement and less open to variation than human judgements.
While WordNet or any other lexical resource isnot a perfect arbiter, it is hoped that this exper iment procedure is both reliable and repeatable.
The ten classes of words chosen were crimes, places, tools, vehicles, musical instruments, clothes, diseases, body parts, academic subjects and foodstuffs.
The classes were chosen beforethe experiment was carried out so that the re sults could not be massaged to only use thoseclasses which gave good results.
(The first 4 cat egories are also used by (Riloff and Shepherd, 1997) and (Roark and Charniak, 1998) and so were included for comparison.)
Having chosen these classes, 20 words were retrieved using asingle seed-word chosen from the class in ques tion.
This list of words clearly depends on the seed word chosen.
While we have tried to optimise this choice, it depends on the corpus and thethe model.
The influence of semantic Proto type Theory (Rosch, 1988) is apparent in this process, a link we would like to investigate in more detail.
It is possible to choose an optimal seed word for a particular category: it should be possible to compare these optimal seed wordswith the ?prototypes?
suggested by psychologi cal experiments (Mervis and Rosch, 1981).
5.2 Results.
The results for a list of ten classes and proto typical words are given in Table 1.
Words which are correct members of the classes sought arein Roman type: incorrect results are in italics.
The decision between correctness and in correctness was made on a strict basis for thesake of objectivity and to enable the repeata bility of the experiment: words which are in WordNet were counted as correct results only if they are actual members of the WordNet class in question.
Thus brigandage is not regarded as a crime even though it is clearly an act ofwrongdoing, orchestra is not regarded as a musical instrument because it is a collection of in struments rather than a single instrument, etc. The only exceptions we have made are the terms wynd and planetology (marked in bold), whichare not in WordNet but are correct nonethe less.
These conditions are at least as stringent as those of previous experiments, particularly those of Riloff and Shepherd (1997) who also give credit for words associated with but not belonging to a particular category.
(It has been pointed out that many polysemous words may occur in several classes, making the task easier because for many words there are several classes which our algorithm would give credit for.)With these conditions, our algorithm re trieves only 36 incorrect terms out of a total of 200, giving an accuracy of 82%.
5.3 Analysis.
Our results are an order of magnitude better than those reported by Riloff and Shepherd (1997) and Roark and Charniak (1998), whoreport average accuracies of 17% and 35% re spectively.
(Our results are also slightly better than those reported by Riloff and Jones (1999)).
Since the algorithms used are in many waysvery similar, this improvement demands expla nation.Some of the difference in accuracy can be at tributed to the corpora used.
The experiments in (Riloff and Shepherd, 1997) were performed on the 500,000 word MUC-4 corpus, and those of (Roark and Charniak, 1998) were performedusing MUC-4 and the Wall Street Journal cor pus (some 30 million words).
Our model was built using the British National Corpus (100 million words).
On the other hand, our modelwas built using only a part-of-speech tagged cor pus.
The high accuracy achieved thus questions the conclusion drawn by Roark and Charniak (1998) that ?parsing is invaluable?.
Our results clearly indicate that a large PoS-tagged corpusmay be much better for automatic lexical ac quisition than a small fully-parsed corpus.
This claim could of course be tested by comparing techniques on the same corpus.To evaluate the advantage of using PoS infor mation, we compared the graph model with asimilarity thesaurus generated using Latent Se mantic Indexing (Manning and Schu?tze, 1999, Ch 15), a ?bag-of-words?
approach, on the samecorpus.
The same number of nouns was re trieved for each class using the graph model and LSI.
The LSI similarity thesaurus obtained an accuracy of 31%, much less than the graph model?s 82%.
This is because LSI retrieves words which are related by context but are not in the same class: for example, the neighbours of piano found using LSI cosine-similarity on the BNC corpus include words such as composer,music, Bach, concerto and dance, which are re lated but certainly not in the same semantic class.The incremental clustering algorithm of Def inition (1) works well at preventing ?infections?
Class Seed Word Neighbours Produced by Graph Model crimes murder crime theft arson importuning incest fraud larceny parricideburglary vandalism indecency violence offences abuse brig andage manslaughter pillage rape robbery assault lewdness places park path village lane viewfield church square road avenue garden castle wynd garage house chapel drive crescent home place cathedral street tools screwdriver chisel naville nail shoulder knife drill matchstick morgenthau gizmo hand knee elbow mallet penknife gallie leg arm sickle bolster hammer vehicle conveyance train tram car driver passengers coach lorry truck aeroplane coons plane trailer boat taxi pedestrians vans vehicles jeep bus buses helicopter musical instruments piano fortepiano orchestra marimba clarsach violin cizek viola oboeflute horn bassoon culbone mandolin clarinet equiluz contra bass saxophone guitar cello clothes shirt chapeaubras cardigan trousers breeches skirt jeans boots pair shoes blouse dress hat waistcoat jumper sweater coat cravat tie leggings diseases typhoid malaria aids polio cancer disease atelectasis illnesses cholerahiv deaths diphtheria infections hepatitis tuberculosis cirrho sis diptheria bronchitis pneumonia measles dysentery body parts stomach head hips thighs neck shoulders chest back eyes toes breasts knees feet face belly buttocks haws ankles waist legs academic subjectsphysics astrophysics philosophy humanities art religion science politics astronomy sociology chemistry history theology eco nomics literature maths anthropology culture mathematics geography planetology foodstuffs cake macaroons confectioneries cream rolls sandwiches croissant buns scones cheese biscuit drinks pastries tea danish butter lemonade bread chocolate coffee milk Table 1: Classes of similar words given by the graph model.
and keeping clusters within one particular class.
The notable exception is the tools class, where the word hand appears to introduce infection.
In conclusion, it is clear that the graph modelcombined with the incremental clustering algo rithm of Definition 1 performs better than mostprevious methods at the task of automatic lex ical acquisition.
So far we have presented a graph model built upon noun co-occurrence which performs much better than previously reported methods at the task of automatic lexical acquisition.
This isan important task, because assembling and tuning lexicons for specific NLP systems is increas ingly necessary.
We now take a step furtherand present a simple method for not only as sembling words with similar meanings, but for empirically recognising when a word has several meanings.
Recognising and resolving ambiguity is an important task in semantic processing.
The traditional Word Sense Disambiguation(WSD) problem addresses only the ambiguityresolution part of the problem: compiling a suit able list of polysemous words and their possiblesenses is a task for which humans are tradition ally needed (Kilgarriff and Rosenzweig, 2000).This makes traditional WSD an intensively supervised and costly process.
Breadth of cover age does not in itself solve this problem: general lexical resources such as WordNet can provide too many senses many of which are rarely used in particular domains or corpora (Gale et al, 1992).The graph model presented in this paper suggests a new method for recognising relevant polysemy.
We will need a small amount of termi nology from graph theory (Bolloba?s, 1998).
Definition 2 (Bolloba?s, 1998, Ch 1 ?1) Let G = (V,E) be a graph, where V is the set of vertices (nodes) of G and E ? V ? V is the set of edges of G. ? Two nodes v1, vn are said to be connected if there exists a path {v1, v2, . . .
, vn?1, vn} such that (vj , vj+1) ? E for 1 ? j < n. ? Connectedness is an equivalence relation.?
The equivalence classes of the graph G un der this relation are called the components of G. We are now in a position to define the senses of a word as represented by a particular graph.
Definition 3 Let G be a graph of words closely related to a seed-word w, and let G \ w be the subgraph which results from the removal of the seed-node w. The connected components of the subgraph G \ w are the senses of the word w with respect to the graph G. As an illustrative example, consider the localgraph generated for the word apple (6).
The re moval of the apple node results in three separate components which represent the different senses of apple: fruit, trees, and computers.
Definition 3 gives an extremely good model of the senses of apple found in the BNC.
(In this case better than WordNet which does not contain the very common corporate meaning.)The intuitive notion of ambiguity being pre sented is as follows.
An ambiguous word often connects otherwise unrelated areas of meaning.
Definition 3 recognises the ambiguity of apple because this word is linked to both banana and novell, words which otherwise have nothing to do with one another.
It is well-known that any graph can be thought of as a collection of feature-vectors, forexample by taking the row-vectors in the adja cency matrix (Bolloba?s, 1998, Ch 2 ?3).
Theremight therefore be fundamental similarities be tween our approach and methods which rely on similarities between feature-vectors.Extra motivation for this technique is pro vided by Word-Sense Disambiguation.
Thestandard method for this task is to use hand labelled data to train a learning algorithm, which will often pick out particular words as Bayesian classifiers which indicate one sense or the other.
(So if microsoft occurs in the same sentence as apple we might take this as evidence that apple is being used in the corporate sense.)
Clearly, the words in the different componentsin Diagram 6 can potentially be used as classi fiers for just this purpose, obviating the need fortime-consuming human annotation.
This tech nique will be assessed and evaluated in future experiments.
DemonstrationAn online version of the graph model and the in cremental clustering algorithm described in this paper are publicly available 1 for demonstrationpurposes and to allow users to observe the gen erality of our techniques.
A sample output is included in Figure 6.
Acknowledgements The authors would like to thank the anonymous reviewers whose comments were a great help inmaking this paper more focussed: any short comings remain entirely our own responsibility.
This research was supported in part by theResearch Collaboration between the NTT Communication Science Laboratories, Nippon Tele graph and Telephone Corporation and CSLI,Stanford University, and by EC/NSF grant IST 1999-11438 for the MUCHMORE project.
2 1http://infomap.stanford.edu/graphs 2http://muchmore.dfki.deFigure 1: Automatically generated graph show ing the word apple and semantically related nouns
