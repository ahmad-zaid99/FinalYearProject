To model the relationship between uplift features and the treatment effect and classification features and outcome probability, we implement six types of association patterns in the data generation process: linear, quadratic, cubic, ReLU (Rectified Linear Unit [25]), trigonometric function sine, and cosine.
We evaluate eight feature selection methods, including five filter methods (F filter, LR filter, KL filter, Chi filter, and ED filter), two embedded methods (Two Model embedded and KL embedded), and one standard embedded method for classification as a benchmark (feature importance based on random forest classifier denoted as ‚Äúoutcome embedded‚Äù).
For all the meta-learners, we use a random forest classifier as the base learner. 
In the simulation, all the random forest classifiers in the meta-learners and uplift random forest share the same hyper-parameter values: the number of trees is 10, the maximum tree depth is 10, the minimum sample size in leaf to perform split is 100, and the maximum number of features for split is 3.
We use the mean RMSE of the t = 100 trials (RMSE) to make the dot plot and calculate the confidence intervals as RMSE  1:96  (RMSE)=pt, where (RMSE) is the standard error of the RMSE across trials.
The uplift model variants considered are: (1) TwoModel-LR, XLearner-LR, RLearner-LR using f Logistic Regression Classifier & Linear Regression Regressor g as base learners; (2) TwoModel-LGBM, XLearner-LGBM, RLearner-LGBM using f Gradient Boosting Classifier & Gradient Boosting Regressor g from Light- GBM implementation [26] as base learners, with hyperparameter values (n estimators = 100; max depth = 8; min child samples = 100); (3) TwoModel-RF, XLearner- RF, RLearner-RF using f Random Forest Classifier & Random Forest Regressor g as base learners, with hyperparameter values (n estimators = 100; max depth = 8; min samples leaf = 100); (4) KL-RF as the uplift random forest using KL divergence criterion with hyperparameter values (n estimators = 10; max depth = 8; min samples leaf = 100).
