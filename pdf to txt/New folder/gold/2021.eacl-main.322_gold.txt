In general, APE can be considered as a task of sequence-to-sequence supervised learning, which requires a considerable amount of human-annotated data.
However, constructing an APE corpus—a set of triplets (Table 1), each of which includes a source text (src), a machine-translated text (mt), and a manually postedited text (pe)—is labor-intensive work because post-editors should create pe in principle by minimally editing mt while preserving the meaning of src. 
In fact, the sizes of currently available ‘genuine’ APE corpora provided by WMT (Bojar et al., 2016, 2017; Chatterjee et al., 2018, 2019, 2020) are too small to train deep APE models effectively.
To overcome the lack of genuine APE corpora, several previous studies have proposed methods to construct synthetic training datasets (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018; Lee et al., 2020), and they appear to be partially helpful in mitigating the data scarcity problem. One such study is eSCAPE (Negri et al., 2018), which has been shown to be effective in training deep models and adopted in a number of APE works (do Carmo et al., 2020).
Utilizing parallel corpora, which comprise pairs of a source sentence (src) and a reference sentence (ref ), eSCAPE was constructed as a set of synthetic APE triplets in the form of (src, mt, ref ) where mt is a machine translation of src, and ref serves as an alternative to pe of a genuine APE triplet.
Beyond the eSCAPE corpus, to yield a more convincing error distribution as well as to supply APE models with more APE resources made out of limited parallel resources, we propose synthetic-data generation methods that can be seen as adaptations of back-translation to the APE task in terms of creating synthetic mt, which is one of the two sources of APE.
3.1 Forward Generation The ‘Forward Generation’ (FG) method lets an APE model take src and mt as input to produce mtFG as output by partially correcting mt through the forward path of APE; the training objective of an FG model is identical to that of a normal APE model (Eq. 1).
We use such triplets to construct a new set of synthetic triplets eSCAPEFG.
However, if the distance between mtFG and ref is excessively small, indicating that the two texts are almost identical, APE models trained on eSCAPEFG may not learn error-correction patterns sufficiently.
Thus, unlike the standard training procedure, we force the FG model’s training process to stop earlier before convergence, making the remaining errors in its output mtFG ample.
Metric. Following the evaluation setting used in the WMT APE shared task, we adopt TER (Snover et al., 2006) as the primary metric to measure the distance between the model’s prediction and the reference text; and BLEU (Koehn et al., 2007) as the secondary metric to measure the degree of ngram match.
We use two kinds of APE datasets: human-made APE datasets, which are provided by WMT, and eSCAPE.
Both are English–German (EN–DE) APE corpora; they are further categorized according to their subtask depending on whether the target MT system is a phrase-based statistical MT (PBSMT) system or a neural MT (NMT) system.
We tokenized all words in our datasets into sub-word units by using SentencePiece (Kudo and Richardson, 2018).
We implemented a Transformer-based APE model, the “sequential” model proposed by Lee et al.
(2019), which again follows almost the same setting of the “base” Transformer described in the original paper Vaswani et al.
We use this model both as generation models that create synthetic mt with our two proposed methods and also as the final APE models to examine the effectiveness of those synthesized data as additional training data.
We observed that when eSCAPEFG or eSCAPEBG is used instead of eSCAPE, the APE model’s performance does not make a big difference from the eSCAPE
baseline.
Nevertheless, we found that when we augment eSCAPE with eSCAPEFG and/or eSCAPEBG, the trained APE model shows consistent improvements in its APE performance and most of the improvements upon the eSCAPE baseline are statistically significant.
Moreover, the results also surpass current state-of-the-art (except the ensemble models) APE models (Correia and Martins, 2019; Lopes et al., 2019), which are built on top of BERT (Devlin et al., 2019), thus contain more model parameters, and exploit a huge amount of monolingual data.
We expect that these results are because, in addition to an increase in the total quantity of training samples, the integration of multiple synthetic datasets, each of which focuses on different aspects of APE from the other—eSCAPE contains actual MT outputs; on the other hand, synthetic triplets better satisfy the minimal-edit criterion—appears to have an effect on the models’ APE performance.
