@&#MAIN-TITLE@&#Noisy Self-Knowledge Distillation for Text Summarization

@&#ABSTRACT@&#

In this paper we apply self-knowledge distillation to text summarization which we argue can alleviate problems with maximumlikelihood training on single reference and noisy datasets. Instead of relying on one-hot annotation labels, our student summarization model is trained with guidance from a teacher which generates smoothed labels to help regularize training. Furthermore, to better model uncertainty during training, we introduce multiple noise signals for both teacher and student models. We demonstrate experimentally on three benchmarks that our framework boosts the performance of both pretrained and nonpretrained summarizers achieving state-of theart results.

@&#INTRODUCTION@&#

Automatic summarization has enjoyed renewed interest in recent years, thanks to the popularity of neural network models and their ability to learn continuous representations without recourse to preprocessing tools or linguistic annotations. The availability of large-scale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018) containing hundreds of thousands of document-summary pairs has driven the development of neural architectures for summarization. Several approaches have been proposed, in the vast majority sequence-to-sequence models which are trained in an end-to-end fashion with a maximum likelihood estimation loss (See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018; Gehrmann et al., 2018). Despite promising results, there are specific characteristics of the summarization task which render it ill-suited to standard sequence-to-sequence training. For instance, maximum-likelihood training on single reference datasets might not be optimal for summarization which is subject to a great deal of human variation (Harman and Over, 2004; Nenkova, 2006). In the context of extractive summarization, different people select different sentences to include in a summary (Rath et al., 1961), and when writing abstracts, disagreement exists both in terms of writing style and the specific content deemed important for the summary (Harman and Over, 2004). Although summarization models would naturally benefit from multiple target references, it is unrealistic to expect that multi-reference datasets can be created at scale for neural network training. In fact, most popular benchmarks are collated opportunistically, based on summaries which only loosely correspond to the source input. For example, Narayan et al. (2018) create a dataset by pairing the first sentence of a news article with the rest of the document under the assumption that the introductory sentence expresses the gist of the article. Grusky et al. (2018) pair articles with metadata available in HTML pages under the assumption that HTML tags (e.g., description) denote summary-like content. In other work (Liu et al., 2018; Perez-Beltrachini et al., 2019), multidocument summarization datasets are created by viewing lead sections in Wikipedia articles as summaries of documents cited therein. The inherent noise in the data collection process further hampers training with models often being prone to hallucination (Song et al., 2018; Maynez et al., 2020), and struggling to identify which content units are salient (Tan et al., 2017). In this paper, we propose to alleviate these problems by turning to knowledge distillation (Bucilu et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015; Kim and Rush, 2016). Knowledge distillation transfers knowledge from a larger “teacher” network to a smaller “student” model by training the student to imitate the teacher’s outputs (in addition to learning from the training data set). In “born-again networks”, (Furlanello et al., 2018) the teacher and student have the same neural archiarXiv:2009.07032v2 [cs.CL] 27 Jul 2021 tecture and model size, and yet surprisingly the student is able to surpass the teacher’s accuracy. Intuitively, such self-knowledge distillation is effective because the teacher’s output distribution provides a richer training signal capturing additional information about training examples. In the context of summarization, the teacher can benefit student training in two ways. It provides a softened distribution over reference summaries thereby enriching the single reference setting. Moreover, the teacher’s distribution is (to a certain extent) denoised enabling the student to circumvent inaccuracies in the training data. We further capitalize on the idea that both the teacher and the student should be robust to noise and introduce several noise injection techniques which together with knowledge distillation improve model generalization and performance. We present experiments on several summarization benchmarks (Narayan et al., 2018; PerezBeltrachini et al., 2019; Hermann et al., 2015) covering single- and multi document summarization settings as well as different types of summaries (e.g., verbose or more telegraphic). Across datasets, the proposed framework boosts the performance of pretrained and non-pretrained abstractive summarizers, achieving new state-of-the art results.

@&#BACKGROUND@&#

2.1 Neural Abstractive Summarization 

Neural approaches to abstractive summarization conceptualize the task as a sequence-to-sequence problem, where the encoder maps the sequence of tokens in the source document x = [x1, ..., xn]to a sequence of continuous representations  z = [z1, ..., zn], and the decoder autoregressively generates the target summary y = (y1, ..., ym) token-by-token, hence modeling the conditional probability p(y1, ..., ym|x1, ..., xn). Rush et al. (2015) and Nallapati et al. (2016) were among the first to apply the neural encoderdecoder architecture to text summarization. See et al. (2017) enhance this model with a pointergenerator network which allows to copy words from the source text, and a coverage mechanism which keeps track of words that have been summarized. Other work develops abstractive models trained end-to-end with reinforcement learning based on multiple encoders and hierarchical attention (Celikyilmaz et al., 2018) or a coverage mechanism where the decoder attends over previously generated words (Paulus et al., 2018). Gehrmann et al. (2018) follow a bottom-up approach where a content selector first determines which phrases in a source document should be part of the summary, and a copy mechanism is applied only to preselected phrases during decoding. Although the majority of summarization systems are composed of LSTM units, Narayan et al. (2018) and (PerezBeltrachini et al., 2019) propose abstractive models based on convolutional neural networks. Pretrained language models have recently emerged as a key technology for achieving impressive gains in abstractive summarization (Liu and Lapata, 2019; Lewis et al., 2020; Song et al., 2019). These models first pretrain a language model with self-supervised objectives on large corpora and then fine-tune it on summarization datasets. Liu and Lapata (2019) combine a pretrained encoder based on BERT (Devlin et al., 2019) with a randomly initialized decoder, demonstrating substantial gains on summarization performance. Song et al. (2019) pretrain an encoder-decoder framework to reconstruct (masked) fragments within a sentence and then fine-tune it on summarization datasets. In the same vein, Lewis et al. (2020) present BART, an encoder-decoder Transformer (Vaswani et al., 2017), pretrained by reconstructing a text corrupted with several arbitrary noising functions. Bao et al. (2020) design UNILMv2, a Transformer-based neural network pretrained as a pseudo-masked language model. Qi et al. (2020) introduce their own novel self-supervised task based on future n-gram prediction.

2.2 Knowledge Distillation

Knowledge Distillation refers to a class of methods for training a new smaller student network by learning from a teacher network (in addition to learning from the training data). It is generally assumed that the teacher has been previously trained, and the parameters for the student are estimated by matching the student’s predictions to the teacher. Let T and S denote teacher and student models, respectively. Let fT and fS be functions of the teacher and student. The models are typically neural networks and function f can be in principle defined using the output of any network layer (e.g., a hidden or softmax layer). Knowledge distillation methods are commonly expressed as minimizing an objective function over training set X : LKD = X xi∈Xl(fT (xi), fS(xi)) (1) where l() is a loss function that penalizes the difference between the teacher and the student. Specific instantiations of this general framework include minimizing the teacher/student difference based on output logits, intermediate hidden representations, attention maps, and derivatives of the loss to the input (Ba and Caruana, 2014; Romero et al., 2014; Zagoruyko and Komodakis, 2017; Czarnecki et al., 2017). Other work integrates an ensemble of teachers in order to improve the student (Urban et al., 2016), trains a succession of students (Furlanello et al., 2018), introduces a “teacher assistant” for better knowledge transfer (Mirzadeh et al., 2019), and regularizes multi-task agents (Parisotto et al., 2015; Teh et al., 2017) in reinforcement learning. Compared to direct training, knowledge distillation provides a more stable training process which leads to better performing student models (Hinton et al., 2015; Phuong and Lampert, 2019). Recent work (Furlanello et al., 2018; Hahn and Choi, 2019) also sheds light on leveraging knowledge distillation for training a high performing student model with the same size as the teacher (see the discussion in the next section). Knowledge distillation has been also shown to improve results for various NLP tasks. Tan et al. (2019) use it to transfer knowledge from BERT to smaller models, helping them approach or exceed the quality of much larger pretrained neural networks. Aside from distilling large models into smaller ones (Kim and Rush, 2016; Mou et al., 2016) or ensembles of models into single models (Kuncoro et al., 2016; Liu et al., 2019), knowledge distillation has been further used in multi-task learning, e.g., to teach a multi-task student from single-task teachers (Clark et al., 2019).

@&#EXPERIMENTS@&#

In this section, we describe the summarization datasets used in our experiments and discuss various implementation details.

4.1 Summarization Datasets

We evaluated our model on two singledocument summarization datasets, namely the CNN/DailyMail news highlights (Hermann et al., 2015) and XSum (Narayan et al., 2018), and one multi-document summarization dataset, i.e., WikiCatSum (Perez-Beltrachini et al., 2019). These datasets represent different summary styles ranging from highlights to very brief-one sentence summaries. The summaries also vary with respect to the type of rewriting operations they exemplify (e.g., CNN/DailyMail showcases more cut and paste operations while XSum is genuinely abstractive). Finally, two of these datasets (XSum and WikiCatSum) were created automatically following various assumptions about the correspondence of purported summaries to the source input. CNN/DailyMail contains news articles and associated highlights, i.e., a few bullet points written by journalists which give a brief overview of the article. We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents). We did not anonymize entities. Sentences were split with the Stanford CoreNLP toolkit (Manning et al., 2014) and the dataset was pre-processed following See et al. (2017). Input documents were truncated to 512 tokens. XSum contains 226,711 news articles accompanied with a one-sentence summary, answering the question “What is this article about?”. We used the splits of Narayan et al. (2018) for training, validation, and testing (204,045/11,332/11,334) and followed the pre-processing introduced in their work. Input documents were also truncated to 512 tokens. WikiCatSum is a multi-document summarization dataset derived from WikiSum (Liu et al., 2018). The target summary is the lead section of a Wikipedia article, and the source input are webpages related to this article. WikiCatSum (PerezBeltrachini et al., 2019) represents three domains from the original Wikisum dataset under the assumption that these vary in terms of the topics the summaries discuss and their linguistic characteristics. Aside from the summaries, the dataset contains the input webpages whose length is truncated to the first 800 tokens. WikiCatSum contains 62,545 samples for the Company domain, 59,973 samples for the Film domain, and 60,816 samples for the Animal domain.

4.2 Implementation Details

For all datasets, we evaluated our self-knowledge distillation framework in two settings. In the first setting, our models are non-pretrained while in the second setting we take advantage of pretrained language models which have demonstrated impressive improvements in summarization (Lewis et al., 2020; Liu and Lapata, 2019; Bao et al., 2020). Specifically, we adopt UNILMv2 (Bao et al., 2020) as the pretrained model. UNILMv2 is a Transformer-based neural network (Vaswani et al., 2017) with 12 Transformer layers and 12 attention heads. It is pretrained as a pseudo masked language model on a large corpus (label smoothing is applied with smoothing factor 0.1). We fine-tuned our teacher models following the procedure outlined in Bao et al. (2020). In the non pretrained setting, we adopt a Transformer encoder-decoder model with 6 layers, 768 hidden size and 2,048 feed-forward filter size. Label smoothing was also used with smoothing factor 0.1. All teacher models in this setting were trained from randomly initialized parameters following Liu and Lapata (2019). In all knowledge distillation experiments, student models have the same neural network architecture with their teachers and are trained with the same hyperparameters as the teacher models. The best teacher and student model are selected by evaluating perplexity on the development set. For noisy distillation models, word drop probability pd was set to 0.1. The candidate length k for word replacement was 10 and word replacement probability pr was 0.1. Sentence drop probability ps was 0.05. During decoding we used beam search (size 5), and tuned α for the length penalty (Wu et al., 2016) between 0.6 and 1 on the validation set; we decode until an end-of-sequence token is emitted. Repeated trigrams are blocked (Paulus et al., 2018).

@&#RESULTS@&#

5.1 Automatic Evaluation 

We evaluated summarization quality automatically using ROUGE (Lin, 2004). We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a means of assessing informativeness and the longest  common subsequence (ROUGE-L) as a means ofassessing fluency. Examples of system output are shown in Table 5. Table 1 summarizes our results on the CNN/DailyMail and XSum (single document) datasets. The first block includes the results of non-pretrained models. We present the LEAD baseline (which simply selects the first three sentences in a document for CNN/DailyMail and the first sentence for XSum). We also report the results of See et al.’s (2017) pointer generator network (PTRNET), and an abstractive system from Liu and Lapata (2019) based on Transformers (TransformerAbs; see Section 4.2 for details). The latter forms the backbone of our self-knowledge distillation models (SKD). We present a variant without noise (+SKD), a variant with noise in the teacher training signal (+Noisy T), and a third variant where the student is additionally trained on noisy data (+Noisy S). The second and third blocks in Table 1 include the results of pretrained models. To make comparisons fairer, we separate LARGE- (second block) from BASE-size (third block) pretrained models based on parameter size (shown within parentheses). With regard to LARGE-size models, we report the results of three very strong summarization systems finetuned with UNILMLARGE (Bao et al., 2020), BARTLARGE (Lewis et al., 2020), and T511B (Raffel et al., 2019). Our BASE-size models include BERTSUMBASE (Liu and Lapata, 2019), a summarizer based on a BASE-size BERT encoder and a randomly initialized decoder, MASSBASE (Song et al., 2019) and UNILMBASE which are both finetuned with BASE-size pretrained models. As can be seen in Table 1, SKD improves over teacher models in both pretrained (BASE-size) and non-pretrained settings. We also observe that injection of noise brings further improvements with noise in the training signal (+Noisy T) seeming more effective compared to noisy data augmentation (+Noisy S). Overall, we obtain competitive results with SKD and BASE-size pretrained models and even manage to outperform UNILMLARGE and T511B on the CNN/DailyMail dataset. Table 2 presents experimental results on the WikiCatSum dataset. The first block in the table includes results for non-pretrained models. CV-S2S and CV-S2D (Perez-Beltrachini et al., 2019) are convolutional encoder-decoder models. The former is a standard convolutional decoder, while the latter adopts a hierarchical convolutional decoder which first generates target sentence vectors, and then generates target words based on sentence vectors. TF-S2S is a standard Transformer encoder-decoder model trained on WikiCatSum (Perez-Beltrachini et al., 2019). TF-S2S is the model used in our SKD system and its noisy version (+Noisy T, +Noisy S). The second block includes the results of a system using the BASE-size pretrained model UNILMBASE on its own and with SKD. Results are reported per domain (Company, Film, and Animal) and across domains (All). Under pretrained and non-pretrained settings, we observe that SKD boosts the performance of the teacher model (UNILMBASE and TF-S2S, respectively) and that the injection of noise is beneficial. Improvements in performance vary across domains, with Film showing the least gains. Column All in Table 2 shows average ROUGE across domains. Although SKD and noise injection improve results, we observe that non pretrained models benefit more. 

5.2 Factual Consistency Evaluation

Besides ROUGE, we also use FactCC (Krysci ´ nski ´ et al., 2019) to evaluate the factual correctness of the generated summaries. FactCC is a BERT-based classifier trained to identify conflicts between a source document and a generated summary. Given a document-sentence pair as input, it assigns a positive label if factual information mentioned in a summary sentence is consistent with the document, otherwise it assigns a negative label. We view the percentage of positive labels assigned by FactCC to all generated summaries as a factual correctness score for a summarization system. We performed experiments with the publicly released version of FactCC.2 Our results on the CNN/DailyMail and XSum datasets are presented in Table 3. Here, we only focus on single-document summarization, as there is no version of FactCC trained on multi-document datasets. As can be seen, the application of SKD (trained with noisy signals and on noisy data) improves factual consistency for non-pretrained and pretrained models on both datasets. All +Noisy SKD students are significantly (p < 0.05) more factually correct compared to their teachers (TransformerAbs and UNILMv2BASE), using a paired student t-test.

5.3 Human Evaluation

In addition to automatic evaluation, we also assessed system output by eliciting human judgments. We compared the quality of the summaries produced by a teacher model (UNILMv2BASE) against its distilled student (+Noisy SKD). For CNN/DailyMail and XSum, human participants were presented with the output of two systems (and the original document) and asked to decide which one was better according to the following criteria: Succinctness (Does the summary avoid repetition?), Informativeness (Does the summary capture the document’s most important information?), and Fluency (Is the summary fluent and grammatical?). Evaluation was conducted on the Amazon Mechanical Turk crowdsourcing platform. We used the same test documents (20 in total) from Liu and Lapata (2019) for both CNN/DailyMail and XSum. We elicited five responses per HIT. Systems were rated along each dimension, and assigned a score corresponding to the proportion of times a system was selected as better against another. Human evaluation results are shown in Table 4 (upper part). On both CNN/DailyMail and XSum datasets participants perceive the student (+Noisy SKD) as significantly (p < 0.05) more succinct and informative compared to the teacher (UNILMv2BASE). However, on Fluency, the student tends to be worse. Upon inspection we found student summaries to be rather telegraphic, and hypothesize that crowdworkers tend to penalize them in terms of fluency, even though they are grammatical. Human evaluation was performed slightly different for WikiCatSum. Recall that this is a multidocument dataset, where input documents are discontinuous webpage fragments. To allow participants to perform the experiment in a timely fashion, we used the gold summary as a proxy for the content of the input. Crowdworkers were presented with the output of two systems (again UNILMv2BASE and +Noisy SKD) and asked to decide which one was better according to the information contained in the gold summary. Evaluation was conducted on AMT, we randomly selected 20 samples from the test set and elicited three responses per HIT. For each domain, we report the proportion of times a system was chosen as better. Human evaluation results are shown in Table 4 (lower part). AMT Crowdworkers prefer the summaries produced by the student for the Animal and Film domains, but not for Company; we found that the distilled model tends to generate too many entities in one sentence which render the summaries too dense for this domain.

@&#CONCLUSION@&#

In this paper we advocated the use of selfknowledge distillation for abstractive summarization, as a means to alleviate problems associated with maximum-likelihood training for this task. We also introduced several noise functions (in the training signal and training data) which help regularize training and further boost performance. Experiments on three benchmark datasets demonstrate that our framework can improve both nonpretrained and pretrained summarizers. In the future we would like to investigate more thoroughly which aspects of pretrained models improve and how self-knowledge distillation can be enhanced with more sophisticated noise functions.

@&#ACKNOWLEDGMENT@&#
 
We gratefully acknowledge the support of the European Research Council (Lapata, award number 681760, “Translating Multiple Modalities into Text”). 