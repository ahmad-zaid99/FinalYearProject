@&#MAIN-TITLE@&#Continual Learning for Text Classification with Information Disentanglement Based Regularization

@&#ABSTRACT@&#

Continual learning has become increasingl important as it enables NLP models to constantly learn and gain knowledge over time. Previous continual learning methods are mainly designed to preserve knowledge from previous tasks, without much emphasis on how to well generalize models to new tasks. In this work, we propose an information disentanglement based regularization method for continual learning on text classification. Our proposed method first disentangles text hidden spaces into representations that are generic to all tasks and representations specific to each individual task, and further regularizes these representations differently to better constrain the knowledge required to generalize. We also introduce two simple auxiliary tasks: next sentence prediction and task-id prediction, for learning better generic and specific representation spaces. Experiments conducted on large-scale benchmarks demonstrate the effectiveness of our method in continual text classification tasks with various sequences and lengths over state-of-the-art baselines.

@&#INTRODUCTION@&#

Computational systems in real world scenarios face changing environment frequently, and thus are often required to learn continually from dynamic streams of data building on what was learnt before (Biesialska et al., 2020). For example, a tweeter classifier needs to deal with trending topics which are constantly emerging. While being an intrinsic nature of human to continually acquire and transfer knowledge throughout lifespans, most machine learning models often suffer from catastrophic forgetting: when learning on new tasks, models dramatically and rapidly forget knowledge from previous tasks (McCloskey and Cohen, 1989). As a result, Continual Learning (CL) (Ring, 1998; Thrun, 1998) has received more attention recently as it can enable models to perform positive transfer (Perkins et al., 1992) as well as remember previously seen tasks.
A growing body of research has been conducted to equip neural networks with the ability of continual learning abilities (Kirkpatrick et al., 2017; Lopez-Paz and Ranzato, 2017; Aljundi et al., 2018). Existing continual learning methods on NLP tasks can be broadly categorized into two classes: purely replay based methods (de Masson d'Autume et al., 2019; Sun et al., 2019) where examples from previous tasks are stored and re-trained during the learning of the new task to retain old information, and regularization based methods (Wang et al., 2019; Han et al., 2020) where constraints are added on model parameters to prevent them from changing too much while learning new tasks. The former usually stores an extensive amount of data from old
tasks (de Masson d'Autume et al., 2019) or trains language models based on task identifiers to generate sufficient examples (Sun et al., 2019), which significantly increases memory costs and training time. While the latter utilizes previous examples efficiently via the constraints added on text hidden space or model parameters, it generally views them as equally important and regularize them to the same extent (Wang et al., 2019; Han et al., 2020), making it hard for models to differentiate informative representation that needs to be retained from ones that need a large degree of updates. However, we argue that when learning new tasks, task generic information and task specific information should be treated differently, as these generic representation might function consistently while task specific representations might need to be changed significantly.
To this end, we propose an information disentanglement based regularization method for continual learning on text classification. Specifically, we first disentangle the text hidden representation space (e.g., the output representation of BERT (Devlin et al., 2019)) into a task generic space and a task specific space using two auxiliary tasks: next sentence prediction for learning task generic information and task identifier prediction for learning task specific representations. When training on new tasks, we constrain the task generic representation to be relatively stable and representations of task specific aspects to be more flexible. To further alleviate catastrophic forgetting without much increases of memory and training time, we propose to augment our regularization-based methods by storing and replaying only a small amount of representative examples (e.g., 1% samples selected by memory selection rules like K-Means (MacQueen et al., 1967)). To sum up, our contributions are threefold:
• We propose an information disentanglement based regularization method for continual text classification, to better learn and constrain task generic and task specific knowledge.
• We augment the regularization approach with a memory selection rule that requires only a small amount of replaying examples.
• Extensive experiments conducted on five benchmark datasets demonstrate the effectiveness of our proposed methods compared to state-of-the-art baselines.

@&#METHOD@&#

In this work, we focus on continual learning for a sequence of text classification tasks fT1; :::Tng, where we learn a model f(:),  is a set of parameters shared by all tasks and each task Ti contains a different set of sentence-label training pairs, (xi 1:m; yi 1:m). After learning all tasks in the sequence, we seek to minimize the generalization
error on all tasks (Biesialska et al., 2020) :
R(f) = Xn i=1 E(xi;yi)Ti L(f(xi); yi)
We use two commonly-used techniques for this problem setting in our proposed model:
• Regularization: in order to preserve knowledge stored in the model, regularization is a constraint added to model output (Li and Hoiem, 2018), hidden space (Zenke et al., 2017) and parameters (Lopez-Paz and Ranzato, 2017; Zenke et al., 2017; Aljundi et al., 2018) to prevent them from changing too much while learning new tasks.
                           
• Replay: when learning new tasks, Experience Replay (Rebuffi et al., 2017) is commonly used to recover knowledge from previous tasks, where a memory buffer is first adopted to store seen examples from previous tasks and then the stored data is replayed with the training set for the current task. Formally, after training on task t 􀀀 1 (t  2),

jSt􀀀1j examples are randomly sampled from the training set St􀀀1 into the memory buffer M, where 0     1 is the store ratio. Data fromMis then merged with the next training set St when learning from task t.                        

@&#METHODS@&#

In continual learning, the model needs to adapt to new tasks quickly while maintaining the ability to recover information from previous tasks, hence not all information stored in the hidden representation space should be treated equally. In previous work like style transfer (John et al., 2019) and controlled text generation (Hu et al., 2017), certain information (such as content and syntax) is extracted and shared among different categories and other information (such as style and polarity) is manipulated for each specific category. Similarly, in our continual learning scenario, there is shared knowledge among different tasks as well while the model needs to learn and maintain specific knowledge for each individual task in the learning process. This key observation motivates us to propose an information-disentanglement based regularization for continual text classification to retain shared knowledge while adapting specific knowledge to streams of tasks (Section 4.1). We also incorporate a small set of representative replay samples to alleviate catastrophic forgetting (Section 4.3). Our model architecture is shown in Figure 1.
                  
4.1 Information Disentanglement (ID)
This section describes how to disentangle sentence representations into task generic space and task specific space, and how separate regularizations are imposed on them for continual text classification. Formally, for a given sentence x, we first use a multi-layer encoder B(:), e.g., BERT (Devlin et al., 2019), to get the hidden representations r which contain both task generic and task specific information. Then we introduce two disentanglement networks G(:) and S(:) to extract the generic representation g and specific representation s from r.

For new tasks, we learn the classifiers by utilizing information from both spaces, and we allow different spaces to change to different extents to best retain knowledge from previous tasks.
Task Generic Space Task generic space is the hidden space containing information generic to different tasks in a task sequence. During switching from one task to another, the generic information should roughly remain the same, e.g., syntactic knowledge should not change too much across the learning process of a sequence of tasks. To extract task generic information g from hidden representations r, we leverage the next sentence prediction task (Devlin et al., 2019) 1 to learn the generic information extractor G(:). More specifically, we insert a [SEP] token into each training example during tokenization to form a sequence pair labeled IsNext, and switch the first sequence and the second sequence to form a sentence pair labeled NotNext.
In order to distinguish IsNext pairs and NotNext pairs, extractor G(:) needs to learn the context dependencies between two segments, which is beneficial to understand every example and generic to any individual task.
Denote ~x as the NotNext example corresponding to x (IsNext), and l 2 f0; 1g as the label for next sentence prediction. We build a sentence relation predictor fnsp on the generic feature extractor G(:): Lnsp = Ex2St[M(L(fnsp(G(B(x)); 0) +L(fnsp(G(B(~x)); 1)) where L is the cross entropy loss,Mis the memory buffer and St is the t-th training set. Task Specific Space Models also need task specific information to perform well over each task.
For example, on sentiment classification words like “good” or “bad” could be very informative, but they might not generalize well for tasks like topic classification. Thus we employ a simple task-identifier prediction task on the task specific representation s, which means for any given example we want to distinguish which task this example belongs to. This simple auxiliary setup will encourage us to embed different information from different tasks. The loss for task-identifier predictor ftask is: Ltask = E(x;z)2St[ML(ftask(S(B(x)); z) where z is the corresponding task id for x. Text Classification To adapt to the t-th task, we combine the task generic representation g = G(B(x)) and task specific representation s = S(B(x)) to perform text classification, where we minimize the cross entropy loss: Lcls = E(x;y)2St[ML(fcls(g  s); y))
Here y is the corresponding class label for x, fcls(:) is the class predictor.  denotes the concatenation of the two representations.
4.2 ID Based Regularization
To further prevent severe distortion when training on new tasks, we employ regularization on both generic representations g and specific representations s. Different from previous approaches (Li and Hoiem, 2018; Wang et al., 2019) which treat all the spaces equally, we allow regularization to different extents on g and s as knowledge in different spaces should be preserved separately to encourage both more positive transfer and less forgetting. Specifically, before training all the modules on task t, we first compute the generic representations and
specific representations of all sentences x from the training set St of current task t and memory buffer Mt. Using the trained Bt􀀀1(:), Gt􀀀1(:) and St􀀀1(:) from previous task t 􀀀 1, for each example x we calculate the generic representation as Gt􀀀1(Bt􀀀1(x)), and the specific representation as St􀀀1(Bt􀀀1(x)) to hoard the knowledge from previous models. The computed generic and specific representations are saved. During the learning from training pairs from task t, we impose two  regularization losses separately: Lgr eg = Ex2St[MtkGt􀀀1(Bt􀀀1(x)) 􀀀 G(B(x))k2 Lsr eg = Ex2St[MtkSt􀀀1(Bt􀀀1(x)) 􀀀 S(B(x))k2
4.3 Memory Selection Rule
Since we only store a small number of examples as a way to balance the replay as well as the extra memory cost and training time, we need to carefully select them in order to utilize the memory bufferMefficiently. Considering that if two stored examples are very similar, then only storing one of them could possibly achieve similar results in the future. Thus, those stored examples should be as diverse and representative as possible. To this end, after training on t-th task, we employ K-means (MacQueen et al., 1967) to cluster all the examples from current training set St: For each x 2 St, we utilize its embedding B(x) as its input feature to conduct K-means. We set the numbers of clusters to  jStj and only select the example closest to each cluster’s centroid, following Wang et al. (2019); Han et al. (2020).

4.4 Overall Objective
We can write the final objective for continual learning on text classification as the following: L = Lcls + Lnsp + Ltask +gLgr eg + sLsr eg


@&#EXPERIMENT@&#


5.1 Datasets
Following MBPA++ (de Masson d'Autume et al., 2019), we use five text classification datasets (Zhang et al., 2015; Chen et al., 2020) to evaluate our methods, including AG News (news classification), Yelp (sentiment analysis), DBPedia (Wikipedia article classification), Amazon (sentiment analysis), and Yahoo! Answer (Q&A classification). A summary of the datasets is shown in Table 1. We merge the label space of Amazon and Yelp considering their domain similarity, with 33  classes in total.
5.2 Experiment Setup 
Due to the limitation of resources, for most of our experiments, we create a reduced dataset by randomly sampling 2000 training examples and 2000 validation examples per class for every task. See Table 1 for the train/test size of each dataset. We name this setting Setting (Sampled). We tune all the hyperparameters on the basis of Setting (Sampled). Beyond that, to have a comparison with previous State-of-the-art, we also conduct experiments on the same training set and test set as MbPA++ (de Masson d'Autume et al., 2019) and LAMOL (Sun et al., 2019), which contains 115,000 training examples and 7,600 test examples for each task. For every task, we randomly hold out 500 examples per class from training examples for validation purpose. We name the latter Setting (Full). During training, we evaluate our model on validation sets from all seen tasks, following Kirkpatrick et al. (2017).
Our experiments are mainly conducted on the task sequences shown in Table 2. To minimize the effect of task order and task sequence length on the results, we examine both length-3 task sequences and length-5 task sequences in various orders. The first 3 task sequences are a cyclic shift of ag yelp yahoo, which are three classification tasks in different domains (news classification, sentiment analysis, Q&A classification). The last four length- 5 task sequences follows de Masson d'Autume et al. (2019).
5.3 Baselines 
We compare our proposed model with the following baselines in our experiments:
• Finetune (Yogatama et al., 2019): finetune BERT model sequentially without the episodic memory module and any other loss.
Order Task Sequence
1 ag yelp yahoo
2 yelp yahoo ag
3 yahoo ag yelp
4 ag yelp amazon yahoo dbpedia
5 yelp yahoo amazon dbpedia ag
6 dbpedia yahoo ag amazon yelp
7 yelp ag dbpedia amazon yahoo 
Table 2: Seven random different task sequences used for experiments. The first 6 are used in Setting (Sampled). The last 4 are used in Setting (Full).
• Replay (Wang et al., 2019; de Masson d'Autume et al., 2019): Finetune model augmented with an episodic memory. Replay examples from old tasks while learning new tasks.
• Regularization: On top of Replay, with an L2 regularization term added on the hidden state of the classifier following BERT.
• MBPA++ (de Masson d'Autume et al., 2019): augment BERT model with an episodic memory module and store all seen examples. MBPA++ performs experience replay at training time, and uses K-nearest neighbors to select examples for local adaptation at test time.
• LAMOL (Sun et al., 2019): train a language model that simultaneously learns to solve the tasks and generate training samples, the latter is for generating pseudo samples used in experience replay. Here the text classification is performed in Q&A formats.
• Multi-task Learning (MTL): The model is trained on all tasks simultaneously, which can be considered as an upper-bound for continual learning methods since it has access to data from all tasks at the same time. 
5.4 Implementation Details
 We use pretrained BERT-base-uncased from HuggingFace Transformers (Wolf et al., 2020) as our base feature extractor. The task generic encoder and task specific encoder are both one linear layer followed by activation function Tanh, their output size are both 128 dimensions. The predictors built on encoders are all one linear layer followed by activation function softmax. All experiments are conducted on NVIDIA RTX 2080 Ti with 11GB memory with the batch size of 8 and the maximum sequence length of 256 (use the first 256 tokens if one’s length is beyond that). We use AdamW (Loshchilov and Hutter, 2019) as optimizer. For all modules except the task id predictor, we set the learning rate lr = 3e􀀀5; for task id predictor, we set its learning rate lrtask = 5e􀀀4. The weight decay for all parameters are 0.01. For experience replay, we set the store ratio  = 0:01, i.e. we store 1% of seen examples into the episodic memory module. Besides, we set the replay frequency  = 10, which means we do experience replay once every ten steps. For information disentanglement, we mainly tune the coefficients of the regularization loss. For batches from memory bufferM, we set g to 2.5, select best s from f1:5; 2:0; 2:5g. For batches from current training set S, we set g to 0.25, select best s from f0:15; 0:20; 0:25g.

@&#RESULT@&#

We evaluate models after training on all tasks and report their average accuracies on all test sets as our metric. Table 3 summarizes our results in Setting (Sampled). While continual finetuning suffered from severe forgetting, experience replay with 1% stored examples achieves promising results, which demonstrates the importance of experience replay for continual learning in NLP. Beyond that, simple regularization turns out to be a robust method on the basis of experience replay, which shows consistent improvements on all 6 orders. Our proposed Information Disentanglement Based Regularization (IDBR) further improves regularization consistently under all circumstances. Table 4 compares IDBR with previous SOTA: MBPA++ and LAMOL in Setting (Full). Note that although we use the same training/testing data, there is some inherent differences between our settings and previous SOTA methods. Despite the fact that MBPA++ applies local adaptation when testing, IDBR still outperforms it by an obvious margin. We achieve comparative results with LAMOL, despite that LAMOL requires task identifiers during inference which makes its prediction task easier.

6.1 Impact of the Lengths of Task Sequences
Comparing results of length-3 sequences and length-5 sequences in Table 3, we found that the gap between IDBR and multi-task learning became bigger when the length of task sequence changed from 3 to 5. To better understand how IDBR grad- ually forgot, we followed Chaudhry et al. (2018) to measure forgetting Fk after trained on task k as follows:
Fk = Ej=1:::t􀀀1fk j ; fk j = max l2f1:::k􀀀1g al;j 􀀀 ak;j where al;j is the is the model’s accuracy on task j after trained on task l. On order 4, 5 and 6, we calculate the forgetting every time after IDBR was trained on a new task and summarize them in Table 5. For continual learning, we hypothesize that the model is prone to suffer from more severe forgetting as the task sequence becomes longer. We found that although there was some big drop after training on the 3rd task, IDBR maintained stable performance as the length of task sequence increased, especially after training on 4-th and 5-th task, the forgetting increment was relatively small, which demonstrated the robustness of IDBR. 
6.2 Visualizing Disentangled Spaces
 To study whether our task generic encoder G tends to learn more generic information and task specific encoder S captures more task specific information, we used t-SNE (van der Maaten and Hinton, 2008) to visualize the two hidden spaces of IDBR, using the final model trained on order 2, and the results are shown in Figure 2, where Figure 2a visualizes task generic space and Figure 2b visualizes task specific space. We observe that compared with task specific space, generic features from different tasks were more mixed, which demonstrates that the next sentence prediction helped task generic space to be more task-agnostic than task specific space, which was induced to learn separated representations for different tasks. Considering we only employed two simple auxiliary tasks, the effect of information disentanglement was noticeable.

6.3 Ablation Studies
Effect of Disentanglement In order to demonstrate that each module of our information disentanglement helps the learning process, we performed ablation study on the two auxiliary tasks using order 5 as a case study. The results are summarized in Table 6. We found that both task-id prediction and next sentence prediction contribute to the final performance. Furthermore, the performance gain was much larger by combing these two auxiliary tasks together. Intuitively, the model needs both tasks to disentangle the representation well, since it is easy for the model to ignore one of the spaces if the constraint is not imposed appropriately. The results show that the two tasks are likely complimentary to each other in helping the model learn better disentangled representations. Impact of Regularization To study the effect of regularization on task generic hidden space g and task specific hidden space s, we performed an ablation study which only applied regularization on g or s, and compared the results with regularization on both in Table 7. We found that regularization on both spaces results in a much better performance than regularization on one of them only, which demonstrates the necessity of both regularizers. While we may expect to give more tolerance to specific space for changing, we found that no regularization on it would lead to severe forgetting of previously learnt good task specific embeddings, hence it is necessary to add a regularizer over this space as well. Beyond that, we also observed that under most circumstances, adding regularization on the task generic space g results in a more significant gain than adding regularization on the task specific space s, consistent with our intuition that task generic space changes less across tasks and thus preserving it better helps more in alleviating catastrophic forgetting. Impact of K-Means To demonstrate our hypothesis that when the memory budget is limited, selecting the most representative subset of examples is vital to the success of continual learning, we performed an ablation study on order 1,2,3 using IDBR with and without K-Means. The result is shown in Table 8. From the table, we found that using K-Means helps boost the overall performance. Specifically, the improvement brought by K-Means was larger on those challenging orders, i.e. orders on which IDBR had worse performance. This is because for these challenging orders, the forgetting is more severe and the model needs more examples from previous tasks to help it retain previous knowledge. Thus with the same memory budget constraint, diversity across saved examples will help the model better recover knowledge learned from previous tasks.

@&#CONCLUSION@&#

In this work, we introduce an information disentanglement based regularization (IDBR) method for continual text classification, where we disentangle the hidden space into task generic space and task specific space and further regularize them differently. We also leverage K-Means as the memory selection rule to help the model benefit from the augmented episodic memory module. Experiments conducted on five benchmark datasets demonstrate that IDBR achieves better performances compared to previous state-of-the-art baselines on sequences of text classification tasks with various orders and lengths. We believe the proposed approach can be extended to continual learning for other NLP tasks such as sequence generation and sequence labeling as well, and plan to explore them in the future.

@&#ACKNOWLEDGEMENTS@&#



@&#REFERENCES@&#

