Once the metric is established, a classification can be performed either based on hard-decision (words below a certain threshold are considered cognates) or by learning a classifier over the distance measures and other features (Kondrak, 2001; Mann and Yarowsky, 2001; Inkpen et al., 2005; Ciobanu and Dinu, 2014a; List et al., 2016); Mulloni and Pekar (2006) have evaluated an alternative approach, in which explicit rules of transformation are derived based on edit operations.
(2020) employ a mixture of-experts approach for lexical translation induction, combining neural and probabilistic methods, and Nishimura et al.
The researchers also applied RNNs on the same dataset, but reported negative results.
5.1 NMT-based Neural Model Our proto-word reconstruction setup follows an encoder-decoder with attention architecture, similar to contemporary neural machine translation (NMT) systems (Bahdanau et al., 2015; Cho et al., 2014).
We use a standard character-based encoderdecoder architecture with attention (Bahdanauet al., 2015).
Both encoder and decoder are GRU networks with 150 cells.
The encoder reads the forms of the words in the daughter languages, and output a contextualized representation of each character.
At each decoding step, the decoder attends to the encoder’s representations via a dotproduct attention.
Input representation Each character (a letter in the orthographic case, and a phoneme in the phonetic case) is represented by an embedding vector of size 100.
A possible approach would encode each language’s characters using a unique embedding table.
We instead share the character embedding table across all languages (including Latin), but concatenate to each character vector also a language embedding vector.
The final representation of a character c in language ` is then W E[c] + UE[`] where E is a shared embedding matrix, c is a character id, ` is a language id, and W and U are a linear projection layers.

 3.1 MODEL ARCHITECTURE CHOICES The backbone of the ALBERT architecture is similar to BERT in that it uses a transformer encoder (Vaswani et al., 2017) with GELU nonlinearities (Hendrycks & Gimpel, 2016).
We follow the BERT notation conventions and denote the vocabulary embedding size as E, the number of encoder layers as L, and the hidden size as H. Following Devlin et al.
There are three main contributions that ALBERT makes over the design choices of BERT.
Factorized embedding parameterization.
In BERT, as well as subsequent modeling improvements such as XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019), theWordPiece embedding size E is tied with the hidden layer size H, i.e., E  H. This decision appears suboptimal for both modeling and practical reasons, as follows.
From a modeling perspective,WordPiece embeddings are meant to learn context-independent representations, whereas hidden-layer embeddings are meant to learn context-dependent representations.
As experiments with context length indicate (Liu et al., 2019), the power of BERT-like representations comes from the use of context to provide the signal for learning such context-dependent representations.
As such, untying the WordPiece embedding size E from the hidden layer size H allows us to make a more efficient usage of the total model parameters as informed by modeling needs, which dictate that H  E. From a practical perspective, natural language processing usually require the vocabulary size V to be large.1 If E  H, then increasing H increases the size of the embedding matrix, which has sizeV E.
Therefore, for ALBERT we use a factorization of the embedding parameters, decomposing them into two smaller matrices.
Instead of projecting the one-hot vectors directly into the hidden space of size H, we first project them into a lower dimensional embedding space of size E, and then project it to the hidden space.
By using this decomposition, we reduce the embedding parameters from O(V  H) to O(V  E + E  H).
This parameter reduction is significant when H  E. We choose to use the same E for all word pieces because they are much more evenly distributed across documents compared to whole-word embedding, where having different embedding size (Grave et al.
For ALBERT, we propose cross-layer parameter sharing as another way to improve parameter efficiency.
The default decision for ALBERT is to share all parameters across layers.
(2018) (Universal Transformer, UT) and Bai et al.
(2019) (Deep Equilibrium Models, DQE) for Transformer networks.
(2018) show that UT outperforms a vanilla Transformer.
(2019) show that their DQEs reach an equilibrium point for which the input and output embedding of a certain layer stay the same.
Our measurement on the L2 distances and cosine similarity show that our embeddings are oscillating rather than converging.
In addition to the masked language modeling (MLM) loss (Devlin et al., 2019), BERT uses an additional loss called next-sentence prediction (NSP).
That is, for ALBERT, we use a sentence-order prediction (SOP) loss, which avoids topic prediction and instead focuses on modeling inter-sentence coherence.
The SOP loss uses as positive examples the same technique as BERT (two consecutive segments from the same document), and as negative examples the same two consecutive segments but with their order swapped.
As a result, ALBERT models consistently improve downstream task performance for multi-sentence encoding tasks.
We present the differences between BERT and ALBERT models with comparable hyperparameter settings in Table 1.
Due to the design choices discussed above, ALBERT models have much smaller parameter size compared to corresponding BERT models.
For example, ALBERT-large has about 18x fewer parameters compared to BERT-large, 18M versus 334M.
An ALBERT-xlarge configuration with H = 2048 has only 60M parameters and an ALBERT-xxlarge configuration with H = 4096 has 233M parameters, i.e., around 70% of BERTlarge’s parameters.
Note that for ALBERT-xxlarge, we mainly report results on a 12-layer network because a 24-layer network (with the same configuration) obtains similar results but is computationally more expensive.
This improvement in parameter efficiency is the most important advantage of ALBERT’s design choices.
4.1 EXPERIMENTAL SETUP To keep the comparison as meaningful as possible, we follow the BERT (Devlin et al., 2019) setup in using the BOOKCORPUS (Zhu et al., 2015) and EnglishWikipedia (Devlin et al., 2019) for pretraining baseline models.
Like BERT, we use a vocabulary size of 30,000, tokenized using SentencePiece (Kudo & Richardson, 2018) as in XLNet (Yang et al., 2019).
The experimental setup described in this section is used for all of our own versions of BERT as well as ALBERT models, unless otherwise specified.
4.3 OVERALL COMPARISON BETWEEN BERT AND ALBERT We are now ready to quantify the impact of the design choices described in Sec.
The improvement in parameter efficiency showcases the most important advantage of ALBERT’s design choices, as shown in Table 2: with only around 70% of BERT-large’s parameters, ALBERT-xxlarge achieves significant improvements over BERT-large, as measured by the difference on development set scores for several representative downstream tasks: SQuAD v1.1 (+1.9%), SQuAD v2.0 (+3.1%), MNLI (+1.4%), SST-2 (+2.2%), and RACE (+8.4%).
Because of less communication and fewer computations, ALBERT models have higher data throughput compared to their corresponding BERT models.
If we use BERT-large as the baseline, we observe that ALBERT-large is about 1.7 times faster in iterating through the data while ALBERT-xxlarge is about 3 times slower because of the larger structure.
Next, we perform ablation experiments that quantify the individual contribution of each of the design choices for ALBERT.
4.4 FACTORIZED EMBEDDING PARAMETERIZATION Table 3 shows the effect of changing the vocabulary embedding size E using an ALBERT-base configuration setting (see Table 1), using the same set of representative downstream tasks.
Under the non-shared condition (BERT-style), larger embedding sizes give better performance, but not by much.
Under the all-shared condition (ALBERT-style), an embedding of size 128 appears to be the best.
Based on these results, we use an embedding size E = 128 in all future settings, as a necessary step to do further scaling.
4.5 CROSS-LAYER PARAMETER SHARING Table 4 presents experiments for various cross-layer parameter-sharing strategies, using an ALBERT-base configuration (Table 1) with two embedding sizes (E = 768 and E = 128).
We compare the all-shared strategy (ALBERT-style), the not-shared strategy (BERT-style), and intermediate strategies in which only the attention parameters are shared (but not the FNN ones) or only the FFN parameters are shared (but not the attention ones).
4.6 SENTENCE ORDER PREDICTION (SOP) We compare head-to-head three experimental conditions for the additional inter-sentence loss: none (XLNet- and RoBERTa-style), NSP (BERT-style), and SOP (ALBERT-style), using an ALBERTbase configuration.
Even more importantly, the SOP loss appears to consistently improve downstream task performance for multi-sentence encoding tasks (around +1% for SQuAD1.1, +2% for SQuAD2.0, +1.7% for RACE), for an Avg score improvement of around +1%.
The authors would like to thank Beer Changpinyo, Nan Ding, Noam Shazeer, and Tomer Levinboim for discussion and providing useful feedback on the project; Omer Levy and Naman Goyal for clarifying experimental setup for RoBERTa; Zihang Dai for clarifying XLNet; Brandon Norick, Emma Strubell, Shaojie Bai, Chas Leichner, and Sachin Mehta for providing useful feedback on the paper; Jacob Devlin for providing the English and multilingual version of training data; Liang Xu, Chenjie Cao and the CLUE community for providing the training data and evaluation benechmark of the Chinese version of ALBERT models.

 Models that compute word embeddings are widespread [Mikolov et al., 2013; Devlin et al., 2018; Peters et al., 2018; Radford et al., 2018].
[2017] introduce the Word Embedding Assocation Test, WEAT, based on the Implicit Association Test, IAT, to measure biases in word embeddings.
[2019] generalize WEAT to biases in sentence embeddings, introducing the Sentence Encoder Association Test (SEAT).
Tan and Celis [2019] generalize SEAT to contextualized word representations, e.g., the encoding of a word in context in the sentence.
These advances are incorporated into the grounded metrics developed here, by measuring thebias of word embeddings, sentence embeddings, as well as contextualized word embeddings.
Second, since COCO and Conceptual Captions form part of the training sets for VisualBERT and ViLBERT respectively, this ensures that biases are not a property of poor out-ofdomain generalization.
[2017] base the Word Embedding Assocation Test (WEAT) on an IAT test administered to humans.
The average cosine similarity between pairs of word embeddings is used as the basis of an indicator of bias, as in: s(w, A, B) = meana∈Acos(w, a) − meanb∈Bcos(w, b) (1) where s measures how close on average the embedding for word w is compared to the words in attribute A and attributeB.
The space of embeddings has structure that may make all targets, e.g., both men’s names and women’s names, closer to one profession than the other.
The difference between the target distances reveals which target sets are more associated with which attribute sets: s(X, Y, A, B) = X x∈X s(x, A, B) − X y∈Y s(y, A, B) (2) The effect size, i.e., the number of standard deviations in which the peaks of the distributions of embedding distances differ, of this metric is computed as d = meanx∈Xs(x, A, B) − meany∈Y s(y, A, B) std devw∈X∪Y s(w, A, B) (3) May et al.
[2019] extend this test to measure sentence embeddings, by using sentences in the target and attribute sets.
Tan and Celis [2019] extend the test to measure contextual effects, by extracting the embedding of single target and attribute tokens in the context of a sentence rather than the encoding of the entire sentence.
In the ungrounded setting, only 4 embeddings can be computed because the attributes are independent of the target category.
This leads to 12 possible grounded embeddings1 ; see table 2.
We find that three such tests, described next, have intuitive explanations and measure different but complementary aspects of bias in word embeddings.
These questions are relevant to both bias and to the quality of word embeddings.
For example, attempting to measure the impact of vision separately from language on joint embeddings can indicate if there is an over-reliance on one modality over another.
4.1 Experiment 1: Do joint embeddings contain biases?
An advantage of grounded over ungrounded embeddings is that we can show scenarios that clearly counter social stereotypes.
For example, the model may think that men are more likely to have some professions, but are the embeddings different when visual input to the contrary is provided?
In practice, systems built on top of grounded embeddings will not be used with balanced images, and so while in a sense more elegant, this construction may completely misstate the biases one would see in the real world.
4.3 Experiment 3: To what degree do biases come from language vs. vision in joint embeddings?
In other words, if the model does not change its biases regardless of the images being shown, then vision does not play a role in encoding biases.
Note that we are not saying that the embeddings do not consider vision, but merely that it may or may not have an effect on biases specifically.
This provides a finer-grained metric for the relevance of vision to embeddings.
We evaluate VisualBERT [Li et al., 2019] on images from COCO, ViLBERT [Lu et al., 2019] on images Conceptual Captions, and both models on images we collected from Google Images.
Images features are computed in the same manner as in the original publications for both VisualBERT and ViLBERT.
Overall, the results are consistent with prior work on biases in both humans and models such as BERT.
The experiments were run on VisualBERT COCO, VisualBERT Google Images, ViLBERT Conceptual Captions and ViLBERT Google Images.
Following Tan and Celis [2019], each experiment examines the bias in three types of embeddings: word embeddings, sentence embeddings, and contextualized word embeddings.
While there is broad agreement between these different ways of using embeddings, they are not identical in terms of which biases are discovered.
Methods to mitigate biases will hopefully address all three embedding types and all of the three questions we restate below.
Do joint embeddings contain biases?
It appears as if more biases exist in the grounded embeddings compared to the ungrounded ones.
To what degree do biases come from language vs. vision in joint embeddings?
It could be that joint embeddings largely ignore vision, or that the biases in language are so powerful that vision does not contribute to them given that on any one example it appears unable to override the existing biases (experiment 2).

2.1.2 Multi-Task Learning (MTL) All tasks are trained simultaneously with the aim of learning a shared representation that will enable the model to generalize better on each task (Caruana, 1997; Collobert and Weston, 2008; Nam et al., 2014; Liu et al., 2016, 2017; Zhang and Yang, 2017; Ruder, 2017; Ruder et al., 2019; Sanh et al., 2019; Pfeiffer et al., 2020b, inter alia).
While  represents the weights of a pretrained model (e.g., a transformer), the parameters n, where n 2 f1; : : : ;Ng, are used to encode task-specific representations in intermediate layers of the shared model.
For NLP tasks, adapters have been introduced for the transformer architecture (Vaswani et al., 2017).
At each transformer layer l, a set of adapter parameters l is introduced.
(2019) experiment with different architectures, finding that a twolayer feed-foward neural network with a bottleneck works well.
They place two of these components within one layer, one after the multi-head attention (further referred to as bottom) and one after the feed-forward layers of the transformer (further referred to as top).1 Bapna and Firat (2019) and Stickland and Murray (2019) only introduce one of these components at the top position, however, Bapna and Firat (2019) include an additional layer norm (Ba et al., 2016).
In all experiments, we use BERT-base-uncased (Devlin et al., 2019) as the pretrained language model.
We train for a maximum of 30 epochs with early stopping.
For AdapterFusion, we empirically find that a learning rate of 5e 􀀀 5 works well, and use this in all experiments.4 We train for a maximum of 10 epochs with early stopping.
To provide a fair comparison to Stickland and Murray (2019) we primarily experiment with BERT-baseuncased.
We additionally validate our best model configurations — ST-A and Fusion with ST-A — with RoBERTa-base, for which we present our results in Appendix Table 4.

We evaluate eight feature selection methods, including five filter methods (F filter, LR filter, KL filter, Chi filter, and ED filter), two embedded methods (Two Model embedded and KL embedded), and one standard embedded method for classification as a benchmark (feature importance based on random forest classifier denoted as “outcome embedded”).
For all the meta-learners, we use a random forest classifier as the base learner.
In the simulation, all the random forest classifiers in the meta-learners and uplift random forest share the same hyper-parameter values: the number of trees is 10, the maximum tree depth is 10, the minimum sample size in leaf to perform split is 100, and the maximum number of features for split is 3.
The uplift model variants considered are: (1) TwoModel-LR, XLearner-LR, RLearner-LR using f Logistic Regression Classifier & Linear Regression Regressor g as base learners; (2) TwoModel-LGBM, XLearner-LGBM, RLearner-LGBM using f Gradient Boosting Classifier & Gradient Boosting Regressor g from Light- GBM implementation [26] as base learners, with hyperparameter values (n estimators = 100; max depth = 8; min child samples = 100); (3) TwoModel-RF, XLearner- RF, RLearner-RF using f Random Forest Classifier & Random Forest Regressor g as base learners, with hyperparameter values (n estimators = 100; max depth = 8; min samples leaf = 100); (4) KL-RF as the uplift random forest using KL divergence criterion with hyperparameter values (n estimators = 10; max depth = 8; min samples leaf = 100).

 2.1 Neural Abstractive Summarization Neural approaches to abstractive summarization conceptualize the task as a sequence-to-sequence problem, where the encoder maps the sequence of tokens in the source document x = [x1, ..., xn]to a sequence of continuous representations  z = [z1, ..., zn], and the decoder autoregressively generates the target summary y = (y1, ..., ym) token-by-token, hence modeling the conditional probability p(y1, ..., ym|x1, ..., xn).
(2016) were among the first to apply the neural encoderdecoder architecture to text summarization.
Other work develops abstractive models trained end-to-end with reinforcement learning based on multiple encoders and hierarchical attention (Celikyilmaz et al., 2018) or a coverage mechanism where the decoder attends over previously generated words (Paulus et al., 2018).
Although the majority of summarization systems are composed of LSTM units, Narayan et al.
(2018) and (PerezBeltrachini et al., 2019) propose abstractive models based on convolutional neural networks.
Liu and Lapata (2019) combine a pretrained encoder based on BERT (Devlin et al., 2019) with a randomly initialized decoder, demonstrating substantial gains on summarization performance.
(2019) pretrain an encoder-decoder framework to reconstruct (masked) fragments within a sentence and then fine-tune it on summarization datasets.
(2020) present BART, an encoder-decoder Transformer (Vaswani et al., 2017), pretrained by reconstructing a text corrupted with several arbitrary noising functions.
(2020) design UNILMv2, a Transformer-based neural network pretrained as a pseudo-masked language model.
The models are typically neural networks and function f can be in principle defined using the output of any network layer (e.g., a hidden or softmax layer).
Specific instantiations of this general framework include minimizing the teacher/student difference based on output logits, intermediate hidden representations, attention maps, and derivatives of the loss to the input (Ba and Caruana, 2014; Romero et al., 2014; Zagoruyko and Komodakis, 2017; Czarnecki et al., 2017).
(2019) use it to transfer knowledge from BERT to smaller models, helping them approach or exceed the quality of much larger pretrained neural networks.
4.1 Summarization Datasets We evaluated our model on two singledocument summarization datasets, namely the CNN/DailyMail news highlights (Hermann et al., 2015) and XSum (Narayan et al., 2018), and one multi-document summarization dataset, i.e., WikiCatSum (Perez-Beltrachini et al., 2019).
The summaries also vary with respect to the type of rewriting operations they exemplify (e.g., CNN/DailyMail showcases more cut and paste operations while XSum is genuinely abstractive).
CNN/DailyMail contains news articles and associated highlights, i.e., a few bullet points written by journalists which give a brief overview of the article.
(2015) for training, validation, and testing (90,266/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents).
Input documents were truncated to 512 tokens.
Input documents were also truncated to 512 tokens.
Aside from the summaries, the dataset contains the input webpages whose length is truncated to the first 800 tokens.
4.2 Implementation Details For all datasets, we evaluated our self-knowledge distillation framework in two settings.
UNILMv2 is a Transformer-based neural network (Vaswani et al., 2017) with 12 Transformer layers and 12 attention heads.
In the non pretrained setting, we adopt a Transformer encoder-decoder model with 6 layers, 768 hidden size and 2,048 feed-forward filter size.
In all knowledge distillation experiments, student models have the same neural network architecture with their teachers and are trained with the same hyperparameters as the teacher models.
During decoding we used beam search (size 5), and tuned α for the length penalty (Wu et al., 2016) between 0.6 and 1 on the validation set; we decode until an end-of-sequence token is emitted.
5.1 Automatic Evaluation We evaluated summarization quality automatically using ROUGE (Lin, 2004).
We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a means of assessing informativeness and the longest  common subsequence (ROUGE-L) as a means ofassessing fluency.
Table 1 summarizes our results on the CNN/DailyMail and XSum (single document) datasets.
We present the LEAD baseline (which simply selects the first three sentences in a document for CNN/DailyMail and the first sentence for XSum).
We also report the results of See et al.’s (2017) pointer generator network (PTRNET), and an abstractive system from Liu and Lapata (2019) based on Transformers (TransformerAbs; see Section 4.2 for details).
With regard to LARGE-size models, we report the results of three very strong summarization systems finetuned with UNILMLARGE (Bao et al., 2020), BARTLARGE (Lewis et al., 2020), and T511B (Raffel et al., 2019).
Our BASE-size models include BERTSUMBASE (Liu and Lapata, 2019), a summarizer based on a BASE-size BERT encoder and a randomly initialized decoder, MASSBASE (Song et al., 2019) and UNILMBASE which are both finetuned with BASE-size pretrained models.
Overall, we obtain competitive results with SKD and BASE-size pretrained models and even manage to outperform UNILMLARGE and T511B on the CNN/DailyMail dataset.
CV-S2S and CV-S2D (Perez-Beltrachini et al., 2019) are convolutional encoder-decoder models.
The former is a standard convolutional decoder, while the latter adopts a hierarchical convolutional decoder which first generates target sentence vectors, and then generates target words based on sentence vectors.
TF-S2S is a standard Transformer encoder-decoder model trained on WikiCatSum (Perez-Beltrachini et al., 2019).
Column All in Table 2 shows average ROUGE across domains.
5.2 Factual Consistency Evaluation Besides ROUGE, we also use FactCC (Krysci ´ nski ´ et al., 2019) to evaluate the factual correctness of the generated summaries.
FactCC is a BERT-based classifier trained to identify conflicts between a source document and a generated summary.
We performed experiments with the publicly released version of FactCC.2 Our results on the CNN/DailyMail and XSum datasets are presented in Table 3.
All +Noisy SKD students are significantly (p < 0.05) more factually correct compared to their teachers (TransformerAbs and UNILMv2BASE), using a paired student t-test.
For CNN/DailyMail and XSum, human participants were presented with the output of two systems (and the original document) and asked to decide which one was better according to the following criteria: Succinctness (Does the summary avoid repetition?
We used the same test documents (20 in total) from Liu and Lapata (2019) for both CNN/DailyMail and XSum.
On both CNN/DailyMail and XSum datasets participants perceive the student (+Noisy SKD) as significantly (p < 0.05) more succinct and informative compared to the teacher (UNILMv2BASE).

Figure 1: Detailed architecture for a Hierarchical Transformer Encoder or HT-Encoder: The main inductive bias incorporated in this model is to encode the full dialog context hierarchically in two stages.
This is done by the two encoders, 1) Shared Utterance Encoder (M layers) and 2) Context Encoder (N layers), as shown in the figure.
Shared encoder first encodes each utterance (u1, u2, .
The same parameterized Shared Encoder is used for encoding all utterances in the context.
In the second Context Encoder the full context is encoded using a single transformer encoder for extracting dialog level features.
The attention mask in context encoder decides how the context encoding is done and is a choice of the user.
Only the final utterance in the Context Encoder gets to attend over all the previous utterances as shown.
This allows the model to have access to both utterance level features and dialog level features till the last layer of the encoding process.
, wi|ui| ], wij is the word embedding for j th word in i th utterance.
2.1 Hierarchical Transformer Encoders (HT-Encoder) Like the original HRED architecture, HT-Encoder also has two basic components, a shared utterance encoder and the context encoder.
Shared utterance encoder, or the Shared Encoder in short, is the first phase of the encoding process where each utterance is processed independently to obtain utterance level representations.
In the second phase, the Context Encoder is used to process the full context together.
We propose two different types of Hierarchical Encoding schemes for the transformer model.
(2016) employed a hierarchical encoder for dialog contexts, they obtained a single representative embedding, usually the final hidden state of an RNN, for each utterance.
Similarly, in HIER-CLS, the context encoder utilizes only a single utterance embedding for each utterance.
We do this by taking the contextual embedding of the first token (often termed as the “CLS” token in transformer based models) of each utterance.
HIER: Recent works have shown the importance of contextual word embeddings.
In HIER, we consider contextual embedding of all utterance tokens as input to the context encoder.
We simply concatenate the whole sequence of contextual embeddings and forward it to the context encoder.
2.2 Conversion Algorithm: Standard Encoder to HT-Encoder In this section, we show how the two-step process of hierarchical encoding can be achieved using a single standard transformer encoder.
If we want to have an M layer utterance encoder followed by an N layer context encoder, we start with an (M +N) layer standard encoder.
Then by applying two separate masks as designed below, we convert the standard encoder into an HT-encoder.
Within the self-attention mechanism of a transformer encoder, which token gets to attend to which other tokens is controlled by the attention mask.
If we apply a block-diagonal mask, each block of size same as the length of utterances (as shown in Figure 2 bottom-left), to the concatenated sequence of tokenized utterances, we effectively achieve the same process of utterance encoding.
We call this block-diagonal mask for utterance encoding the UT-mask.
Similarly, another attention mask (CT-Mask) can explain the context encoding phase that allows tokens to attend beyond the respective utterance boundaries.
From here, it can be quickly concluded that if we apply the UT-Mask for the first few layers of the encoder and the CT-Mask in the remaining few layers, we effectively have a hierarchical encoder.
The CT-Mask also gives us more freedom on what kind of global attention we want to allow during context encoding.
Positional encoding is applied once before utterance encoder (local PE) and once more before context encoder (global PE).
UT-Mask and Local Positional Encoding The steps for obtaining the UT-Mask and positional encoding for the utterance encoder are given below and is accompanied by Figure 2.
wij is the jth token of ith utterance.
PI has the same dimensions as CI , and it stores the position of each token wij in context C, relative to utterance ui .
P : I 7→ Rd is the positional encoding function that takes an index (or indices) and returns their d dim positional embedding.
, lT − 1] CIR = repeat(CI , len(CI ), 0) A = 12CIR == (CTIR + CIR)) Pc = P[PI , :] CT-Masks for Models The attention masks for context encoding depends on the choice for model architecture.
2.3 Model Architectures We propose several model architectures to test the effectiveness of the proposed HIER-Encoder in various experimental settings.
Using the HIER encoding scheme described in Section 2.1, we test two model architectures for response generation, namely HIER and HIER++.
HIER: HIER is the most straightforward model architecture with an HT-Encoder replacing the encoder in a Transformer Seq2Seq.
First, in the utterance encoding phase, each utterance is encoded independently with the help of the UT-Mask.
In the second half of the encoder, we apply a CT-Mask as depicted by the figure’s block attention matrix.
Block Bij is a matrix which, if all ones, means that utterance i can attend to utterance j’s contextual token embeddings.
The local and global positional encodings are applied, as explained in Section 2.2.
A standard transformer decoder follows the HTEncoder for generating the response.
A linear feedforward layer (FFN) acts as the embedding layer for converting their 44-dimension multi-hot dialog act representation.
The output embedding is added to the input token embeddings of the decoder in HIER++ model.
Similar to HDSA, we also use ground truth dialog acts during training, and predictions from a fine-tuned BERT model during validation and testing.
HIER-CLS: As described in Section 2.1, the encoding scheme of HIER-CLS is more akin to the HRED (Chen et al., 2019) and HIBERT (Zhang et al., 2019) models.
SET: HIER without the context encoder.
It shows the importance of context encoding.
Effectively, this model is only the shared utterance encoder (SET) applied to each utterance independently.
MAT: HIER without the utterance encoder.
This model only uses the context encoder as per the context attention mask of Figure 3a.
As this is equivalent to a simple transformer encoder with a special attention mask, we call it the Masked Attention Transformer or MAT.
SET++: An alternative version of SET with dialog-act input to the decoder similar to HIER++.
The HIER-Joint model comprises an HT-Encoder and three transformer decoders for decoding belief state sequence, dialog act sequence, and response.
As belief state labels can help dialog-act generation, and similarly, both belief and act labels can assist response generation, we pass the token embedding from the belief decoder and act decoder to the response decoder.
Act decoder receives mean token embedding from the belief decoder too.
Token-MoE (Pei et al., 2019) is a token level mixture-of-experts (MoE) model.
It builds upon the base architecture of LSTM-Seq2Seq with soft attention.
In the decoding phase, they employ k expert decoders and a chair decoder network which combines the outputs from the experts.
AttnLSTM (Budzianowski et al., 2018) uses an LSTM Seq2Seq model with attention on encoded context utterance, oracle belief state and DB search results.
HRED (Serban et al., 2017) model is based on the same idea of hierarchical encoding in RNN Seq2Seq networks (results source: Peng et al.,2019, 2020b).
The transformer based baseline (Vaswani et al., 2017) concatenates the utterances in dialog context to obtain a single source sequence and treats the task as a sequence transduction problem.
HDSA (Chen et al., 2019) uses a dialog act graph to control the state of the attention heads of a Seq2Seq transformer model.
Using this method they train a domain-aware multi-decoder (DAMD) network for predicting belief, action and response, jointly.
SimpleTOD (HosseiniAsl et al., 2020) and SOLOIST (Peng et al., 2020a) are both based on the GPT-2 (Radford et al., 2019) architecture.
The main difference between these two architectures is that SOLOIST further pretrains the GPT-2 model on two more dialog corpus before fine-tuning on MultiWOZ dataset.
3.1 Task Settings: Following the literature (Zhang et al., 2020a; Peng et al., 2020a), we now consider four different settings for evaluating the strength of hierarchical encoding.
No Annotations First, to simply gauge the benefit of using a Hierarchical encoder in a Transformer Seq2Seq model, we compare the performance of HIER to other baselines including HRED and vanilla Transformer without any belief states and dialog act annotations.
Some of the baselines generate dialog act as an intermediate step in their architecture whereas others use a fine-tuned BERT model.
We did hyperparameter search using the Optuna library (Akiba et al., 2019) by training the model upto 5 epochs.
Final models were trained 8 upto 30 epochs with early stopping.
By comparing the performance of Transformer, SET and MAT baselines against that of HIER we can see that in each case HIER is able to improve in terms of BLEU, Success and overall Score.
HIER being better than SET and MAT implies that only the UT-Mask or the CT-Mask is not sufficient, the full scheme of HT-Encoder is necessary for the improvement.
This is reflected upto some extent in Entity-F1 score (H-Mean of entity recall and precision), but it too ignores tokens other than task related entities.
In the Context-to-Response generation task with oracle policy (Table 3), our HIER++ and HIERCLS models show very strong performance and beat the HDSA model (in terms of Inform and Success rates) and even the GPT-2 based baseline SimpleTOD (in terms of BLEU and Success rate).
This shows that without the intricacies of the baselines, just by applying a hierarchical encoder based model we are able to perform almost at the level of the state-of-the-art model.
Compared to HIER, SimpleTOD utilizes GPT-2’s pretraining, and DAMD uses attention over previous belief states and action sequences.
Whereas, HIER’s access to oracle policy is only through the average embedding of its tokens.
While the above experiments focus on proving the base performance of the proposed response generation models (HIER, HIER++, HIER-CLS, and ablations), HT-Encoder can be applied to any model that uses a standard transformer encoder.
Hence, in a final experiment (Table 6), we integrate HT-Encoder with an existing state-of-the-art model Marco.
We replace the standard transformer in Marco with an HT-Encoder and rerun the contextto-response generation experiment.
Introducing HT-Encoder into Marco helps improve in terms of inform (minor), success and the combined score metric.
The results of this experiment show that HT-Encoder is suitable for any model architecture.
Overall, our experiments show how useful the proposed HT-Encoder module can be for dialog systems built upon transformer encoder decoder architecture.
We believe that our proposed approach for hierarchical encoding in transformers and the algorithm for converting the standard transformer encoder makes it an invaluable but accessible resource for future researchers working on dialog systems or similar problem statements with transformer-based architectures.
(2017) proposed using neural networks for extracting features like intent, belief states, etc.
Marco (Wang et al., 2020) and HDSA (Chen et al., 2019) used a finetuned BERT model as their act predictor as it often triumphs other ways to train the dialog policy network (even joint learning).
HDSA is a transformer Seq2Seq model with act-controllable self-attention heads (in the decoder) to disentangle the individual tasks and domains within the network.
Hierarchical Encoders The concept of Hierarchical Encoders have been used in many different context in the past.
Many open domain dialog systems have used the hierarchical recurrent encoding scheme of HRED for various tasks and architectures.
Hierarchical Encoder was first proposed by (Sordoni et al., 2015a) for using in a query suggestion system.
They used it encode the user history comprising multiple queries using an Hierarchical LSTM network.
HRED captures the high level features of the conversation in a context RNN.
Another area in which researchers have proposed the use of hierarchical encoder is for processing of paragraph or long documents.
(2015) used a hierarchical LSTM network for training an autoencoder that can encode and decode long paragraphs and documents.
(2019) proposed HIBERT where they introduced hierarchy into the BERT architecture to remove the limitation on length of input sequence.
HIBERT samples a single vector for each sentence or document segment (usually contextual embedding of CLS or EOS token) from the sentence encoder to be passed onto the higher level transformer encoder.
Liu and Lapata (2019) applies a similar approach for encoding documents in a multi document summarization task.

Both are English–German (EN–DE) APE corpora; they are further categorized according to their subtask depending on whether the target MT system is a phrase-based statistical MT (PBSMT) system or a neural MT (NMT) system.
We tokenized all words in our datasets into sub-word units by using SentencePiece (Kudo and Richardson, 2018).
We implemented a Transformer-based APE model, the “sequential” model proposed by Lee et al.
(2019), which again follows almost the same setting of the “base” Transformer described in the original paper Vaswani et al.
Moreover, the results also surpass current state-of-the-art (except the ensemble models) APE models (Correia and Martins, 2019; Lopes et al., 2019), which are built on top of BERT (Devlin et al., 2019), thus contain more model parameters, and exploit a huge amount of monolingual data.

 Overall Architecture Fig 1(a) shows the overall architecture of our proposed two-stage framework.
We first train an in-domain intent classifier to extract intent representations using two objectives then use the detection algorithms MSP (Hendrycks and Gimpel, 2017), LOF (Lin and Xu, 2019) or GDA (Xu et al., 2020) to detect OOD.
In the training stage, we first train a BiLSTM in domain intent classifier similar to Lin and Xu (2019) using labeled in-domain data.
Self Supervised Contrastive Learning To simultaneously model semantic features of both in-domain and OOD data, we propose a selfsupervised contrastive learning framework to utilize unlabeled data.
Following (Chen et al., 2020a; He et al., 2020a; Chen et al., 2020b; Winkens et al., 2020; Jiang et al., 2020), we formulate the contrastive loss for a positive pair of examples (i,j) as:`i,j  lij = − log exp (sim (zi, zj ) /τ )P2Nk=1 1[k6=i] exp (sim (zi, zk) /τ )(1) where zi represents the feature vector of i-th sentence sample extracted by concatenating the first and final hidden states of BiLSTM, and 1[k6=i] ∈ {0, 1} is an indicator function evaluating to 1 if k 6= i. τ denotes a temperature parameter.
Previous work (Chen et al., 2020a) has shown the necessity of more data augmentations, thus we propose an adversarial neural augmentation as follows.
Adversarial Neural Augmentation To improve the diversity of data augmentation and avoid handcrafted engineering, we apply adversarial attack (Goodfellow et al., 2015; Kurakin et al., 2016; Miyato et al., 2016; Jia and Liang, 2017; Zhang et al., 2019; Ren et al., 2019b) to generate pseudo positive samples.
It should be noted that samples obtained by adversarial attack is in the form of embedding to ensure end-to-end training.
GDA (Gaussian Discriminant Analysis)(Xu et al., 2020) is a generative distance-based classifier for out-ofdomain detection with Euclidean and Mahalanobis distances.

We compare TSRT5 with existing methods using the accuracy metric (accuracy and F-score are equivalent in this balanced corpus).
BERT and TSRT have the same optimized hyperparameters6 .
The round-trip translation (Nguyen-Son et al., 2019a) based on BLEU and BERT (Devlin et al., 2019) improves by approximately 10%.
BERT surpasses round trips in only short length ranges, while TSRT outperforms the others in all ranges.
In the top three detectors, while BERT and round-trip translation yield unstable results, TSRT remains consistent.
We used BERT as the identification baseline.

We assume a black-box setting where the attacker can only query the classifier for output label probabilities for the given input.
For evaluating the effectiveness of our proposed approach, we experiment with SOTA text classifiers i.e.
transformer based models like BERT (Devlin et al., 2018), ALBERT (Lan et al., 2019), RoBERTa (Liu et al., 2019) and DistilBERT (Sanh et al., 2019).
We use the default language model (BERT) employed in the OLM and OLM-S, and kept the number of samples generated by the OLM language model as 30 in all the experiments.
Number of queries in Adv-OLM: From equations 2 and 3, it is clear that unlike other methods of deletion and [UNK] token replacement, which perform only a single query, we need to perform multiple queries.
In the worst case, we would have all 30 samples of the token as unique, which will query the model 30 times.
Table 2 provides the results on AG News and Yelp datasets on fine-tuned BERT and ALBERT model.
Table 3 gives the results of attacking a fine-tuned BERT on MNLI.
Due to the unavailability of MNLI fine-tuned ALBERT model in TextAttack, we did not perform an attack on ALBERT.
To compare attacks across different transformerbased models, we evaluate the performance of Adv-OLM on IMDB dataset.
Table 4 provides the results of different attack methods on BERT, ALBERT, RoBERTa, DistilBERT and BiLSTM.
Adv-OLM was able to outperform previous attack methods on BERT, ALBERT, RoBERTa by increasing the success rate up to 10% for BAE-R and up to 6% for TextFooler.
On DistilBERT, Adv-OLM showed no change in the success rate, but the perturbation percentage was lowered slightly.
We also performed an attack on a non-transformer based BiLSTM model which did not show any improvements in the success rate.
One possible reason for this might be that in both OLM and OLM-S word sampling is performed using a transformer-based BERT language model.

 Our attack framework extends TextFooler (TF, Jin et al., 2020) in several ways.
First, a substitute gender classifier is trained, from which the logit output given a document is used to rank words by their prediction importance through an omission score (Section 3.1).
3.1 TargetWord Importance We are given a target classifier f, substitute classifier f0, a document D consisting of tokens Di, and a target label y.
The omission score is then given by oy(D) 􀀀 oy(Dni), and used in an importance score I of token Di, asas: IDi = 8>>>>< >>>>: oy(D) 􀀀 oy(Dni); if f0(D) = f0(Dni) = y: oy(D) 􀀀 oy(Dni) + oy(D) 􀀀 oy(Dni); if f0(D) = y; f0(Dni) = y; y 6= y: (1) With IDi calculated for all words in D, the top k ranked tokens are chosen as target words T. 3.2 Lexical Substitution Attacks Four approaches to perturb a target word t 2 T are considered in our experiments.
Synonym Substitution (WS) This TF-based substitution embeds t as t using a pre-trained embedding matrix V .
Ct is selected by computing the cosine similarity between t and all available wordembeddings w 2 V .
Masked Substitution (MB) The embeddingbased substitutions can be replaced by a language model predicting the contextually most likely token.
BERT (Devlin et al., 2019)—a bi-directional encoder (Vaswani et al., 2017) trained through masked language modeling and next-sentence prediction—makes this fairly trivial.
By replacing t with a mask, BERT produces a top-k most likely Ct for that position.
Dropout Substitution (DB) A method to circumvent the former (i.e., BERT’s masked prediction limitations for lexical substitution), was presented by Zhou et al.
They apply dropout (Srivastava et al., 2014) to BERT’s internal embedding of target word t before it is passed to the transformer—zeroing part of the weights with some probability.
The assumption is that Ct (BERT’s top-k) will contain candidates closer to the original t than the masked suggestions.
Random spaces: splits a token into two at a random position.
Part-of-Speech and Document Encoding TF employs two checking components: first, it removes any c that has a different POS tag than t. If multiple D0 exist so that f0(D0) = y, it selects the document D0 which has the highest cosine similarity to the Universal Sentence Encoder (USE) embedding (Cer et al., 2018) of the original document D. If not, the D0 with the lowest target word omission score is chosen (as per TF’s method).
BERT Similarity Zhou et al.
(2019) use the concatenation of the last four layers in BERT as a sentence’s contextualized representation h. We apply this in both Masked (MB) and Dropout (DB) BERT to re-rank all possible D0 by embedding them.
Given document D, target t, and perturbation candidate document D0, Ct would be ranked via an embedding similarity score: 4.1 Data We use three author profiling sets (see Table 1 for statistics) that are annotated for binary gender classification (male or female): first, that of Volkova et al.
Preprocessing & Sampling All three corpora were tokenized using spaCy5 (Honnibal and Montani, 2017).
Other than lowercasing, allocating special tokens to user mentions and hashtags (# and text were split), and URL removal, no additional preprocessing steps were applied.
For their synonym substitution component, we similarly used counter-fitted embeddings by Mrkˇsi´c et al.
The USE (Cer et al., 2018) implementation uses TensorFlow9 (Abadi et al., 2016a) as back-end, and all BERTvariants were implemented in Hugging Face’s10 Transformers library (Wolf et al., 2020) with Py- Torch11 (Paszke et al., 2019) as back-end.
The simplicity of this classifier also makes it a substitute model that can realistically be run by an author.
(2018), was proposed as a highly effective—simple—model that outperforms more complex (neural) alternatives on author profiling with little to no tuning.
It uses tfidf-weighted uni and bi-gram token features, character hexa-grams, and sublinearly scaled tf (1 + log(tf)).
If we assume an author does not have access to the data, the substitute classifier is trained on any other data than the Volkova et al.
To evaluate the semantic preservation of the attacked sentences, we calculate both METEOR (Banerjee and Lavie, 2005; Lavie and Denkowski, 2009) using nltk, and BERTScore (Zhang et al., 2020a) between D and DADV.
METEOR captures flexible uni-gram token overlap including morphological variants, and BERTScore calculates similarities with respect to the sentence context.
5.4 Transformer Performance Looking at the Top-1, Check and Check brackets (Table 3), other than the BERT-based models having higher success of transferability than TF, they also retain obfuscation success; deteriorating the target model’s performance to lower than chance level (55%) for the settings not using additional checks.
Additionally, the BERT similarity ranking (described in Section 3.3) applied to the Masked substitution candidates could have some beneficial effect.

The input to our model is a document D consisting of n tokens and k (predicted) event mentions {m1, m2, .
For other symbolic features, we train a joint classification model based on SpanBERT.
2.2 Single-Mention Encoder Given a document D, our model first forms a contextualized representation for each input token using a Transformer encoder (Joshi et al., 2020).
Let X = (x1, ..., xn) be the output of the encoder, where xi ∈ Rd.
Then, for each mention mi, its trigger’s representation ti is defined as the average of its token embeddings: ti =Xeij=sixjei − si + 1 Next, by using K trainable embedding matrices, we convert the symbolic features of mi into K vectors {h(1)i, h(2)i, .
2.3 Mention-Pair Encoder and Scorer Given two event mentions mi and mj , we define their trigger-based pair representation as: tij = FFNNtti, tj , ti ◦ tj (2) where FFNNt is a feedforward network mapping from R3×d → Rp, and ◦ is element-wise multiplication.
We use SpanBERT (spanbert-base-cased) as the Transformer encoder (Wolf et al., 2020a; Joshi et al., 2020).
Other features are predicted by a simple Transformer model.
The symbolic features contain information complementary to that in the SpanBERT contextual embeddings.

Formally, for a given sentence x, we first use a multi-layer encoder B(:), e.g., BERT (Devlin et al., 2019), to get the hidden representations r which contain both task generic and task specific information.
Then we introduce two disentanglement networks G(:) and S(:) to extract the generic representation g and specific representation s from r. For new tasks, we learn the classifiers by utilizing information from both spaces, and we allow different spaces to change to different extents to best retain knowledge from previous tasks.
More specifically, we insert a [SEP] token into each training example during tokenization to form a sequence pair labeled IsNext, and switch the first sequence and the second sequence to form a sentence pair labeled NotNext.
To this end, after training on t-th task, we employ K-means (MacQueen et al., 1967) to cluster all the examples from current training set St: For each x 2 St, we utilize its embedding B(x) as its input feature to conduct K-means.
5.3 Baselines We compare our proposed model with the following baselines in our experiments: • Finetune (Yogatama et al., 2019): finetune BERT model sequentially without the episodic memory module and any other loss.
• Regularization: On top of Replay, with an L2 regularization term added on the hidden state of the classifier following BERT.
• MBPA++ (de Masson d'Autume et al., 2019): augment BERT model with an episodic memory module and store all seen examples.
5.4 Implementation Details We use pretrained BERT-base-uncased from HuggingFace Transformers (Wolf et al., 2020) as our base feature extractor.
The task generic encoder and task specific encoder are both one linear layer followed by activation function Tanh, their output size are both 128 dimensions.
The predictors built on encoders are all one linear layer followed by activation function softmax.
All experiments are conducted on NVIDIA RTX 2080 Ti with 11GB memory with the batch size of 8 and the maximum sequence length of 256 (use the first 256 tokens if one’s length is beyond that).
6.2 Visualizing Disentangled Spaces To study whether our task generic encoder G tends to learn more generic information and task specific encoder S captures more task specific information, we used t-SNE (van der Maaten and Hinton, 2008) to visualize the two hidden spaces of IDBR, using the final model trained on order 2, and the results are shown in Figure 2, where Figure 2a visualizes task generic space and Figure 2b visualizes task specific space.
While we may expect to give more tolerance to specific space for changing, we found that no regularization on it would lead to severe forgetting of previously learnt good task specific embeddings, hence it is necessary to add a regularizer over this space as well.

Work has been done to solve TN by pure encoder-decoder methods particularly Recurrent Neural Networks (Sproat and Jaitly, 2017; Zhang et al., 2019).
The data should represent all the forms a particular token can appear in a given language.
Our approach curtails such errors by breaking down complex entities like dates into multiple tokens by a granular tokenization mechanism and also by limiting which tokens can be accepted into a class.
A way to limit the unacceptable errors in such systems would be to limit the kind of normalizations the network can generate for a token (Sproat and Jaitly, 2017).
On the other hand, solutions based on semiotic classification convert TN into a sequence tagging problem, where each class has associated mechanisms for normalizing the corresponding unnormalized token(s).
It produces verbalizations by first suitably tokenizing the input, then classifying the tokens, and then verbalizing each token according to its corresponding class.
These approaches often have a complex tokenization mechanism which is not easily transferable across languages and also need all the possible classes to be exhaustively defined manually.
We solve both these problems by a granular tokenization mechanism which extends the concept of semiotic classification to a granular level wherein each unique unnormalized token to normalized token mapping can have a class of its own.
Our classes represent whether a particular token is of a certain type and convert unnormalized tokens into their normalized form.
The solution is divided into 4 stages: i) Tokenization of unnormalized data, ii) Data preparation, iii) Classifying unnormalized tokens into correct classes, iv) Normalizing tokens using the corresponding class.
3.1 Tokenizer Typically TN approaches either assume presegmented text by the rule-based standard (Ebden and Sproat, 2014) which identifies multiword sequences as single segment like dates (Jan. 3, 2016) according to pre-defined semiotic classes or train a neural network for tokenization together with a normalization model (Zhang et al., 2019).
Proteno’s tokenization on the other hand, has elementary rules and is deterministic.
Eg: after splitting on spaces a token like ‘C3PO’ will be further split into [‘C’,‘3’,‘PO’].
Such tokenization enables the system to accurately split complex entities like dates while eliminating the need for a manually defined complex class for them.
The same tokenization mechanism was used for all the languages tested.
3.2 Data Preparation While collecting training data, first the unnormalized data is tokenized according to the granular tokenization mechanism described above and then each token is annotated with its corresponding normalized form.
Thus, we obtain unnormalized token to normalized token mappings.
Eg: a date occurrence ‘1/1/2020’ tokenized as [‘1’,‘/’,‘1’,‘/’,‘2020’] is annotated as [‘first’,‘of’, ‘January’,‘’,‘twenty twenty’].
Hence, while collecting the data we try to ensure decent coverage of different semiotic classes by having at least 25% of tokens which need normalization (i.e.
3.3 Classes Each class has 2 functions: i) Accepts: This function returns a boolean value of whether a token is accepted by the class.
Eg: cardinal class accepts only numeric tokens, ii) Normalize: This is a deterministic function that transforms the unnormalized token into its verbalized form A token can be classified into a class only if it is accepted by it.
By restricting the classes a token is accepted into, we limit the kind of normalization output that can be generated.
A token can be accepted by multiple classes which can give different normalizations.
If multiple classes give the same normalization for a token, then during inference it doesn’t matter which class is chosen.
Eg: self class indicates that the input is to be passed through as it is and it accepts tokens containing only alphabetical characters.
ii) Auto Generated (AG): Apart from pre-defined classes, the model learns automatically generated classes from the data by going through the unnormalized to normalized token mappings in the dataset.
If none of the existing classes (pre-coded or AG) can generate the target normalization for a token in the training data, then a class is automatically generated which accepts only the token responsible for its generation.
Its normalize function returns the target normalization observed in the annotated data for that token.
If multiple normalizations are observed for an unnormalized token in the dataset which cannot be generated by existing classes then multiple AGs are stored.
3.4 Classification & Normalization We model TN as a sequence tagging problem where the input is a sequence of unnormalized tokens and the output is the sequence of classes which can generate the normalized text.
Before training the classification model we transform the data to get unnormalized token to class mappings.
We prepare this data by going over the unnormalized token to normalized token mapping for a sentence and identifying which existing classes can give the target normalization.
For a token there can be multiple matching classes.
To classify the sequence of unnormalized tokens to their corresponding classes we experimented with 4 classifiers.
We first train a first order Conditional Random Fields (CRFs) (Lafferty et al., 2001) and then train neural network (NN) based architectures like Bi-LSTMs (Hochreiter and Schmidhuber, 1997), BiLSTM-CRFs (Huang et al., 2015) and Transformers (Vaswani et al., 2017).
We used word embeddings from Mikolov et al.
i) CRF: The features used for each unnormalized token in the model are - part of speech tag, list of classes which accept the token as an input, next token in sequence, suffix of the token (from length 1-4), prefix of the token (from length 1-4), is the token in upper case, is the token numeric and is the token capitalized, ii) Bi-LSTM & BiLSTM-CRFs: Using word embeddings and list of classes which accept the token as input features, iii) Transformer: A Transformer with 6 heads with word embeddings as input features.
For each token we renormalize the probabilities predicted over all classes to only the classes which accept the token.
Hence, the model is restricted to classify a token only to one of its few accepted classes.
If the system is unable to find a suitable class for the given token (i.e.
none of the given classes accept that token) then it gives a empty output instead of an incorrect normalization.
3.5 Aligning tokens in order of verbalization One of the major challenges in automated TN is handling realignment of tokens which may be required between the written and its spoken form.
Our method so far assumes monotonic alignment between the written unormalized tokens and their corresponding spoken normalizations.
Proteno first recognises instances of currency/measure in the text and prevents them from further splitting by the granular tokenizer.
The currency/measure classes have the same granular tokenisation logic along with realignment conditions.
They further pass the final tokens to their corresponding classes.
Due to budget constraints we could collect a dataset of only 135k tokens (5k sentences), ii) Tamil: We annotate the data sourced from English-Tamil parallel corpus (Ramasamy et al., 2012) and Comparable Corpora (Eckart and Quasthoff, 2013).
From these datasets we sampled 500k tokens (30k sentences) with higher preference towards sentences that needed normalization, iii) English: We used a portion of the annotated data from Sproat and Jaitly (2016).
First, we run the Proteno tokenizer over the unnormalized section of the dataset and got unnormalized token to normalized token mappings using elementary rules.
By doing so, we were able to correctly match only a portion of the dataset due to its different tokenization.
And then, from this subset, 300k tokens (24.7k sentences) were randomly sampled to keep the data size comparable to that used for Tamil.
This is 1.5% of the data used by Pramanik and Hussain (2019) which used first 20M tokens and 3% of data used by Zhang et al.
(2019) which used first 10M tokens.
Word Error Rate (WER) is used as the evaluation metric for the different classifiers.
We use this metric instead of classification accuracy on the classes in order to enable comparison of results from different TN approaches in the future, which may not use the same tokenization mechanism and hence may not have the same classes benchmarked by previous work.
We first evaluate all the classifiers on Spanish and then choose the classifier with lowest WER for Tamil and English.
Normalization was required for 27% of tokens in both the training and the test set.
On the test set, all models except Transformers showed statistically significant difference (p<<0.01) in comparison to the RB system.
We can attribute the lower performance of Transformers to lack of accepted classes as input features.
Even though Transformers give unstable performance in class prediction, they still give a low enough WER.
This particular iteration has a bias towards predicting cardinal_ masculine over cardinal_ feminine.
This bias changes with different iterations but the WER remains consistent as the normalization output remains unaffected.
BiLSTMs with the same configurations.
The token proportion and high level classification accuracy results for the tokens are detailed in Table 3.
Out of the 99.26% correctly normalized tokens, 88.2% of the non-self tokens were normalized via AGs i.e.
Moreover, Proteno does not have the same set of classes due to its granular tokenization mechanism.
It cannot use the full dataset due to differing tokenization mechanisms which result into mismatch in the alignment between the unnormalized token and their corresponding normalized forms.
However, we extract their pre-defined categories on the dataset we used and evaluate how many tokens within them were normalized correctly.
It illustrates the token normalization accuracy achieved by Proteno on the test dataset for all the categories which had instances in the small subset we have used.
For complex entities likes date Proteno gave 98.16% accuracy on the 6% tokens available in test set.

To do this, a classifier and a discriminator (adversary) are trained jointly from the same feature representation to maximize the classifier’s performance while simultaneously minimizing the discriminator’s.
2.2 Model Components (a) Topic-oriented Document Encoder We encode each example x = (d, t, y) using bidirectional conditional encoding (BiCond) (Augenstein et al., 2016), since computing representations conditioned on the topic have been shown to be crucial for zero-shot stance detection (Allaway and McKeown, 2020).
Specifically, we first encode the topic as ht using a BiLSTM (Hochreiter and Schmidhuber, 1997) and then encode the text using a second BiLSTM conditioned on ht.
To compute a document-level representation vdt, we apply scaled dot-product attention (Vaswani et al., 2017) over the output of the text BiLSTM, using the topic representation ht as the query.
This encourages the text encoder to produce representations that are indicative of stance on  the topic and so would improve classification performance.
To prevent the adversary corrupting the encoder to reduce its own performance, we add a document reconstruction term (Lrec d) to our loss function, as in Zhang et al.
(2017), as well as a topic reconstruction term (Lrect), to ensure the output of neither BiLSTM is corrupted.
We use a non-linear transformation over the hidden states of each BiLSTM for reconstruction.
(c) Stance Classifier We use a two-layer feedforward neural network with a ReLU activation to predict stance labels ` ∈ {−1, 0, 1}.
Since stance is inherently dependent on a topic, and the output of the transformation layer should be topic-invariant, we add a residual connection between the topic encoder ht and the stance classifier.
(d) Topic Discriminator Our topic discriminator is also a two-layer feed-forward neural network with ReLU and predicts the topic t of the input x, given the output of the transformation layer vfdt.
For both the stance classifier and topic-discriminator we use cross-entropy loss (Ls and Lt respectively).
The hyperparameter ρ gradually increases across epochs, following Ganin and Lempitsky (2015).
Baselines We compare against a BERT (Devlin et al., 2019) baseline that encodes the document and topic jointly for classification, as in Allaway and McKeown (2020) and BiCond – bidirectional conditional encoding (§2.2) without attention (Augenstein et al., 2016).
We select the best hyperparameter setting using the average rank of the stance classifier F1 (higher is better) and topic discriminator F1 (lower is better).
Our implementations of BERT and BiCond are trained in the same setting as TOAD (i.e., 5 topics for train/dev, 1 topic for test).
These results are statistically significant (p < 0.005)when compared to both the BERT baseline and to TOAD without the adversary 1.
We also observe that TOAD is statistically indistinguishable from BERT on three additional topics (HC, LA, CC) while having only 0.5% as many parameters (600k versus 110mil).
As a result of this small size, TOAD can be trained using only the CPU and, because of it’s recurrent architecture, would gain less from the increased parallel computation of a GPU (compared to a transformer-based model).
Therefore, TOAD has a potentially much lower environmental impact than BERT with similar (or better) performance on five out of six zero-shot topics.

A training epoch starts with all neurons being equally active, which are progressively polarized within the epoch.
Specifically, s is annealed as follows: s =1smax+ (smax − 1smax)b − 1B − 1, (11) where b is the batch index and B is the total number of batches in an epoch.
We have 3 baselines under NL, (1) BERT, (2) Adapter-BERT and (3) W2V (word2vec embeddings).
For BERT, we use trainable BERT to perform ASC (see Sec.
3); Adapter-BERT adapts the BERT as in (Houlsby et al., 2019), where only the adapter blocks are trainable; W2V uses embeddings trained on the Amazon review data in (Xu et al., 2018) using FastText (Grave et al., 2018).
The 3 baselines under WDF are also (4) BERT, (5) Adapter-BERT and (6) W2V.
We replace their original MLP or CNN image classification network with CNN for text classification (Kim, 2014).
From the 6 systems, we created 6 baselines using W2V embeddings with the aspect term added before the sentence so that the CL methods can take both aspect and the review sentence, and 6 baselines using BERT (Frozen) (which replaces W2V embeddings).
Following the BERT formulation in Sec.
Adapter-BERT is not applicable to them as their architecture cannot use an adapter.
The dynamic routing is repeated for 3 iterations.
For the task-specific module, We employ the embedding with 2000 dimensions as the final and hidden layer of the TSM.
The task ID embeddings have 2000 dimensions.
A fully connected layer with softmax output is used as the classification heads in the last layer of the BERT, together with the categorical cross-entropy loss.
The training of BERT, Adapter-BERT and B-CL follow that of (Xu et al., 2019).
We adopt BERTBASE (uncased).
For the SemEval datasets, 10 epochs are used and for all other datasets, 30 epochs are used based on results from validation data.
We early-stop training when there is no improvement in the validation loss for 5 epochs.
We discuss the detailed observations below: (1) For non-continual learning (NL) baselines, BERT and Adapter-BERT perform similarly.
However, WDF is much worse than NL for BERT (with fine-tuning) and Adapter-BERT (with adapter-tuning).
This is because BERT with finetuning learns highly task specific knowledge (Merchant et al., 2020).
(3) Unlike BERT and Adapter-BERT, our BCL can do very well in both forgetting avoidance and knowledge transfer (outperforming all baselines).
“-KSM;-TSM” means without knowledge sharing and task specific modules, simply deploying an Adapter-BERT.

