Formally, for a given sentence x, we first use a multi-layer encoder B(:), e.g., BERT (Devlin et al., 2019), to get the hidden representations r which contain both task generic and task specific information.
Then we introduce two disentanglement networks G(:) and S(:) to extract the generic representation g and specific representation s from r. For new tasks, we learn the classifiers by utilizing information from both spaces, and we allow different spaces to change to different extents to best retain knowledge from previous tasks.
More specifically, we insert a [SEP] token into each training example during tokenization to form a sequence pair labeled IsNext, and switch the first sequence and the second sequence to form a sentence pair labeled NotNext.
To this end, after training on t-th task, we employ K-means (MacQueen et al., 1967) to cluster all the examples from current training set St: For each x 2 St, we utilize its embedding B(x) as its input feature to conduct K-means.
5.3 Baselines We compare our proposed model with the following baselines in our experiments: • Finetune (Yogatama et al., 2019): finetune BERT model sequentially without the episodic memory module and any other loss.
• Regularization: On top of Replay, with an L2 regularization term added on the hidden state of the classifier following BERT.
• MBPA++ (de Masson d'Autume et al., 2019): augment BERT model with an episodic memory module and store all seen examples.
5.4 Implementation Details We use pretrained BERT-base-uncased from HuggingFace Transformers (Wolf et al., 2020) as our base feature extractor.
The task generic encoder and task specific encoder are both one linear layer followed by activation function Tanh, their output size are both 128 dimensions.
The predictors built on encoders are all one linear layer followed by activation function softmax.
All experiments are conducted on NVIDIA RTX 2080 Ti with 11GB memory with the batch size of 8 and the maximum sequence length of 256 (use the first 256 tokens if one’s length is beyond that).
6.2 Visualizing Disentangled Spaces To study whether our task generic encoder G tends to learn more generic information and task specific encoder S captures more task specific information, we used t-SNE (van der Maaten and Hinton, 2008) to visualize the two hidden spaces of IDBR, using the final model trained on order 2, and the results are shown in Figure 2, where Figure 2a visualizes task generic space and Figure 2b visualizes task specific space.
While we may expect to give more tolerance to specific space for changing, we found that no regularization on it would lead to severe forgetting of previously learnt good task specific embeddings, hence it is necessary to add a regularizer over this space as well.
