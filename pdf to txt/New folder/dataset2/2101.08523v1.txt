We assume a black-box setting where the attacker can only query the classifier for output label probabilities for the given input.
For evaluating the effectiveness of our proposed approach, we experiment with SOTA text classifiers i.e.
transformer based models like BERT (Devlin et al., 2018), ALBERT (Lan et al., 2019), RoBERTa (Liu et al., 2019) and DistilBERT (Sanh et al., 2019).
We use the default language model (BERT) employed in the OLM and OLM-S, and kept the number of samples generated by the OLM language model as 30 in all the experiments.
Number of queries in Adv-OLM: From equations 2 and 3, it is clear that unlike other methods of deletion and [UNK] token replacement, which perform only a single query, we need to perform multiple queries.
In the worst case, we would have all 30 samples of the token as unique, which will query the model 30 times.
Table 2 provides the results on AG News and Yelp datasets on fine-tuned BERT and ALBERT model.
Table 3 gives the results of attacking a fine-tuned BERT on MNLI.
Due to the unavailability of MNLI fine-tuned ALBERT model in TextAttack, we did not perform an attack on ALBERT.
To compare attacks across different transformerbased models, we evaluate the performance of Adv-OLM on IMDB dataset.
Table 4 provides the results of different attack methods on BERT, ALBERT, RoBERTa, DistilBERT and BiLSTM.
Adv-OLM was able to outperform previous attack methods on BERT, ALBERT, RoBERTa by increasing the success rate up to 10% for BAE-R and up to 6% for TextFooler.
On DistilBERT, Adv-OLM showed no change in the success rate, but the perturbation percentage was lowered slightly.
We also performed an attack on a non-transformer based BiLSTM model which did not show any improvements in the success rate.
One possible reason for this might be that in both OLM and OLM-S word sampling is performed using a transformer-based BERT language model.
