@&#MAIN-TITLE@&#Adversarial Stylometry in the Wild: Transferable Lexical Substitution Attacks on Author Profiling



@&#ABSTRACT@&#

Written language contains stylistic cues that can be exploited to automatically infer a variety of potentially sensitive author information. Adversarial stylometry intends to attack such models by rewriting an authorâ€™s text. Our research proposes several components to facilitate deployment of these adversarial attacks in the wild, where neither data nor target models are accessible. We introduce a transformerbased extension of a lexical replacement attack, and show it achieves high transferability when trained on a weakly labeled corpusâ€” decreasing target model performance below chance. While not completely inconspicuous, our more successful attacks also prove notably less detectable by humans. Our framework therefore provides a promising direction for future privacy-preserving adversarial attacks.


@&#METHOD@&#

Our attack framework extends TextFooler (TF, Jin et al., 2020) in several ways. First, a substitute gender classifier is trained, from which the logit output given a document is used to rank words by their prediction importance through an omission score (Section 3.1). For the top most important words, substitute candidates are proposed, for which we add two additional techniques (Section 3.2). These candidates can be checked and filtered on consistency with the original words (by their POS tags, for example), accepted as-is, or re-ranked (Section 3.3). For the latter, we add a scoring method. Finally, the remaining candidates are used for iterative substitution until TFâ€™s stopping criterion is met (i.e., the prediction changes, or candidates run out).
3.1 TargetWord Importance
We are given a target classifier f, substitute classifier f0, a document D consisting of tokens Di, and a target label y. Here, f0 is trained on some corpus X, and receives an authorâ€™s new input text D, where the author provides label y. We denote a class label as y if f0(D) predicts anything but y. Our perturbations form adversarial input DADV, that intends to produce f0(DADV) = y, and thereby
implicitly f(DADV) = y. Note that we only submit D to f for evaluating the attack effectiveness, and it is never used to fit the attack itself. To create DADV, a minimum number of edits is preferred, and thus we rank all words in D by their omission score (similar to e.g., KÂ´adÂ´ar et al., 2017) according to f0 (omission score in Algorithm 1). Let Dni denote the document after deleting Di, and oy(D) the logit score by f0. The omission score is then given by oy(D) ô€€€ oy(Dni), and used in an importance score I of token Di, asas:
IDi =
8>>>><
>>>>:
oy(D) ô€€€ oy(Dni);
if f0(D) = f0(Dni) = y:
oy(D) ô€€€ oy(Dni) + oy(D) ô€€€ oy(Dni);
if f0(D) = y; f0(Dni) = y; y 6= y:
(1)
With IDi calculated for all words in D, the top k ranked tokens are chosen as target words T.

3.2 Lexical Substitution Attacks
Four approaches to perturb a target word t 2 T are considered in our experiments. These operations are referred to as candidates in Algorithm 1. Synonym Substitution (WS) This TF-based substitution embeds t as t using a pre-trained embedding matrix V . Ct is selected by computing the cosine similarity between t and all available wordembeddings w 2 V . We denote cosine similarity with (t;w). A threshold  is used to keep only reliable candidates (t;w) > . Masked Substitution (MB) The embeddingbased substitutions can be replaced by a language model predicting the contextually most likely token. BERT (Devlin et al., 2019)â€”a bi-directional encoder (Vaswani et al., 2017) trained through masked language modeling and next-sentence predictionâ€”makes this fairly trivial. By replacing t with a mask, BERT produces a top-k most likely Ct for that position. Implementing this in TF does imply each previous substitution of t might be included in the context of the current one. This method of contextual replacement has two drawbacks: i) semantic consistency with the original word is not guaranteed (as the model has no knowledge of t), and ii) the replaced context means semantic drift can occur, as all subsequent substitutions follow the new, possibly incorrect context.
Dropout Substitution (DB) A method to circumvent the former (i.e., BERTâ€™s masked prediction limitations for lexical substitution), was
presented by Zhou et al. (2019). They apply dropout (Srivastava et al., 2014) to BERTâ€™s internal embedding of target word t before it is passed to the transformerâ€”zeroing part of the weights with some probability. The assumption is that Ct (BERTâ€™s top-k) will contain candidates closer to the original t than the masked suggestions.
Heuristic Substitution To evaluate the relative performance of the techniques we described before, we employ several heuristic attacks as baselines. In the order of Table 3: 1337-speak: converts characters to their leetspeak variants, in a similar vein to e.g. diacritic conversion (Belinkov and Bisk, 2018). Character flip: inverts two characters in the middle of a word, which was shown to least affect readability (Rayner et al., 2006). Random spaces: splits a token into two at a random position.
3.3 Candidate Filtering and Re-ranking
Given Ct, either all, or only the highest ranked candidate can be accepted as-is. Alternatively, all D0 can be filtered by submitting them to checks, or reranked based on their semantic consistency with D. These operations are referred to as rank/filter in Algorithm 1â€”both of which can be executed. 
Part-of-Speech and Document Encoding TF employs two checking components: first, it removes any c that has a different POS tag than t. If multiple D0 exist so that f0(D0) = y, it selects the document D0 which has the highest cosine similarity to the Universal Sentence Encoder (USE) embedding (Cer et al., 2018) of the original document D. If not, the D0 with the lowest target word omission score is chosen (as per TFâ€™s method).
BERT Similarity Zhou et al. (2019) use the concatenation of the last four layers in BERT as a sentenceâ€™s contextualized representation h. We apply this in both Masked (MB) and Dropout (DB) BERT to re-rank all possible D0 by embedding them. Given document D, target t, and perturbation candidate document D0, Ct would be ranked via an embedding similarity score:

                           
                        

@&#EXPERIMENT@&#

4.1 Data
We use three author profiling sets (see Table 1 for statistics) that are annotated for binary gender classification (male or female): first, that of Volkova et al. (2015) which was collected through annotating 5,0004 English Twitter profiles by crowdsourcing via Mechanical Turk. This can be considered a â€˜randomâ€™ sample of Twitter profiles, and is therefore the most unbiased set of the three. Hence, we consider it the most representative of an author profiling set, and employ this as training split (80%) for f, and test split for our attacks (20%). The second is the English portion of the Multilingual Hate Speech Fairness corpus of Huang et al. (2020), which was collected with a different objective than author profiling. It was aggregated from existing hate speech corpora (by Waseem and Hovy, 2016; Waseem, 2016; Founta et al., 2018)â€”which were largely bootstrapped with lookup terms, selection of frequently abusive users, etc.â€”and annotated post-hoc with demographic information. The collection did not focus on profiles, and most authors are only associated with a single tweet. This can cause a significant domain shift compared to general author profiling. However, it can be seen as freely available (noisy) data. Lastly, we include a weakly labeled author profiling corpus by Emmery et al. (2017), collected through English keyword look-up for self-reportsâ€” similar to Beller et al. (2014). This corpus likely includes incorrect labels, but was collected in less than a day, making it an ideal candidate for realistic access to (new) data to fit the substitute model.
Preprocessing & Sampling All three corpora were tokenized using spaCy5 (Honnibal and Montani, 2017). Other than lowercasing, allocating special tokens to user mentions and hashtags (# and text were split), and URL removal, no additional preprocessing steps were applied. Every author timeline was divided into chunks for a maximum of 100 tweets (i.e., some contain less) to form our documents, implying a maximum of 25 instances per author (some contain one, 2,500 is the API history limit). From the test set, the last6 200 instances were sampled for the attack (110 male, 90 female). While fairly small, this sample does reflect a realistic attack duration and timeline size, as they would be executed for a single profile.
4.2 Attacks
For the extension of TF, we re-implemented code7 by Jin et al. (2020) to work with Scikit-learn8 (Pedregosa et al., 2011). For their synonym substitution component, we similarly used counter-fitted embeddings by MrkË‡siÂ´c et al. (2016) trained on Simlex-999 (Hill et al., 2015). The USE (Cer et al., 2018) implementation uses TensorFlow9 (Abadi et al., 2016a) as back-end, and all BERTvariants were implemented in Hugging Faceâ€™s10 Transformers library (Wolf et al., 2020) with Py- Torch11 (Paszke et al., 2019) as back-end. We adopt the same parameter settings as Jin et al. (2020) throughout our TF experiments: they set N (considered synonyms) and  (cosine similarity minimum) empirically to 50 and 0.7 respectively. For MB and DB, we capped T at 50 and top-k at 10 (to improve speed). For DB, we follow Zhou et al. (2019) and set the dropout probability to 0.3.

4.3 Models
For f and f0 we require (preferably fast) pipelines that achieve high accuracy on author profiling tasks, and are sufficiently distinct to gauge how well our attacks transfer across architectures, rather than solely across corpora. As state-of-the-art algorithms have not yet proven to be sufficiently effective for author profiling (Joo et al., 2019) we opt for common n-gram features and linear models. 
Logistic Regression Logistic Regression (LR) trained on tfidf using uni and bi-gram features proved a strong baseline in author profiling in prior work. The simplicity of this classifier also makes it a substitute model that can realistically be run by an author. No tuning was performed: C is set to 1. 
N-GrAM The New Groningen Author-profiling Model (N-GrAM) from Basile et al. (2018), was proposed as a highly effectiveâ€”simpleâ€”model that outperforms more complex (neural) alternatives on author profiling with little to no tuning. It uses tfidf-weighted uni and bi-gram token features, character hexa-grams, and sublinearly scaled tf (1 + log(tf)). These features are then passed to a Linear Support Vector Machine (Cortes and Vapnik, 1995; Fan et al., 2008), where C = 1.

4.4 Experimental Setup
To summarize (and see Table 2), the experiment is conducted as follows: the substitute target model (f0)â€”LR for all experimentsâ€”is fit on a given corpus. The real target model (f, either LR or NGrAM) is always fit on the corpus of Volkova et al. (2015). To evaluate the attacks, a 200-instance sample is used. Target words are ranked via omission scores from f0, fed to either our Heuristics, TF, MB, or DB attacks. The heuristics directly change the target words, while the rest outputs a ranked set of replacement candidates. The latter can either be evaluated against f0 through the TF pipeline, or the Top-1 candidate is returned. Filtering can be applied through POS/USE for semantic similarity and POS compatibility checks (Check), or not (Check) Note that we are predominantly interested in transferability, and would therefore like to test as many combinations of data and architecture access limitations as possible. If we assume an author does not have access to the data, the substitute classifier is trained on any other data than the Volkova et al. corpus. If we assume the author does not know the target model architecture, the target model is N-GrAM (rather than LR). A full model transfer setting (in both data and architecture) will therefore be, e.g.: data f0 = Emmery et al., data f = Volkova et al., f0 = LR, and f = NGrAM. Finally, for comparison to an optimal situation, we test a setting where we do have access to the adversaryâ€™s data.\

4.5 Evaluation
Metrics The obfuscation success is measured as any accuracy score below chance level performance, which given our test sample is 55%. We would argue that random performance is preferred in scenarios where the prediction of the opposite label is undesired. For the current task, however, any accuracy drop to around or lower than chance level satisfies the conditions for successful obfuscation. To evaluate the semantic preservation of the attacked sentences, we calculate both METEOR (Banerjee and Lavie, 2005; Lavie and Denkowski, 2009) using nltk, and BERTScore (Zhang et al., 2020a) between D and DADV. METEOR captures flexible uni-gram token overlap including morphological variants, and BERTScore calculates similarities with respect to the sentence context.
Human Evaluation For the human evaluation, we sampled 20 document pieces (one or more tweets) for each attack type in the best performing
experimental configuration. A piece was chosen if it satisfied these criteria: i) contains changes for all three attacks, ii) consists of at least 15 words (excluding emojis and tags), and iii) does not contain obvious profanity.14 All 60 document pieces of the three models were shuffled, and the 20 original versions were appended at the end (so that â€˜correctâ€™ pieces were seen last). Each substitute model therefore has 80 items for evaluation. While in prior work it is common to rate semantic consistency, fluency, and label a text (see e.g., Potthast et al., 2016; Jin et al., 2020), our Twitter data are too noisy (including many spelling and grammar errors in the originals), and document batches too long to make this a feasible task. Instead, our six participants (three per substitute) were asked to indicate if: a) a sentence was artificially changed, and if so, b) indicate one word that raised their suspicion. This way, we can evaluate which attack produces the most natural sentences, and the least obvious changes to the input The items were rated individually; the human evaluators did not know beforehand that different versions of the same sentences were repeated, nor that the originals were shown at the end. All participant have a university-level education, a high English proficiency, and are familiar with the domain of the data. Several example ratings of the same sentence can be found in Table 6.
                  


@&#RESULT@&#


As we alluded to in Section 4.1, both corpora used to train our substitute models were in fact not reference corpora for author profiling, and can therefore be considered as suboptimal, disjoint domains. The Huang et al. corpus in particular shows a strong domain shift (see Table 4) for both training and test sets. The distantly labeled Emmery et al. corpus achieves 7.5% more accuracy on the train split of Volkova et al., and test performance is significantly higher (27%). We might therefore expect better obfuscation performance from the latter.
5.2 Baselines
The results for all attacks are shown in Table 3. Note that these are performances for f; therefore, when no attacks are applied (none), the performance for both substitute corpora stays the same (as those only influence the attacks). For the heuristic attacks, 1337 seems to make the more robust baseline; outperforming some of the other settingsâ€” even on transferability. A surface-level advantage is that this attack has a minor impact on readability (when applied conservatively) and does not change semantics; however, the heuristic attacks are fairly simple to mitigate in preprocessing (Juola and Vescovi, 2011) and through character features (as shown by the performance of the N-GrAM model). For transferability, we evidently need to do more than simply trying to convert words to be out-of-vocabulary (OOV) with noise. While it can be argued the heuristics could change all words, shifting everything OOV would not be robust; the target model side could easily spot the anomalous input and might act (e.g., reject) accordingly.
5.3 Attack Transferability
Transferability can be assessed by comparing the LR and N-GrAM (NG) columns. Globally it can be observed that the substitute models trained on the Emmery et al. corpus systematically outperform those trained on Huang et al.; both for the settings where the adversaryâ€™s architecture is known (LR), and where it is unknown (NG). This matches our expectations from the observed domain shift. Our results also show that a noticeable decrease in obfuscation performance occurs (10-30% increased target model performance) when the attacks are transferred to different data and another model. In contrast, as can be observed from the last two columns in Table 3, in a practically unrealistic setting where the model and data are available, the obfuscation is specifically tailored to known weaknesses and therefore highly effective.

5.4 Transformer Performance
Looking at the Top-1, Check and Check brackets (Table 3), other than the BERT-based models having higher success of transferability than TF, they also retain obfuscation success; deteriorating the target modelâ€™s performance to lower than chance level (55%) for the settings not using additional checks. This comparison also demonstrates the synonym ranking to work (Top-1 vs. Check and Check), and the Check condition to be too restrictive; attaining lower attack power, and low transferability. This is further illustrated by the %- changes shown in Figure 2. Comparing the MB and DB variants, their performance seems almost identical, with masking having a slight advantage. As Zhou et al. (2019) argued, applying dropout should produce words that are closer to the original (compared to MB), which might affect obfuscation performance. Additionally, the BERT similarity ranking (described in Section 3.3) applied to the Masked substitution candidates could have some beneficial effect. This will have to be studied in more detail using the output evaluations.


@&#DISCUSSION@&#


@&#CONCLUSION@&#


In our work, we argued realistic adversarial stylometry should be tested on transferability in settings where there is no access to the target modelâ€™s data or architecture. We extended previous adversarial text classification work with two transformer-based models, and studied their obfuscation success in such a setting. We showed them to reliably drop target model performance below chance, though human detectability of the attacks remained above chance. Future work could focus on further minimizing this detection under our realistic constraints.




@&#REFERENCES@&#

