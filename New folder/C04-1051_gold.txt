Altogether we collected 11,162 clusters in an 8 month period, assembling 177,095 articles with an average of 15.8 articles per cluster.
Filtered in this way, our dataset yields 139K non identical sentence pairs at a Levenshtein distance of n ? 12.
3 Mean Levenshtein distance was 5.17, and mean sentence length was 18.6 words.
Nevertheless, even after filtering in these ways, a significant amount of unfiltered noise remains in the F2 corpus, which consisted of 214K sentence pairs.
Out of a sample of 448 held-out sentence pairs, 118 (26.3%) were rated by two independent human evaluators as sentence-level paraphrases, while 151 (33.7%) were rated as partial paraphrases.
The remaining ~40% were assessed as News article clusters: URLs Download URLs, Isolate content (HMM), Sentence separate Textual content of articles Select and filter first sentence pairs Approximately parallel monolingual corpus Figure 1.
4 Thus, although the F2 data set is nominally larger than the L12 data set, when the noise factor is taken into account, the actual number of full paraphrase sentences in this data set is estimated to be in the region of 56K sentences, with a further estimated 72K sentences containing some paraphrase material that might be a potential source of alignment.
AER measures how accurately an automatic algorithm can align words in corpus of parallel sentence pairs, with a human 4 This contrasts with 16.7% pairs assessed as unrelated in a 10,000 pair sampling of the L12 data.
After an initial training pass and refinement of the linking specification, interrater agreement measured in terms of AER5 was 93.1% for the edit distance test set versus 83.7% for the F2 test set, suggestive of the greater variability in the latter data set.
The best overall performance, irrespective of test data type, is achieved by the L12 training set, with an 11.58% overall AER on the 250 sentence pair edit distance test set (20.88% AER for non-identical words).
The F2 training data is probably too sparse and, with 40% unrelated sentence pairs, too noisy to achieve equally good results; nevertheless the gap between the results for the two training data types is dramatically narrower on the F2 test data.
The nearly comparable numbers for the two training data sets, at 13.2% and 14.7% respectively, suggest that the L12 training corpus provides no substantive advantage over the F2 data when tested on the more complex test data.
The most common paraphrase alternations that we observed fell into the following broad categories: ? Elaboration: Sentence pairs can differ in total information content, with an added word, phrase or clause in one sentence that has no Training Data Type: L12 F2 L12 F2 Test Data Type: 250 Edit Dist 250 Edit Dist 116 F2 Heuristic 116 F2 Heuristic Precision 87.46% 86.44% 85.07% 84.16% Recall 89.52% 82.64% 88.70% 86.55% AER 11.58% 15.41% 13.24% 14.71% Identical word precision 89.36% 88.79% 92.92% 93.41% Identical word recall 89.50% 83.10% 93.49% 92.47% Identical word AER 10.57% 14.14% 6.80% 7.06% Non-Identical word precision 76.99% 71.86% 60.54% 53.69% Non-Identical word recall 90.22% 69.57% 59.50% 50.41% Non-Identical word AER 20.88% 28.57% 39.81% 47.46% Table 1.
