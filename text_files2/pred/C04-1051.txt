Altogether we collected 11,162 clusters in an 8 month period, assembling 177,095 articles with an average of 15.8 articles per cluster.
The first used string edit distance, counting the number of lexical deletions and insertions needed to transform one string into another.
3 Mean Levenshtein distance was 5.17, and mean sentence length was 18.6 words.
The combination of the first-two sentences heuristic plus topical article clusters allows us to take advantage of meta-information implicit in our corpus, since clustering exploits lexical information from the entire document, not just the few sentences that are our focus.
Out of a sample of 448 held-out sentence pairs, 118 (26.3%) were rated by two independent human evaluators as sentence-level paraphrases, while 151 (33.7%) were rated as partial paraphrases.
The remaining ~40% were assessed as News article clusters: URLs Download URLs, Isolate content (HMM), Sentence separate Textual content of articles Select and filter first sentence pairs Approximately parallel monolingual corpus Figure 1.
4 Thus, although the F2 data set is nominally larger than the L12 data set, when the noise factor is taken into account, the actual number of full paraphrase sentences in this data set is estimated to be in the region of 56K sentences, with a further estimated 72K sentences containing some paraphrase material that might be a potential source of alignment.
The following pair, for example, would be unlikely to pass muster on edit distance grounds, but nonetheless contains an inversion of deep semantic roles, employing different lexical items.
AER measures how accurately an automatic algorithm can align words in corpus of parallel sentence pairs, with a human 4 This contrasts with 16.7% pairs assessed as unrelated in a 10,000 pair sampling of the L12 data.
The first is a set of 250 sentence pairs extracted on the basis of an edit distance of 5 ?
After an initial training pass and refinement of the linking specification, interrater agreement measured in terms of AER5 was 93.1% for the edit distance test set versus 83.7% for the F2 test set, suggestive of the greater variability in the latter data set.
Because the AER is asymmetric (though each direction differs by less than 5%), we have presented the average of the directional AERs.
The best overall performance, irrespective of test data type, is achieved by the L12 training set, with an 11.58% overall AER on the 250 sentence pair edit distance test set (20.88% AER for non-identical words).
The F2 training data is probably too sparse and, with 40% unrelated sentence pairs, too noisy to achieve equally good results; nevertheless the gap between the results for the two training data types is dramatically narrower on the F2 test data.
The nearly comparable numbers for the two training data sets, at 13.2% and 14.7% respectively, suggest that the L12 training corpus provides no substantive advantage over the F2 data when tested on the more complex test data.
Elaboration: Sentence pairs can differ in total information content, with an added word, phrase or clause in one sentence that has no Training Data Type: L12 F2 L12 F2 Test Data Type: 250 Edit Dist 250 Edit Dist 116 F2 Heuristic 116 F2 Heuristic Precision 87.46% 86.44% 85.07% 84.16% Recall 89.52% 82.64% 88.70% 86.55% AER 11.58% 15.41% 13.24% 14.71% Identical word precision 89.36% 88.79% 92.92% 93.41% Identical word recall 89.50% 83.10% 93.49% 92.47% Identical word AER 10.57% 14.14% 6.80% 7.06% Non-Identical word precision 76.99% 71.86% 60.54% 53.69% Non-Identical word recall 90.22% 69.57% 59.50% 50.41% Non-Identical word AER 20.88% 28.57% 39.81% 47.46% Table 1.
Precision, recall, and alignment error rates (AER) for F2 and L12 counterpart in the other (e.g.
Figure 2 shows a hand-aligned paraphrase pair taken from the F2 data.
To quantify the differences between L12 and F2, we randomly chose 100 sentence pairs from each dataset and counted the number of times each phenomenon was encountered.
