For example, the set of attribute-value pairs { $cityfr = New York City, $city-to = Seattle , $time-dep = 6 a.m., $date-dep = Wednesday } represent the meaning of the noun phrase &quot;a flight to Seattle that departs from New York City at 6 a.m. on Wednesday&quot;.
The goal, more specifically, is then to learn the optimal attribute ordering and lexical choice for the text to be generated from the attribute-value pairs.
Furthermore, it should automatically decide if the lexical choice in &quot;flights departing to New York&quot; is better or worse than the choice in &quot;flights leaving to New York&quot;.
The systems NLG1, NLG2 and NLG3 all implement step 1; they produce a sequence of words intermixed with attributes, i.e., a template, from the the attributes alone.
The surface generation system NLG2 assumes that the best choice to express any given attribute-value set is the word sequence with the highest probability that mentions all of the input attributes exactly once.
The local and non-local information is integrated with use of features in a maximum entropy probability model, and a highly pruned search procedure attempts to find the best scoring word sequence according to the model.
The probability model in NLG2 is a conditional distribution over V U * stop*, where V is the generation vocabulary and where *stop* is a special &quot;stop&quot; symbol.
The form of the maximum entropy probability model is identical to the one used in (Berger et al., 1996; Ratnaparkhi, 1998): where wi ranges over V U *stop* and {wi-i wi-2, attri} is the history, where wi denotes the ith word in the phrase, and attri denotes the attributes that remain to be generated at position i in the phrase.
The probability of the sequence W = wn, given the attribute set A, (and also given that its length is n) is: The feature patterns, used in NLG2 are shown in Table 3.
01 if w = from and wi-i = flight and $city — fr E attri otherwise Input to Step 1: 1 $city-fr, $city-to, $time-dep, $date-dep 1 Output of Step 1: &quot;a flight to $city-to that departs from $city-fr at $time-dep on $date-dep&quot; Input to Step 2: &quot;a flight to $city-to that departs from $city-fr at $time-dep on $date-dep&quot;, $city-fr = New York City, $city-to = Seattle , $time-dep = 6 a.m., $date-dep = Wednesday } Output of Step 2: &quot;a flight to Seattle that departs from New York City at 6 a.m. on Wednesday&quot; Low frequency features involving word n—grams tend to be unreliable; the NLG2 system therefore only uses features which occur K times or more in the training data.
More specifically, the search procedure implements the recurrence: VVNa = top(N,{wlw E V}) 147.mi-1-1 = top(N,next(WN,i)) The set WN,i is the top N scoring sequences of length i, and the expression next(WN,i) returns all sequences w1 w,+1 such that wi w, E WN,, and wi-Fi E V U *stop*.
The expression top(N,next(WN,i)) finds the top N sequences in next(WN,i).
Currently, there is no normalization for different lengths, i.e., all sequences of length n < M are equiprobable: NLG2 chooses the best answer to express the attribute set A as follows: where Wnig2 are the completed word sequences that satisfy the conditions of the NLG2 search described above.
The probability model for NLG3, shown in Figure 2, conditions on the parent, the two closest siblings, the direction of the child relative to the parent, and the attributes that remain to be generated.
The expression chi(w) denotes the ith closest child to the headword w, par(w) denotes the parent of the headword w, dir E {left, right} denotes the direction of the child relative to the parent, and attr,i denotes the attributes that remain to be generated in the tree when headword w is predicting its ith child.
As shown in Figure 3, the probability of a dependency tree that expresses an attribute set A can be found by computing, for each word in the tree, the probability of generating its left children and then its right children.'
The features in NLG3 have access to syntactic information whereas the features in NLG2 do not.
NLG3 chooses the best answer to express the attribute set A as follows: where Tn193 are the completed dependency trees that satisfy the conditions of the NLG3 search described above.
The training and test sets used to evaluate NLG1, NLG2 and NLG3 were derived semi-automatically from a pre-existing annotated corpus of user queries in the air travel domain.
The training set consisted of 6000 templates describing flights while the test set consisted of 1946 templates describing flights.
Bad: Words are missing or extraneous words are present No Output: The system failed to produce any output While there were a total 1946 attribute sets from the test examples, the judges only needed to evaluate the 190 unique attribute sets, e.g., the attribute set { $city-fr $city-to } occurs 741 times in the test data.
(The values for N, M, and K were determined by manually evaluating the output of the 4 or 5 most common attribute sets in the training data).
The weighted results in Tables 5 and 6 account for multiple occurrences of attribute sets, whereas the unweighted results in Tables 7 and 8 count each unique attribute set once, i.e., { $city-fr Scity-to } is counted 741 times in the weighted results but once in the unweighted results.
Using the weighted results, which represent testing conditions more realistically than the unweighted results, both judges found an improvement from NLG1 to NLG2, and from NLG2 to NLG3.
NLG3 cuts the error rate from NLG1 by at least 33% (counting anything without a rank of Correct as wrong).
NLG2 cuts the error rate by at least 22% and underperforms NLG3, but requires far less annotation in its training data.
NLG1 has no chance of generating anything for 3% of the data — it fails completely on novel attribute sets.
Using the unweighted results, both judges found an improvement from NLG1 to NLG2, but, surprisingly, judge A found a slight decrease while judge B found an increase in accuracy from NLG2 to NLG3.
The unweighted results show that the baseline NLG1 does well on the common attribute sets, since it correctly generates only less than 50% of the unweighted cases but over 80% of the weighted cases.
The NLG2 and NLG3 systems automatically attempt to generalize from the knowledge inherent in the training corpus of templates, so that they can generate templates for novel attribute sets.
There Probability 0.107582 0.00822441 0.00564712 0.00343372 0.0012465 Generated Text $time-dep flights from $city-fr to $city-to $time-dep flights between $city-fr and $city-to $time-dep flights $city-fr to $city-to flights from $city-fr to $city-to at $time-dep $time-dep flights from $city-fr to to $city-to Table 9: Sample output from NLG3.
The trainable surface NLG systems in this paper differ from grammar-based systems in how they determine the attribute ordering and lexical choice.
NLG2 and NLG3 automatically determine attribute ordering by simultaneously searching multiple orderings.
NLG2 and NLG3 solve the lexical choice problem by learning the words (via features in the maximum entropy probability model) that correlate with a given attribute and local context, whereas (Elhadad et al., 1997) uses a rule-based approach to decide the word choice.
While trainable approaches avoid the expense of crafting a grammar to determine attribute ordering and lexical choice, they are less accurate than grammar-based approaches.
For short phrases, accuracy is typically 100% with grammar-based approaches since the grammar writer can either correct or add a rule to generate the phrase of interest once an error is detected.
Whereas with NLG2 and NLG3, one can tune the feature patterns, search parameters, and training data itself, but there is no guarantee that the tuning will result in 100% generation accuracy.
(Langkilde and Knight, 1998) maps from semantics to words with a concept ontology, grammar, and lexicon, and ranks the resulting word lattice with corpus-based statistics, whereas NLG2 and NLG3 automatically learn the mapping from semantics to words from a corpus.
NLG2 and NLG3 are also statistical learning approaches but generate from an actual semantic representation.
This limitation can be overcome by using features on values, so that NLG2 and NLG3 might discover — to use a hypothetical example — that &quot;flights leaving $city-fr&quot; is preferred over &quot;flights from $city-fr&quot; when $city-fr is a particular value, such as &quot;Miami&quot;.
