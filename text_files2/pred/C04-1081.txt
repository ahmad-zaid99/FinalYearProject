Conditional random fields (CRFs) are undirected graphical models trained to maximize a conditional probability (Lafferty et al, 2001).
k ?kfk(yt?1, yt,x, t) ) , (1) where Zx is the per-input normalization that makes the probability of all state sequences sum to one;fk(yt?1, yt,x, t) is a feature function which is of ten binary-valued, but can be real-valued, and ?k is a learned weight associated with feature fk.
yt, and the entire observation se quence, x, centered at the current time step, t. For example, one feature function might have value 1when yt?1 is the state START, yt is the state NOT START, and xt is a word appearing in a lexicon of people?s first names.
The parameters can be estimated by maximum likelihood?maximizing the conditional probabilityof a set of label sequences, each given their cor responding input sequences.
(y|x(i))fk(yt?1, yt,x(i), t) CRFs share many of the advantageous properties of standard maximum entropy classifiers, including their convex likelihood function, which guarantees that the learning procedure converges to the global maximum.
3.1 Lexicon features as domain knowledge.
The open features include a large word list (containing single and multiple-character words), a character list, and additional topic or part-of-speech character lexicons obtained from various sources.
Many lexicons of Chinese words and characters are available from the Internet and other sources.
Besides the word list and character list, our lexiconsinclude 24 lists of Chinese words and characters obtained from several Internet sites1 cleaned and augmented by a local native Chinese speaker indepen dently of the competition data.
The list of lexicons used in our experiments is shown in Figure 1.
Figure 1: Lexicons used in our experiments C?2: second previous character in lexicon C?1: previous character in lexicon C1: next character in lexicon C2: second next character in lexicon C0C1: current and next character in lexicon C?1C0: current and previous character in lexicon C?2C?1: previous two characters in lexicon C?1C0C1: previous, current, and next character in the lexicon Figure 2: Feature conjunctions used in experiments use feature conjunctions in both the open and closed tests, as listed Figure 2.
We consider here new word detection as an integral part of segmentation, aimingto improve both segmentation and new word detec tion: detected new words are added to the word list lexicon in order to improve segmentation; improved segmentation can potentially further improve new word detection.
is identified as a new word and added to the word list lexicon.
Since CTB and PK are provided in the GB encod ing while AS and HK use the Big5 encoding, we convert AS and HK datasets to GB in order to make cross-training-and-testing possible.
Thus, each training set is randomly split?80% used for training and theremaining 20% for validation?and based on vali dation set performance, choices are made for model structure, prior, and which word lexicons to include.
The closed tests use only material from the training data for the particular corpus being tested.Open tests allows using other material, such as lexicons from Internet.
Encoding #Train words #Test Words OOV rate (%) UPenn Chinese Treebank CTB GB 250K 40K 18.1 Beijing University PK GB 1.1M 17K 6.9 Hong Kong City U HK Big 5 240K 35K 7.1 Academia Sinica AS Big 5 5.8M 12K 2.2 Table 1: Datasets statistics bin-Size M Markov order CTB 10 first-order + transitions PK 15 first-order + transitions HK 1 first-order AS 15 first-order + transitions Table 2: Optimal prior and Markov order setting in Section 3.1.
Closed Precision Recall F1 Roov CTB 0.828 0.870 0.849 0.550 PK 0.935 0.947 0.941 0.660 HK 0.917 0.940 0.928 0.531 AS 0.950 0.962 0.956 0.292 Open Precision Recall F1 Roov CTB 0.889 0.898 0.894 0.619 PK 0.941 0.952 0.946 0.676 HK 0.944 0.948 0.946 0.629 AS 0.953 0.961 0.957 0.403 Table 3: Overall results of CRF segmentation on closed and open tests To compare our results against other systems, we summarize the competition results reported in (Sproat and Emerson, 2003) in Table 4.
Column OUR-AVG is the average F1 performance of our system over the same datasets.Comparing performance across systems is diffi cult since none of those systems reported results on all eight datasets (open and closed runs on 4 datasets).
Only one site (S01)achieved two best runs (CTBc and PKc) with an av erage of 91.8% over 6 runs.
We also achieve two best runs (ASo and HKc), with a comparable average of 91.9% over the same 6 runs, and a 92.7% average over all the 8 runs.Second, performance varies significantly across dif ferent datasets, indicating that the four datasets havedifferent characteristics and use very different seg mentation guidelines.
We randomly split the training data into 80% training and 20%testing, and run the experiments for 3 times, result ing in a testing F1 of 97.13%.
They participated in two datasets, with an average of 93.8%.
Our average over the same two runs is 94.2%.
Not surprisingly, cross testing re sults are worse than the results using the same ASc ASo CTBc CTBo HKc HKo PKc PKo SITE-AVG OUR-AVG S01 93.8 88.1 88.1 90.1 95.1 95.3 91.8 91.9 S02 87.4 91.2 89.3 87.2 S03 87.2 82.9 88.6 92.5 87.8 93.6 S04 93.9 93.7 93.8 94.4 S05 94.2 73.2 89.4 85.6 91.5 S06 94.5 82.9 92.4 92.4 90.6 91.9 S07 94.0 94.0 94.6 S08 90.4 95.6 93.6 93.8 93.4 94.0 S09 96.1 94.6 95.4 94.9 S10 83.1 90.1 94.7 95.9 91.0 90.8 S11 90.4 88.4 87.9 88.6 88.8 93.6 S12 95.9 91.6 93.8 94.2 95.6 95.7 84.9 89.4 92.8 94.6 94.1 94.6 92.7 Table 4: Comparisons against other systems: the first column contains the 12 sites participating in bake-off competition; the second to the ninth columns contain their results on the 8 runs, where a bold entry is the winner of that run; column SITE-AVG contains the average performance of the site over the runs in which it participated, where a bold entry indicates that this site performs better than our system; column OUR-AVG is the average of our system over the same runs, where a bolded entry indicates our system performs better than the other site; the last row is the performance of our system over all the runs and the overall average.
From both Table 3 and 5, we see, as expected,improvement from closed tests to open tests, indicating the significant contribution of domain knowl edge lexicons.
Closed CTB PK HK AS CTB 0.822 0.810 0.815 PK 0.816 0.824 0.830 HK 0.790 0.807 0.825 AS 0.890 0.844 0.864 Open CTB PK HK AS CTB 0.863 0.870 0.894 PK 0.852 0.862 0.871 HK 0.861 0.871 0.889 AS 0.898 0.867 0.871 Table 5: Crossing test of CRF segmentation 5.3 Effects of new word detection.
An interesting observation is CTB PK HK AS w/o NWD 0.792 0.934 0.916 0.956 NWD 0.849 0.941 0.928 0.946 Table 6: New word detection effects: w/o NWD is the results without new word detection and NWD is the results with new word detection.
However, obtaining these lexicons is not a trivial matter.
The quality of lexicons can affect the performance of CRFs significantly.
