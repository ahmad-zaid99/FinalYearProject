Predicate subcategorization is a key component of a lexical entry, because most, if not all, recent syntactic theories 'project' syntactic structure from the lexicon.
Therefore, a wide-coverage parser utilizing such a lexicalist grammar must have access to an accurate and comprehensive dictionary encoding (at a minimum) the number and category of a predicate's arguments and ideally also information about control with predicative arguments, semantic selection preferences on arguments, and so forth, to allow the recovery of the correct predicate-argument structure.
Unfortunately, neither approach can yield a genuinely accurate or comprehensive computational lexicon, because both rest ultimately on the manual efforts of lexicographers / linguists and are, therefore, prone to errors of omission and commission which are hard or impossible to detect automatically (e.g.
Furthermore, manual encoding is labour intensive and, therefore, it is costly to extend it to neologisms, information not currently encoded (such as relative frequency of different subcategorizations), or other (sub)languages.
These problems are compounded by the fact that predicate subcategorization is closely associated to lexical sense and the senses of a word change between corpora, sublanguages and/or subject domains (Jensen, 1991).
In a recent experiment with a wide-coverage parsing system utilizing a lexicalist grammatical framework, Briscoe & Carroll (1993) observed that half of parse failures on unseen test data were caused by inaccurate subcategorization information in the ANLT dictionary.
Moreover, although Schabes (1992) and others have proposed `lexicalized' probabilistic grammars to improve the accuracy of parse ranking, no wide-coverage parser has yet been constructed incorporating probabilities of different subcategorizations for individual predicates, because of the problems of accurately estimating them.
We achieve this performance by exploiting a more sophisticated robust statistical parser which yields complete though 'shallow' parses, a more comprehensive subcategorization class classifier, and a priori estimates of the probability of membership of these classes.
The tagger, lemmatizer, grammar and parser have been described elsewhere (see previous references), so we provide only brief relevant details here, concentrating on the description of the components of the system that are new: the extractor, classifier and evaluator.
arguments are sisters within X1 projections; X1 XO Argl... ArgN).
There are 29 distinct values for VSUBCAT and 10 for PSUBCAT; these are analysed in patterns along with specific closed-class head lemmas of arguments, such as it (dummy subjects), whether (wh-complements), and so forth, to classify patterns as evidence for one of the 160 subcategorization classes.
Currently, the coverage of this grammar-the proportion of sentences for which at least one analysis is found-is 79% when applied to the Susanne corpus (Sampson, 1995), a 138K word treebanked and balanced subset of the Brown corpus.
The combined throughput of the parsing components on a Sun UltraSparc 1/140 is around 50 words per CPU second.
The subcategorization classes recognized by the classifier were obtained by manually merging the classes exemplified in the COMLEX Syntax and ANLT dictionaries and adding around 30 classes found by manual inspection of unclassifiable patterns for corpus examples during development of the system.
The classifier filters out as unclassifiable around 15% of patterns found by the extractor when run on all the patternsets extracted from the Susanne corpus.
This demonstrates the value of the classifier as a filter of spurious analyses, as well as providing both translation between extracted patterns and two existing subcategorization dictionaries and a definition of the target subcategorization dictionary.
Evaluating putative entries on binomial frequency data requires that we record the total number of patternsets n for a given predicate, and the number of these patternsets containing a pattern supporting an entry for given class m. These figures are straightforwardly computed from the output of the classifier; however, we also require an estimate of the probability that a pattern for class i will occur with a verb which is not a member of subcategorization class i. Brent proposes estimating these probabilities experimentally on the basis of the behaviour of the extractor.
We estimate this probability more directly by first extracting the number of verbs which are members of each class in the ANLT dictionary (with intuitive estimates for the membership of the novel classes) and converting this to a probability of class membership by dividing by the total number of verbs in the dictionary; and secondly, by multiplying the complement of these probabilities by the probability of a pattern for class i, defined as the number of patterns for i extracted from the Susanne corpus divided by the total number of patterns.
So, p(v -i), the probability of verb v not of class i occurring with a pattern for class i is: The probability of the event happening m or more times is: Thus P(m,n,p(v -i)) is the probability that m or more occurrences of patterns for i will occur with a verb which is not a member of i, given n occurrences of that verb.
Setting a threshold of less than or equal to 0.05 yields a 95% or better confidence that a high enough proportion of patterns for i have been observed for the verb to be in class i3.
Our approach to acquiring subcategorization classes is predicated on the following assumptions: probabilistic cue for membership of i, so membership should only be inferred if there are enough occurrences of patterns for i in the data to outweigh the error probability for i.
However, recognizing same/similar arguments requires considerable quantities of lexical data or the ability to back-off to lexical semantic classes.
At the moment, we exploit linguistic information about the syntactic type, obligatoriness and position of arguments, as well as the set of possible subcategorization classes, and combine this with statistical inference based on the probability of class membership and the frequency and reliability of patterns for classes.
In order to test the accuracy of our system (as developed so far) and to provide empirical feedback for further development, we took the Susanne, SEC (Taylor & Knowles, 1988) and LOB corpora (Garside et at., 1987)-a total of 1.2 million words-and extracted all sentences containing an occurrence of one of fourteen verbs, up to a maximum of 1000 citations of each.
The sentences containing these verbs were tagged and parsed automatically, and the extractor, classifier and evaluator were applied to the resulting The binomial distribution gives the probability of an event with probability p happening exactly m times out of n attempts: successful analyses.
One ANLT entry covers two COMLEX entries given the different treatment of the relevant complements but the classifier keeps them distinct.
This comparison illustrates the problem of errors of omission common to computational lexicons constructed manually and also from machine-readable dictionaries.
Figure 3 gives the type precision and recall of our system's recognition of subcategorization classes as evaluated against the merged dictionary entries (14 verbs) and against the manually analysed corpus data (7 verbs).
The frequency distribution of the classes is highly skewed: for example for believe, there are 107 instances of the most common class in the corpus data, but only 6 instances in total of the least common four classes.
More generally, for the manually analysed verbs, almost 60% of the false negatives have only one or two exemplars each in the corpus citations.
For each of the seven verbs for which we undertook a corpus analysis, we calculate the token recall of our system as the percentage (over all exemplars) of true positives in the corpus.
There are only 13 true negatives which the system failed to propose, each exemplified in the data by a mean of 4.5 examples.
On the other hand, there are 67 false negatives supported by an estimated mean of 7.1 examples which should, ideally, have been accepted by the filter, and 11 false positives which should have been rejected.
The performance of the filter for classes with less than 10 exemplars is around chance, and a simple heuristic of accepting all classes with more than 10 exemplars would have produced broadly similar results for these verbs.
The filter may well be performing poorly because the probability of generating a subcategorization class for a given verb is often lower than the error probability for that class.
In addition to evaluating the acquired subcategorization information against existing lexical resources, we have also evaluated the information in the context of an actual parsing system.
We randomly selected a test set of 250 in-coverage sentences (of lengths 3-56 tokens, mean 18.2) from the Susanne treebank, retagged with possibly multiple tags per word, and measured the 'baseline' accuracy of the unlexicalized parser on the sentences using the now standard PARSEVAL/GEIG evaluation metrics of mean crossing brackets per sentence and (unlabelled) bracket recall and precision (e.g.
Next, we collected all words in the test corpus tagged as possibly being verbs (giving a total of 356 distinct lemmas) and retrieved all citations of them in the LOB corpus, plus Susanne with the 250 test sentences excluded.
The GEIG measures for the lexicalized parser show a 7% improvement in the crossing bracket score (figure 6).
Over the existing test corpus this is not statistically significant at the 95% level (paired t-test, 1.21, 249 df, p = 0.11)-although if the pattern of differences were maintained over a larger test set of 470 sentences it would be significant.
Nevertheless, this experiment demonstrates that lexicalizing a grammar/parser with subcategorization frequencies can appreciably improve the accuracy of parse ranking.
