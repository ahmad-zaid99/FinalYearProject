Parsing using CCG can be viewed as a two-stage process: first assign lexical categories to the wordsin the sentence, and then combine the categories to gether using CCG?s combinatory rules.1 The first stage can be accomplished by simply assigning to each word all categories from the word?s entry in the lexicon (Hockenmaier, 2003).
The WSJ is a publication that I enjoy reading NP/N N (S[dcl]\NP)/NP NP/N N (NP\NP)/(S[dcl]/NP) NP (S[dcl]\NP)/(S[ng]\NP) (S[ng]\NP)/NP Figure 1: Example sentence with CCG lexical categories frequency # cat types # cat tokens in # sentences in 2-21 # cat tokens in # sentences in 00 cut-off 2-21 not in cat set with missing cat 00 not in cat set with missing cat 1 1 225 0 0 12 (0.03%) 12 (0.6%) 10 409 1 933 (0.2%) 1 712 (4.3%) 79 (0.2%) 69 (3.6%) Table 1: Statistics for the lexical category setAn alternative is to use a statistical tagging approach to assign one or more categories.
The next section describes the set of lexical categories used by our supertagger and parser.
2.1 The Lexical Category Set.
The set of lexical categories is obtained from CCGbank (Hockenmaier and Steedman, 2002; Hockenmaier, 2003), a corpus of CCG normal-form deriva tions derived semi-automatically from the PennTreebank.
Figure 1 gives an example sentence su pertagged with the correct CCG lexical categories.
Table 1 gives the number of different category types and shows the coverage on training (seen) anddevelopment (unseen) data (section 00 from CCGbank).
The table also gives statistics for the com plete set containing every lexical category type inCCGbank.2 These figures show that using a fre quency cutoff can significantly reduce the size of the category set with only a small loss in coverage.
The grammars of Chen and Vijay Shanker (2000) contain between 2,000 and 9,000 tree frames, depending on the parameters used inthe extraction process, significantly more elemen tary structures than the number of lexical categories derived from CCGbank.
The supertagger uses probabilities p(y|x) where y is a lexical category and x is a context.
Theper-word accuracy is between 91 and 92% on un seen data in CCGbank; however, Clark (2002) shows this is not high enough for integration into a parser since the large number of incorrect categories results in a significant loss in coverage.
CATS/ ACC SENT ACC SENT WORD ACC (POS) ACC 0.1 1.4 97.0 62.6 96.4 57.4 0.075 1.5 97.4 65.9 96.8 60.6 0.05 1.7 97.8 70.2 97.3 64.4 0.01 2.9 98.5 78.4 98.2 74.2 0.01k=100 3.5 98.9 83.6 98.6 78.9 0 21.9 99.1 84.8 99.0 83.0 Table 2: Supertagger accuracy on section 00 egories whose probability according to (1) is within some factor, ?, of the highest probability category for the word.
A value of k = 20was used in this work, and sections 2-21 of CCG bank were used as training data.Table 2 gives the per-word accuracy (acc) on sec tion 00 for various values of ?, together with the average number of categories per word.
= 0.01k=100 correspond to a value of 100 for thetag dictionary parameter k. The set of categories as signed to a word is considered correct if it contains the correct category.
To give one example, the number of categories in the tag dictionary?s entry for the wordis is 45 (only considering categories which have appeared at least 10 times in the training data).
It takes POS tagged sentences as input with each word assigned a set of lexical categories.
Featuresare defined in terms of the local trees in the derivation, including lexical head information and word word dependencies.
The dependency relations are de fined in terms of the argument slots of CCG lexical categories.
The objective function is optimised using L-BFGS (Nocedal and Wright, 1999), an iterative algorithm from the numerical optimisation lit erature.The algorithm requires the gradient of the objective function, and the value of the objective function, at each iteration.
Since the training data contains the correct lexicalcategories, we ensure the correct category is as signed to each word when generating the packed charts for model estimation.
0.1 17 GB 31 GB CCGbank constraints 13 GB 23 GB Eisner constraints 9 GB 16 GB ?
0.1 2 GB 4 GB Table 3: Space requirements for model training dataprevent any constituent which is the result of a forward (backward) composition serving as the primary functor in another forward (backward) composition or a forward (backward) application.
The training sen tences are parsed using a number of nodes from a 64-node Beowulf cluster.3 The time taken to parse the training sentences depends on the supertagging and parsing constraints, and the number of nodes used, but is typically around 30 minutes.
Packed charts are created for approximately 94% of the sentences in sec tions 2-21 of CCGbank.
The coverage is not 100% because, for some sentences, the parser cannot pro vide an analysis, and some charts exceed the node limit even at the ?
The results show that the memory and disk usage are reduced by approx imately 25% using these constraints.
The combination of the two types of normal form constraints reduces the memory requirements by 48% over the original approach.
However, the memory requirement for training the model is now only 4 GB, a reduction of 87% compared with the original approach.
= 0.01?0.1 3 523 0.7 16 CCGbank constraints 1 181 2.0 46 Eisner constraints 995 2.4 55 ?
= 0.1?0.01k=100 608 3.9 90 CCGbank constraints 124 19.4 440 Eisner constraints 100 24.0 546 Parser beam 67 35.8 814 94% coverage 49 49.0 1 114 Parser beam 46 52.2 1 186 Oracle 18 133.4 3 031 Table 4: Parse times for section 23 The problem with this approach is that, for some sentences, the number of categories in the chart still gets extremely large and so parsing is unacceptably slow.
All experiments were run on a 2.8 GHZ Intel Xeon P4 with 2 GB RAM.
Table 4 gives parse times for the 2,401 sentences in section 23 of CCGbank.
CATS/ 0.1 FIRST 0.01 FIRST WORD PARSES % PARSES % 0.1 1.4 1689 88.4 0 0.0 0.075 1.5 43 2.3 7 0.4 0.05 1.7 51 2.7 39 2.0 0.01 2.9 79 4.1 1816 95.1 0.01k=100 3.5 33 1.7 33 1.7 NO SPAN 15 0.8 15 0.8 Table 5: Supertagger ?
For all of the figures re ported on section 23, unless stated otherwise, the parser is able to provide an analysis for 98.5% of the sentences.
The table shows that the normal-form constraints have a significant impact on the speed, reducing theparse times for the old strategy by 72%, and reduc ing the times for the new strategy by 84%.
The new strategy also has a spectacular impact on the speed compared with the old strategy, reducing the times by 83% without the normal-form constraints and 90% with the constraints.
The 94% coverage row corresponds to using only the first two supertagging levels; the parser ignores the sentence if it cannot get an analysis at the ?
The percentage of sentences without an analysis is now 6%, but the parser is extremely fast,processing almost 50 sentences a second.
This configuration of the system would be useful for obtaining data for lexical knowledge acquisition, for ex ample, for which large amounts of data are required.
The oracle row shows the parser speed when it is provided with only the correct lexical categories.The parser is extremely fast, and in Clark and Cur ran (2004) we show that the F-score for labelled dependencies is almost 98%.
This demonstratesthe large amount of information in the lexical categories, and the potential for improving parser ac curacy and efficiency by improving the supertagger.
that we use here reduces the accu racy of the parser on section 00 by a small amount (0.3% labelled F-score), but has a significant impacton parser speed, reducing the parse times by a fur ther 33%.
The new strategy increases the F-score over labelled de pendencies by approximately 0.5%, leading to the figures reported in Clark and Curran (2004).
did find that LTAG supertagging increased pars ing speed, but at a significant cost in coverage: only 1,324 sentences out of a test set of 2,250 received a parse.
The parse times reported are also not as good as those reported here: the time taken to parse the 2,250 test sentences was over 5 hours.5Multiplying by an estimate of the outside score may im prove the efficacy of the beam.
Kaplan et al (2004) report high parsing speedsfor a deep parsing system which uses an LFG gram mar: 1.9 sentences per second for 560 sentencesfrom section 23 of the Penn Treebank.
