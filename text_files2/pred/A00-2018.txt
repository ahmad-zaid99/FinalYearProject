The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for &quot;tag&quot;), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c).
Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g., whether it is a noun phrase (np), verb-phrase, etc.)
and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.
In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.
M(c) is the constituent from which the head lexical item h is obtained according to deterministic rules that pick the head of a constituent from among the heads of its children.
Thus an expansion e(c) looks like: The expansion is generated by guessing first M, then in order L1 through L,„.+1 (= A), and similarly for RI through In a pure Markov PCFG we are given the left-hand side label 1 and then probabilistically generate the right-hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right-hand side itself.
So, for example, in a second-order Markov PCFG, L2 would be conditioned on L1 and M. In our complete model, of course, the probability of each label in the expansions is also conditioned on other material as specified in Equation 1, e.g., p(e t, h, H).
Note that the As on both ends of the expansion in Expression 2 are conditioned just like any other label in the expansion.
For example, in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1, M,1, t, h, H).
Also, remember that H is a placeholder for any other information beyond the constituent c that may be useful in assigning c a probability.
To compute a probability in a log-linear model one first defines a set of &quot;features&quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.
In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.
For example, in computing the probability of the head's pre-terminal t we might want a feature schema f (t, 1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1, and zero otherwise.
Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability, the higher the absolute value of the associated A.
The intuitive idea is that each factor gi is larger than one if the feature in question makes the probability more likely, one if the feature has no effect, and smaller than one if it makes the probability less likely.
First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable — just change the set of features used.
That is, suppose we want to compute a conditional probability p(a b, c), but we are not sure that we have enough examples of the conditioning event b, c in the training corpus to ensure that the empirically obtained probability P (a I b, c) is accurate.
In max-entropy models one can simply include features for all three events f1 (a, b, c), f2 (a, b), and f3(a, c) and combine them in the model according to Equation 3, or equivalently, Equation 4.
Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.
Consider the probability distribution for choosing the pre-terminal for the head of a constituent.
For example, one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling, or the grandparent label.
With no prior knowledge of values for the Ai one traditionally starts with Ai = 0, this being a neutral assumption that the feature has neither a positive nor negative impact on the probability in question.
With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.
(Our experience is that rather than requiring 50 or so iterations, three suffice.)
We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.
As noted above, the probability model uses five smoothed probability distributions, one each for Li, M,Ri,t, and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7.
In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning).
As noted in [5], these definitions typically give results about 0.4% higher than the more obvious ones.
Looking in particular at the precision and recall figures, the new parser's give us a 13% error reduction over the best of the previous work, Co1199 [9].
We take as our starting point the parser labled Char97 in Figure 1 [5], as that is the program from which our current parser derives.
(It is &quot;soft&quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)
Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.
For example, the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.
This is indicated in Figure 2, where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.
This is in accord with our experience that developmentcorpus results are from 0.3% to 0.5% lower than those obtained on the test corpus.
It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments), it does not guess the pre-terminal before guessing the lexical head, and it uses a tree-bank grammar rather than a Markov grammar.
This parser achieves an average precision/recall of 86.2%.
This is consistent with the average precision/recall of 86.6% for [5] mentioned above, as the latter was on the test corpus and the former on the development corpus.
As already noted, Char97 first guesses the lexical head of a constituent and then, given the head, guesses the PCFG rule used to expand the constituent in question.
In Figure 2 we show that this one factor improves performance by nearly 2%.
(For example, part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].)
This quantity is a relatively intuitive one (as, for example, it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it, in effect, as the unsmoothed probability upon which all smoothing of p(h) is based.
The second major reason why first guessing the pre-terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion.
For example, when we first guess the lexical head we can move from computing p(r I 1,1p, h) to p(r I /, t, /p, h).
So, e.g., even if the word &quot;conflating&quot; does not appear in the training corpus (and it does not), the &quot;ng&quot; ending allows our program to guess with relative security that the word has the vbg pre-terminal, and thus the probability of various rule expansions can be considerable sharpened.
For example, the tree-bank PCFG probability of the rule &quot;VP --+ vbg np&quot; is 0.0145, whereas once we condition on the fact that the lexical head is a vbg we get a probability of 0.214.
As shown in Figure 2, conditioning on this information gives a 0.6% improvement.
Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.
When we do so using our maximum-entropy-inspired conditioning, we get another 0.45% improvement in average precision/recall, as indicated in Figure 2 on the line labeled &quot;MaxEnt-Inspired'.
Including this information within a standard deleted-interpolation model causes a 0.6% decrease from the results using the less conventional model.
