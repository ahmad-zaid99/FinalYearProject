Q: Which country gave New York the Statue of Liberty?However, confined by the huge amount of man ual work needed for constructing a classifier for a complicated taxonomy of questions, most questionanswering systems can only perform a coarse clas sification for no more than 20 classes.
The approach we adopted is a multi-level learning approach: some of our fea tures rely on finer analysis of the questions that are outcomes of learned classifiers; the QC module then applies learning with these as input features.
The hierarchy con tains 6 coarse classes (ABBREVIATION, ENTITY,DESCRIPTION, HUMAN, LOCATION and NU MERIC VALUE) and 50 fine classes, Table 1 showsthe distribution of these classes in the 500 ques tions of TREC 10.
We also hoped that a hierarchi cal classifier would have a performance advantage over a multi-class classifier; this point, however is not fully supported by our experiments.
9 description 7 abb 1 manner 2 exp 8 reason 6 ENTITY 94 HUMAN 65 animal 16 group 6 body 2 individual 55 color 10 title 1 creative 0 description 3 currency 6 LOCATION 81 dis.med.
2 city 18 event 2 country 3 food 4 mountain 3 instrument 1 other 50 lang 2 state 7 letter 0 NUMERIC 113 other 12 code 0 plant 5 count 9 product 4 date 47 religion 0 distance 16 sport 1 money 3 substance 15 order 0 symbol 0 other 12 technique 1 period 8 term 7 percent 3 vehicle 4 speed 6 word 0 temp 5 DESCRIPTION 138 size 0 definition 123 weight 4 Table 1: The distribution of 500 TREC 10 questions over the question hierarchy.
Question 1 could belong to definition or dis ease medicine; Question 2 could belong to food,plant or animal; And Question 3 could be a numeric value or a definition.
To avoid this problem,we allow our classifiers to assign multiple class la bels for a single question.
3 Learning a Question Classifier.
More over, mapping questions into fine classes requiresthe use of lexical items (specific words) and there fore an explicit representation of the mapping may be very large.
It is hard to imagine writing explicitly a classifier that depends on thousands or more features.
Finally, a learned classifier is more flexible to reconstruct than a man ual one because it can be trained on a new taxonomy in a very short time.One way to exhibit the difficulty in manually con structing a classifier is to consider reformulations of a question: What tourist attractions are there in Reims?
3.1 A Hierarchical ClassifierQuestion classification is a multi-class classification.
Our learned classifier is based on the SNoW learning architecture (Carlson et al, 1999; Roth, 1998)2 where, in order to allow the classifier to output more than one class label, wemap the classifier?s output activation into a condi tional probability of the class labels and threshold it.
The question classifier makes use of a sequence of two simple classifiers (Even-Zohar and Roth, 2001), each utilizing the Winnow algorithm within SNoW.
The first classifies questions into coarse classes (Coarse Classifier) and the second into fineclasses (Fine Classifier).
The second classifier depends on the first in2Freely available at http://L2R.cs.uiuc.edu/cogcomp/cc software.html ABBR, ENTITY,DESC,HUMAN,LOC,NUM ABBR, ENTITY ENTITY, HUMAN ENTITY, LOC,NUM DESC Coarse Classifier Fine Classifier abb,exp ind, plant date abb, animal, food, plant?
Map coarse classes to fine classes C0 C1 C2 C3 abb,def animal,food all possible subsets of C0 wih size = 5 all possible subsets of C2 with size =5 Figure 1: The hierarchical classifier that its candidate labels are generated by expanding the set of retained coarse classes from the first into a set of fine classes; this set is then treated as the confusion set for the second classifier.Figure 1 shows the basic structure of the hierar chical classifier.
The initial confusion set of any question is C 0 = fc 1 ; c 2 ; : : : ; c n g, the set of all the coarse classes.
The coarse classifier determines a set of preferred labels, C 1 = Coarse Classifier(C 0 ), C 1  C 0 so that jC 1 j  5.
That is, sup pose the coarse class c i is mapped into the set c i = ff i1 ; f i2 ; : : : ; f im g of fine classes, then C 2 = S c i 2C 1 c i .
The fine classifier determines a set of.
preferred labels, C 3 = Fine Classifier(C 2 ) so that C 3  C 2 and jC 3 j  5.
C 1 and C 3are the ul timate outputs from the whole classifier which are used in our evaluation.
Chunks are extracted using a previously learned classifier (Punyakanok and Roth, 2001; Li and Roth, 2001).
The named entity classifier isalso learned and makes use of the same technol ogy developed for the chunker (Roth et al, 2002).The ?related word?
That is, Feature set 1 (denoted by Word) contains word features; Feature set 2 (Pos) contains featurescomposed of words and pos tags and so on; The fi nal feature set, Feature set 6 (RelWord) contains all the feature types and is the only one that containsthe related words lists.
The classifiers will be experimented with different feature sets to test the influ ence of different features.
For both the coarse and fine classifiers, the same decision model is used to choose class labels for a question.
After ranking the classes in the decreasing order of density values, we have the possible class labels C = fc 1 ; c 2 ; : : : ; c n g, with their densities P = fp 1 ; p 2 ; : : : ; p n g (where, P n 1 p i = 1, 0  p i 1, 1  i  n).
As discussed earlier, for each question we output the first k classes (1  k  5), c 1 ; c 2 ; : : : c kwhere k satis fies, k = min(argmin t ( t X 1 p i  T ); 5) (1) T is a threshold value in [0,1].
If we treat pi as the probability that a question belongs to Class i, the decision model yields a reasonable probabilistic interpretation.
We designed two experiments to test the accuracy of our classifier on TREC questions.
Our hi erarchical classifier is trained and tested using oneof the six feature sets defined in Sect.
We construct a multi-class classifier only for fine classes.
This flat classifier takes all fine classes as its initial confusion set and classifies a questioninto fine classes directly.
By comparing this flat classifier with our hi erarchical classifier in classifying fine classes, we hope to know whether the hierarchical classifier hasany advantage in performance, in addition to the ad vantages it might have in downstream processing and comprehensibility.
Data are collected from four sources: 4,500 English questions published by USC (Hovy et al, 2001), about 500 manually constructed questions for a few rare classes, 894 TREC 8 and TREC 9 questions, and also 500 questions from TREC 10 which serves as our test set3.These questions were manually labeled accord ing to our question hierarchy.
Although we allow multiple labels for one question in our classifiers, in our labeling, for simplicity, we assigned exactly 3The annotated data and experimental results are available from http://L2R.cs.uiuc.edu/cogcomp/one label to each question.
In training, we divide the 5,500 questions from the first three sources randomly into 5 training sets of 1,000, 2,000, 3,000, 4,000 and 5,500 questions.
All 500 TREC 10 questions are used as the test set.
In this paper, we count the number of correctly clas sified questions by two different precision standards P 1 and P 5 .
We define I ij = f 1; if the correct label of the ith question is output in rank j; 0; otherwise: (2) Then, P 1 = P m i=1 I i1 =m and P 5 = P m i=1 P k i j=1 I ij =m where m is the total number of test examples.
P 5reflects the accuracy of our classifier with respect to later stages in a question answering sys tem.
As the results below show, although questionclasses are still ambiguous, few mistakes are intro duced by our classifier in this step.
Performance of the hierarchical classifier Table 2 shows the P 5precision of the hierarchi cal classifier when trained on 5,500 examples andtested on the 500 TREC 10 questions.
Overall, we get a98.80% precision for coarse classes with all the fea tures and 95% for the fine classes.
P =5 Word Pos Chunk NE Head RelWord Coarse 92.00 96.60 97.00 97.00 97.80 98.80 Fine 86.00 86.60 87.60 88.60 89.40 95.00Table 2: Classification results of the hierarchical clas sifier on 500 TREC 10 questions.
Train Test P 1 P =5 1 1000 500 83.80 95.60 2 2000 500 84.80 96.40 3 3000 500 91.00 98.00 4 4000 500 90.80 98.00 Table 3: Classification accuracy for coarse classes ondifferent training sets using the feature set RelWord.
Re sults are evaluated using P 1 and P 5 .
Train Test P 1 P =5 1 1000 500 71.00 83.80 2 2000 500 77.80 88.20 3 3000 500 79.80 90.60 4 4000 500 80.00 91.20Table 4: Classification accuracy for fine classes on different training sets using the feature set RelWord.
Re sults are evaluated using P 1 and P 5 .
Tables 3 and 4 show the P 1 and P 5 accuracyof the hierarchical classifier on training sets of dif ferent sizes and exhibit the learning curve for this problem.We note that the average numbers of labels out put by the coarse and fine classifiers are 1.54 and 2.05 resp., (using the feature set RelWord and 5,500 training examples), which shows the decision model is accurate as well as efficient.
Comparison of the hierarchical and the flat classifier The flat classifier consists of one classifier which isalmost the same as the fine classifier in the hierar chical case, except that its initial confusion set is the whole set of fine classes.
Our original hope was that the hierarchical classifier would have a better performance, given that its fine classifier only needs to deal with a smaller confusion set.
We define the tendency of Class i to be confused with Class j as follows: D ij = Err ij  2=(N i + N j ); (3) where (when using P 1 ), Err ij is the number ofquestions in Class i that are misclassified as belong P 1 Word Pos Chunk NE Head RelWord h 77.60 78.20 77.40 78.80 78.80 84.20 f 52.40 77.20 77.00 78.40 76.80 84.00 P =5 Word Pos Chunk NE Head RelWord h 86.00 86.60 87.60 88.60 89.40 95.00 f 83.20 86.80 86.60 88.40 89.80 95.60 Table 5: Comparing accuracy of the hierarchical (h) and flat (f) classifiers on 500 TREC 10 question; training is done on 5,500 questions.
Results are shown for different feature sets using P 1 and P 5 .
Fine Classes 1?50 Fi ne C la ss es 1 ?5 0 2 24 28 32 37 50 2 24 28 32 37 50 Figure 2: The gray?scale map of the matrix D[n,n].
Nevertheless, it is constructive to consider some cases in which the classifier fails.Below are some examples misclassified by the hier archical classifier.What French ruler was defeated at the battle of Water loo?
The correct label is individual, but the classifier, failing to relate the word ?ruler?
The correct label is speed, but the classifier outputs animal.
The classifier returns other entities instead ofequivalent term.
The ambiguity causes the classifier not to output equivalent term as the first choice.
