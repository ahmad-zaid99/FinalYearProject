Diathesis alternations have been proposed for a number of NLP tasks.
Several researchers have suggested using them for improving lexical acquisition.
Korhonen (1997) uses them in subcategorization frame (scF) acquisition to improve the performance of a statistical filter which determines whether a SCF observed for a particular verb is genuine or not.
They have also been suggested for the recovery of predicate argument structure, necessary for SCF acquisition (Briscoe and Carroll, 1997; Boguraev and Briscoe, 1987).
And Ribas (1995) showed that selectional preferences acquired using alternations performed better on a word sense disambiguation task compared to preferences acquired without alternations.
He used alternations to indicate where the argument head data from different slots can be combined since it occupies the same semantic relationship with the predicate.
Different diathesis alternations give different emphasis and nuances of meaning to the same basic content.
These subtle changes of meaning are important in natural language generation (Stede, 1998).
Alternations provide a means of reducing redundancy in the lexicon since the alternating scFs need not be enumerated for each individual verb if a marker is used to specify which verbs the alternation applies to.
Alternations also provide a means of generalizing patterns of behaviour over groups of verbs, typically the group members are semantically related.
Levin (1993) provides a classification of over 3000 verbs according to their participation in alternations involving NP and PP constituents.
Levin's classification is not intended to be exhaustive.
Automatic identification of alternations would be a useful tool for extending the classification with new participants.
Levin's taxonomy might also be used alongside observed behaviour, to predict unseen behaviour.
Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., 1998).
Dang et al. (1998) modify it by adding new classes which remove the overlap between classes from the original scheme.
Dorr and Jones (1996) extend the classification by using grammatical information in LDOCE alongside semantic information in WordNet.
What is missing is a way of classifying verbs when the relevant information is not available in a manmade resource.
Using corpora by-passes reliance on the availability and adequacy of mRDs.
Additionally, the frequency information in corpora is helpful for estimating alternation productivity (Lapata, 1999).
Estimations of productivity have been suggested for controlling the application of alternations (Briscoe and Copestake, 1996).
We propose a method to acquire knowledge of alternation participation directly from corpora, with frequency information available as a by-product.
We use both syntactic and semantic information for identifying participants in RsAs.
Firstly, syntactic processing is used to find candidates taking the alternating SCFS.
Secondly, selectional preference models are acquired for the argument heads associated with a specific slot in a specific SCF of a verb.
We use the SCF acquisition system of Briscoe and Carroll (1997), with a probabilistic LR parser (Inui et al., 1997) for syntactic processing.
The corpus data is POS tagged and lemmatised before the LR parser is applied.
Subcategorization patterns are extracted from the parses, these include both the syntactic categories and the argument heads of the constituents.
These subcategorization patterns are then classified according to a set of 161 SCF classes.
The SCF entries for each verb are then subjected to a statistical filter which removes scFs that have occurred with a frequency less than would be expected by chance.
The resulting SCF lexicon lists each verb with the scFs it takes.
Each SCF entry includes a frequency count and lists the argument heads at all slots.
Selectional preferences are automatically acquired for the slots involved in the role switching.
We refer to these as the target slots.
For the causative alternation, the slots are the direct object slot of the transitive SCF and the subject slot of the intransitive.
For the conative, the slots are the direct object of the transitive and the PP of the up v pp SCF.
Selectional preferences are acquired using the method devised by Li and Abe (1995).
The preferences for a slot are represented as a tree cut model (Tcm).
This is a set of disjoint classes that partition the leaves of the WordNet noun hypernym hierarchy.
A conditional probability is attached to each of the classes in the set.
To ensure the TCM covers all the word senses in WordNet, we modify Li and Abe's original scheme by creating hyponym leaf classes below all WordNet's hypernym (internal) classes.
Each leaf holds the word senses previously held at the internal class.
The nominal argument heads from a target slot are collected and used to populate the WordNet hierarchy with frequency information.
The head lemmas are matched to the classes which contain them as synonyms.
Where a lemma appears as a synonym in more than one class, its frequency count is divided between all classes for which it has direct membership.
The frequency counts from hyponym classes are added to the count for each hypernym class.
A root node, created above all the WordNet roots, contains the total frequency count for all the argument head lemmas found within WordNet.
The minimum description length principle (MDL) (Rissanen, 1978) is used to find the best TCM by considering the cost (in bits) of describing both the model and the argument head data encoded in the model.
The cost (or description length) for a TCM is calculated according to equation 1.
The number of parameters of the model is given by k, this is the number of classes in the TCM minus one.
S is the sample size of the argument head data.
The cost of describing each argument head (n) is calculated using the log of the probability estimate for the classes on the TCM that n belongs to (ca).
A small portion of the TCM for the object slot of start in the transitive frame is displayed in figure 1.
WordNet classes are displayed in boxes with a label which best reflects the sense of the class.
The probability estimates are shown for the classes along the TCM.
Examples of the argument head data are displayed below the WordNet classes with dotted lines indicating membership at a hyponym class beneath these classes.
We assume that verbs which participate will show a higher degree of similarity between the preferences at the target slots compared with non-participating verbs.
To compare the preferences we compare the probability distributions across WordNet using a measure of distributional similarity.
Since the probability distributions may be at different levels of WordNet, we map the Tcms at the target slots to a common tree cut, a &quot;base cut&quot;.
We experiment with two different types of base cut.
The first is simply a base cut at the eleven root classes of WordNet.
We refer to this as the &quot;root base cut&quot; (RBC).
The second is termed the &quot;union base cut&quot; (PBc).
This is obtained by taking all classes from the union of the two Tcms which are not subsumed by another class in this union.
Duplicates are removed.
Probabilities are assigned to the classes of a base cut using the estimates on the original TCM.
The probability estimate for a hypernym class is obtained by combining the probability estimates for all its hyponyms on the original cut.
Figure 2 exemplifies this process for two Tcms (Tcml and Tcm2) in an imaginary hierarchy.
The UBC is at the classes B, C and D. To quantify the similarity between the probability distributions for the target slots we use the a-skew divergence (asp) proposed by Lee (1999).
1 This measure, defined in equation 2, is a smoothed version of the Kulback-Liebler divergence. p1(x) and p2(x) are the two probability distributions which are being compared.
The a constant is a value between 0 and We also experimented with euclidian distance, the LI norm, and cosine measures.
The differences in performance of these measures were not statistically significant.
1 which smooths p1(x) with p2(x) so that asri is always defined.
We use the same value (0.99) for a as Lee.
If a is set to 1 then this measure is equivalent to the Kulback-Liebler divergence.
We experiment with a SCF lexicon produced from 19.3 million words of parsed text from the BNC (Leech, 1992).
We used the causative and conative alternations, since these have enough candidates in our lexicon for experimentation.
Evaluation is performed on verbs already filtered by the syntactic processing.
The SCF acquisition system has been evaluated elsewhere (Briscoe and Carroll, 1997).
We selected candidate verbs which occurred with 10 or more nominal argument heads at the target slots.
The argument heads were restricted to those which can be classified in the WordNet hypernym hierarchy.
Candidates were selected by hand so as to obtain an even split between candidates which did participate in the alternation (positive candidates) and those which did not (negative candidates).
Four human judges were used to determine the &quot;gold standard&quot;.
The judges were asked to specify a yes or no decision on participation for each verb.
They were also permitted a don't know verdict.
The kappa statistic (Siegel and Castellan, 1988) was calculated to ensure that there was significant agreement between judges for the initial set of candidates.
From these, verbs were selected which had 75% or more agreement, i.e. three or more judges giving the same yes or no decision for the verb.
For the causative alternation we were left with 46 positives and 53 negatives.
For the conative alternation we had 6 of each.
In both cases, we used the Mann Whitney U test to see if there was a significant relationship between the similarity measure and participation.
We then used a threshold on the similarity scores as the decision point for participation to determine a level of accuracy.
We experimented with both the mean and median of the scores as a threshold.
Seven of the negative causative candidates were randomly chosen and removed to ensure an even split between positive and negative candidates for determining accuracy using the mean and median as thresholds.
The following subsection describes the results of the experiments using the method described in section 3 above.
Subsection 4.2 describes an experiment on the same data to determine participation using a similarity measure based on the intersection of the lemmas at the target slots.
The results for the causative alternation are displayed in table 1 for both the RBC and the UBC.
The relationship between participation and asp is highly significant in both cases, with values of p well below 0.01.
Accuracy for the mean and median thresholds are displayed in the fourth and fifth columns.
Both thresholds outperform the random baseline of 50%.
The results for the UBC are slightly improved, compared to those for the RBC, however the improvement is not significant.
The numbers of false negative (FN) and false positive (FP) errors for the mean and median thresholds are displayed in table 2, along with the threshold and accuracy.
The outcomes for each individual verb for the experiment using the RBC and the mean threshold are as follows: add admit answer believe borrow cost declare demand expect feel imagine know notice pay perform practise proclaim read remember sing survive understand win write accelerate bang bend boil break burn change close cook cool crack decrease drop dry end expand fly improve increase match melt open ring rip rock roll shatter shut slam smash snap spill split spread start stop stretch swing tilt turn wake ask attack catch choose climb drink eat help kick knit miss outline pack paint plan prescribe pull remain steal suck warn wash The results for the UBC experiment are very similar.
If the median is used, the number of FPs and FNs are evenly balanced.
This is because the median threshold is, by definition, taken midway between the test items arranged in order of their similarity scores.
There are an even number of items on either side of the decision point, and an even number of positive and negative candidates in our test sample.
Thus, the errors on either side of the decision point are equal in number.
For both base cuts, there are a larger number of false positives than false negatives when the mean is used.
The mean produces a higher accuracy than the median, but gives an increase in false positives.
Many false positives arise where the preferences at both target slots are near neighbours in WordNet.
For example, this occurred for eat and drink.
There verbs have a high probability mass (around 0.7) under the entity class in both target slots, since both people and types of food occur under this class.
In cases like these, the probability distributions at the RBC, and frequently the UBC, are not sufficiently distinctive.
The polysemy of the verbs may provide another explanation for the large quantity of false positives.
The scFs and data of different senses should not ideally be combined, at least not for coarse grained sense distinctions.
We tested the false positive and true negative candidates to see if there was a relationship between the polysemy of a verb and its misclassification.
The number of senses (according to WordNet) was used to indicate the polysemy of a verb.
The Mann Whitney U test was performed on the verbs found to be true negative and false positive using the RBC.
A significant relationship was not found between participation and misclassification.
Both groups had an average of 5 senses per verb.
This is not to say that distinguishing verb senses would not improve performance, provided that there was sufficient data.
However, verb polysemy does not appear to be a major source of error, from our preliminary analysis.
In many cases, such as read which was classified both by the judges, and the system as a negative candidate, the predominant sense of the verb provides the majority of the data.
Alternate senses, for example, The book reads well, often do not contribute enough data so as to give rise to a large proportion of errors.
Finding an appropriate inventory of senses would be difficult, since we would not wish to separate related senses which occur as alternate variants of one another.
The inventory would therefore require knowledge of the phenomena that we are endeavouring to acquire automatically.
To show that our method will work for other RSAS, we use the conative.
Our sample size is rather small since we are limited by the number of positive candidates in the corpus having sufficient frequency for both scFs.
The sparse data problem is acute when we look at alternations with specific prepositions.
A sample of 12 verbs (6 positive and 6 negative) remained after the selection process outlined above.
For this small sample we obtained a significant result (p = 0.02) with a mean accuracy of 67% and a median accuracy of 83%.
On this occasion, the median performed better than the mean.
More data is required to see if this difference is significant.
This experiment was conducted using the same data as that used in the previous subsection.
In this experiment, we used a similarity score on the argument heads directly, instead of generalizing the argument heads to WordNet classes.
The venn diagram in figure 3 shows a subset of the lemmas at the transitive and intransitive scFs for the verb break.
The lemma based similarity measure is termed lemma overlap (LO) and is given in equation 3, where A and B represent the target slots.
LO is the size of the intersection of the multisets of argument heads at the target slots, divided by the size of the smaller of the two multisets.
The intersection of two multisets includes duplicate items only as many times as the item is in both sets.
For example, if one slot contained the argument heads (person, person, person, child, man, spokeswoman), and the other slot contained {person, person, child, chair, collection}, then the intersection would be {person, person, child}, and LO would be t .
This measure ranges between zero (no overlap) and I (where one set is a proper subset of that at the other slot).
Using the Mann Whitney U test on the LO scores, we obtained a z score of 2.00.
This is significant to the 95% level, a lower level than that for the classbased experiments.
The results using the mean and median of the LO scores are shown in table 3.
Performance is lower than that for the class-based experiments.
The outcome for the individual verbs using the mean as a threshold was:add admit answer borrow choose climb cost declare demand drink eat feel imagine notice outline pack paint perform plan practise prescribe proclaim read remain sing steal suck survive understand wash win write accelerate bang break cook crack decrease drop expand flood land march repeat rip rock shatter Interestingly, the errors for the LO measure tend to be false negatives, rather than false positives.
The LO measure is much more conservative than the approach using the Tcms.
In this case the median threshold produces better results.
For the conative alternation, the lemma based method does not show a significant relationship between participation and the LO scores.
Moreover, there is no difference between the sums of the ranks of the two groups for the Mann Whitney U test.
The mean produces an accuracy of 58% whilst the median produces an accuracy of 50%.
