The tagger works by automatically recognizing and remedying its weaknesses, thereby incrementally improving its performance.
The tagger initially tags by assigning each word its most likely tag, estimated by examining a large tagged corpus, without regard to context.
In both sentences below, run would be tagged as a verb: The run lasted thirty minutes.
3.
One of the two preceding (following) words is tagged We run three miles every day.
The initial tagger has two procedures built in to improve performance; both make use of no contextual information.
One procedure is provided with information that words that were not in the training corpus and are capitalized tend to be proper nouns, and attempts to fix tagging mistakes accordingly.
This information could be acquired automatically (see below), but is prespecified in the current implementation.
In addition, there is a procedure which attempts to tag words not seen in the training corpus by assigning such words the tag most common for words ending in the same three letters.
For example, blahblahous would be tagged as an adjective, because this is the most common tag for words ending in ous.
This information is derived automatically from the training corpus.
This very simple algorithm has an error rate of about 7.9% when trained on 90% of the tagged Brown Corpus' [Francis and Kueera 82], and tested on a separate 5% of the corpus.2 Training consists of compiling a list of the most common tag for each word in the training corpus.
The tagger then acquires patches to improve its performance.
Patch templates are of the form: The initial tagger was trained on 90% of the corpus (the training corpus).
5% was held back to be used for the patch acquisition procedure (the patch corpus) and 5% for testing.
Once the initial tagger is trained, it is used to tag the patch corpus.
A list of tagging errors is compiled by comparing the output of the tagger to the correct tagging of the patch corpus.
This list consists of triples < taga,tagb,number >, indicating the number of times the tagger mistagged a word with taga when it should have been tagged with tagb in the patch corpus.
Next, for each error triple, it is determined which instantiation of a template from the prespecified set of pdtch templates results in the greatest error reduction.
Currently, the patch templates are: Change tag a to tag b when: 8.
The previous word is (is not) capitalized.
For each error triple < taga,tagb, number > and patch, we compute the reduction in error which results from applying the patch to remedy the mistagging of a word as taga when it should have been tagged tagb.
We then compute the number of new errors caused by applying the patch; that is, the number of times the patch results in a word being tagged as tagb when it should be tagged taga.
The net improvement is calculated by subtracting the latter value from the former.
For example, when the initial tagger tags the patch corpus, it mistags 159 words as verbs when they should be nouns.
If the patch change the tag from verb to noun if one of the two preceding words is tagged as a determiner is applied, it corrects 98 of the 159 errors.
However, it results in an additional 18 errors from changing tags which really should have been verb to noun.
This patch results in a net decrease of 80 errors on the patch corpus.
The patch which results in the greatest improvement to the patch corpus is added to the list of patches.
The patch is then applied in order to improve the tagging of the patch corpus, and the patch acquisition procedure continues.
The first ten patches found by the system are listed below.
The first patch states that if a word is tagged TO and the following word is tagged AT, then switch the tag from TO to IN.
This is because a noun phrase is much more likely to immediately follow a preposition than to immediately follow infinitive TO.
The second patch states that a tag should be switched from VBN to VBD if the preceding word is capitalized.
This patch arises from two facts: the past verb tag is more likely than the past participle verb tag after a proper noun, and is also the more likely tag for the second word of the sentence.4 The third patch states that VBD should be changed to VBN if any of the preceding three words are tagged HVD.
Once the list of patches has been acquired, new text can be tagged as follows.
First, tag the text using the basic lexical tagger.
Next, apply each patch in turn to the corpus to decrease the error rate.
A patch which changes the tagging of a word from a to b only applies if the word has been tagged b somewhere in the training corpus.
Note that one need not be too careful when constructing the list of patch templates.
Adding a bad template to the list will not worsen performance.
If a template is bad, then no rules which are instantiations of that template will appear in the final list of patches learned by the tagger.
This makes it easy to experiment with extensions to the tagger.
The tagger was tested on 5% of the Brown Corpus including sections from every genre.
First, the test corpus was tagged by the simple lexical tagger.
Next, each of the patches was in turn applied to the corpus.
Below is a graph showing the improvement in accuracy from applying patches.
It is significant that with only 71 patches, an error rate of 5.1% was obtained'.
Of the 71 patches, 66 resulted in a reduction in the number of errors in the test corpus, 3 resulted in no net change, and 2 resulted in a higher number of errors.
Almost all patches which were effective on the training corpus were also effective on the test corpus.
Unfortunately, it is difficult to compare our results with other published results.
In [Meteer et at.
91], an error rate of 3-4% on one domain, Wall Street Journal articles and 5.6% on another domain, texts on terrorism in Latin American countries, is quoted.
However, both the domains and the tag set are different from what we use.
[Church 88] reports an accuracy of &quot;95-99% correct, depending on the definition of correct&quot;.
We implemented a version of the algorithm described by Church.
When trained and tested on the same samples used in our experiment, we found the error rate to be about 4.5%.
[DeRose 88] quotes a 4% error rate; however, the sample used for testing was part of the training corpus.
[Garside et al. 87] reports an accuracy of 96-97%.
Their probabilistic tagger has been augmented with a handcrafted procedure to pretag problematic &quot;idioms&quot;.
This procedure, which requires that a list of idioms be laboriously created by hand, contributes 3% toward the accuracy of their tagger, according to [DeRose 88].
The idiom list would have to be rewritten if one wished to use this tagger for a different tag set or a different corpus.
It is interesting to note that the information contained in the idiom list can be automatically acquired by the rule-based tagger.
For example, their tagger had difficulty tagging as old as.
An explicit rule was written to pretag as old as with the proper tags.
According to the tagging scheme of the Brown Corpus, the first as should be tagged as a qualifier, and the second as a subordinating conjunction.
In the rule-based tagger, the most common tag for as is subordinating conjunction.
So initially, the second as is tagged correctly and the first as is tagged incorrectly.
To remedy this, the system acquires the patch: if the current word is tagged as a subordinating conjunction, and so is the word two positions ahead, then change the tag of the current word to gualifier.6 The rule-based tagger has automatically learned how to properly tag this &quot;idiom.&quot; Regardless of the precise rankings of the various taggers, we have demonstrated that a simple rule-based tagger with very few rules performs on par with stochastic taggers.
6This was one of the 71 patches acquired by the rule-based tagger.
