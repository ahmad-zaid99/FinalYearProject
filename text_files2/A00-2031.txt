We have found it useful to define our statistical model in terms of features.
A 'feature', in this context, is a boolean-valued function, generally over parse tree nodes and either node labels or lexical items.
Features can be fairly simple and easily read off the tree (e.g.
'this node's label is X', 'this node's parent's label is Y'), or slightly more complex (`this node's head's partof-speech is Z').
This is concordant with the usage in the maximum entropy literature (Berger et al., 1996).
When using a number of known features to guess an unknown one, the usual procedure is to calculate the value of each feature, and then essentially look up the empirically most probable value for the feature to be guessed based on those known values.
Due to sparse data, some of the features later in the list may need to be ignored; thus the probability of an unknown feature value would be estimated as where P refers to an empirically observed probability.
Of course, if features 1 through i only co-occur a few times in the training, this value may not be reliable, so the empirical probability is usually smoothed: The values for Ai can then be determined according to the number of occurrences of features 1 through i together in the training.
One way to think about equation 1 (and specifically, the notion that j will depend on the values of Ii fn) is as follows: We begin with the prior probability of f. If we have data indicating P(fIfi), we multiply in that likelihood, while dividing out the original prior.
If we have data for P( fl f2), we multiply that in while dividing out the P (f in) term.
This is repeated for each piece of feature data we have; at each point, we are adjusting the probability we already have estimated.
If knowledge about feature fi makes f more likely than with just fi_i, the term where fi is added will be greater than one and the running probability will be adjusted upward.
This gives us the new probability shown in equation 3, which is exactly equivalent to equation 1 since everything except the last numerator cancels out of the equation.
The value of j is chosen such that features Ii fi are sufficiently represented in the training data; sometimes all n features are used, but often that would cause sparse data problems.
Smoothing is performed on this equation exactly as before: each term is interpolated between the empirical value and the prior estimated probability, according to a value of Ai that estimates confidence.
But aside from perhaps providing a new way to think about the problem, equation 3 is not particularly useful as it is—it is exactly the same as what we had before.
Its real usefulness comes, as shown in (Charniak, 1999), when we move from the notion of a feature chain to a feature tree.
These feature chains don't capture everything we'd like them to.
If there are two independent features that are each relatively sparse but occasionally carry a lot of information, then putting one before the other in a chain will effectively block the second from having any effect, since its information is (uselessly) conditioned on the first one, whose sparseness will completely dilute any gain.
What we'd really like is to be able to have a feature tree, whereby we can condition those two sparse features independently on one common predecessor feature.
As we said before, equation 3 represents, for each feature the probability of f based on fi and all its predecessors, divided by the probability of f based only on the predecessors.
In the chain case, this means that the denominator is conditioned on every feature from 1 to i — 1, but if we use a feature tree, it is conditioned only on those features along the path to the root of the tree.
A notable issue with feature trees as opposed to feature chains is that the terms do not all cancel out.
Every leaf on the tree will be represented in the numerator, and every fork in the tree (from which multiple nodes depend) will be represented at least once in the denominator.
For example: in figure 3 we have a small feature tree that has one target feature and four conditioning features.
Features b and d are independent of each other, but each depends on a; c depends directly only on b.
The unsmoothed version of the corresponding equation would be which, after cancelling of terms and smoothing, results in Note that strictly speaking the result is not a probability distribution.
It could be made into one with an appropriate normalisation—the so-called partition function in the maximumentropy literature.
However, if the independence assumptions made in the derivation of equation 4 are good ones, the partition function will be close to 1.0.
We assume this to be the case for our feature trees.
Now we return the discussion to function tagging.
There are a number of features that seem tar et feature to condition strongly for one function tag or another; we have assembled them into the feature tree shown in figure 4.2 This figure should be relatively self-explanatory, except for the notion of an 'alternate head'; currently, an alternate head is only defined for prepositional phrases, and is the head of the object of the prepositional phrase.
This data is very important in distinguishing, for example, 'by John' (where John might be a logical subject) from 'by next year' (a temporal modifier) and 'by selling it' (an adverbial indicating manner).
In the training phase of our experiment, we gathered statistics on the occurrence of function tags in sections 2-21 of the Penn treebank.
Specifically, for every constituent in the treebank, we recorded the presence of its function tags (or lack thereof) along with its conditioning information.
From this we calculated the empirical probabilities of each function tag referenced in section 2 of this paper.
Values of A were determined using EM on the development corpus (treebank section 24).
To test, then, we simply took the output of our parser on the test corpus (treebank section 23), and applied a postprocessing step to add function tags.
For each constituent in the tree, we calculated the likelihood of each function tag according to the feature tree in figure 4, and for each category (see figure 2) we assigned the most likely function tag (which might be the null tag).
2The reader will note that the 'features' listed in the tree are in fact not boolean-valued; each node in the given tree can be assumed to stand for a chain of boolean features, one per potential value at that node, exactly one of which will be true.
To evaluate our results, we first need to determine what is 'correct'.
The definition we chose is to call a constituent correct if there exists in the correct parse a constituent with the same start and end points, label, and function tag (or lack thereof).
Since we treated each of the four function tag categories as a separate feature for the purpose of tagging, evaluation was also done on a per-category basis.
The denominator of the accuracy measure should be the maximum possible number we could get correct.
In this case, that means excluding those constituents that were already wrong in the parser output; the parser we used attains 89% labelled precision-recall, so roughly 11% of the constituents are excluded from the function tag accuracy evaluation.
(For reference, we have also included the performance of our function tagger directly on treebank parses; the slight gain that resulted is discussed below.)
Another consideration is whether to count non-tagged constituents in our evaluation.
On the one hand, we could count as correct any constituent with the correct tag as well as any correctly non-tagged constituent, and use as our denominator the number of all correctlylabelled constituents.
(We will henceforth refer to this as the 'with-null' measure.)
On the other hand, we could just count constituents with the correct tag, and use as our denominators the total number of tagged, correctly-labelled constituents.
We believe the latter number (`nonull') to be a better performance metric, as it is not overwhelmed by the large number of untagged constituents.
Both are reported below.
There are, it seems, two reasonable baselines for this and future work.
First of all, most constituents in the corpus have no tags at all, so obviously one baseline is to simply guess no tag for any constituent.
Even for the most common type of function tag (grammatical), this method performs with 87% accuracy.
Thus the with-null accuracy of a function tagger needs to be very high to be significant here.
The second baseline might be useful in examining the no-null accuracy values (particularly the recall): always guess the most common tag in a category.
This means that every constituent gets labelled with `--SBJ-TMP-TPC-CLR' (meaning that it is a topicalised temporal subject that is 'closely related' to its verb).
This combination of tags is in fact entirely illegal by the treebank guidelines, but performs adequately for a baseline.
The precision is, of course, abysmal, for the same reasons the first baseline did so well; but the recall is (as one might expect) substantial.
The performances of the two baseline measures are given in Table 1.
In table 2, we give the results for each category.
The first column is the with-null accuracy, and the precision and recall values given are the nonull accuracy, as noted in section 4.
Grammatical tagging performs the best of the four categories.
Even using the more difficult no-null accuracy measure, it has a 96% accuracy.
This seems to reflect the fact that grammatical relations can often be guessed based on constituent labels, parts of speech, and highfrequency lexical items, largely avoiding sparsedata problems.
Topicalisation can similarly be guessed largely on high-frequency information, and performed almost as well (93%).
On the other hand, we have the form/function tags and the 'miscellaneous' tags.
These are characterised by much more semantic information, and the relationships between lexical items are very important, making sparse data a real problem.
All the same, it should be noted that the performance is still far better than the baselines.
The feature tree given in figure 4 is by no means the only feature tree we could have used.
Indeed, we tried a number of different trees on the development corpus; this tree gave among the best overall results, with no category performing too badly.
However, there is no reason to use only one feature tree for all four categories; the best results can be got by using a separate tree for each one.
One can thus achieve slight (one to three point) gains in each category.
The overall performance, given in table 3, appears promising.
With a tagging accuracy of about 87%, various information retrieval and knowledge base applications can reasonably expect to extract useful information.
The performance given in the first row is (like all previously given performance values) the function-tagger's performance on the correctlylabelled constituents output by our parser.
For comparison, we also give its performance when run directly on the original treebank parse; since the parser's accuracy is about 89%, working directly with the treebank means our statistics are over roughly 12% more constituents.
This second version does slightly better.
The main reason that tagging does worse on the parsed version is that although the constituent itself may be correctly bracketed and labelled, its exterior conditioning information can still be incorrect.
An example of this that actually occurred in the development corpus (section 24 of the treebank) is the 'that' clause in the phrase 'can swallow the premise that the rewards for such ineptitude are six-figure salaries', correctly diagrammed in figure 5.
The function tagger gave this SBAR an ADV tag, indicating an unspecified adverbial function.
This seems extremely odd, given that its conditioning information (nodes circled in the figure) clearly show that it is part of an NP, and hence probably modifies the preceding NN.
Indeed, the statistics give the probability of an ADV tag in this conditioning environment as vanishingly small.
However, this was not the conditioning information that the tagger received.
The parser had instead decided on the (incorrect) parse in figure 6.
As such, the tagger's decision makes much more sense, since an SBAR under two VPs whose heads are VB and MD is rather likely to be an ADV.
(For instance, the 'although' clause of the sentence 'he can help, although he doesn't want to.' has exactly the conditioning environment given in figure 6, except that its predecessor is a comma; and this SBAR would be correctly tagged ADV.)
The SBAR itself is correctly bracketed and labelled, so it still gets counted in the statistics.
Happily, this sort of case seems to be relatively rare.
Another thing that lowers the overall performance somewhat is the existence of error and inconsistency in the treebank tagging.
Some tags seem to have been relatively easy for the human treebank taggers, and have few errors.
Other tags have explicit caveats that, however welljustified, proved difficult to remember for the taggers—for instance, there are 37 instances of a PP being tagged with LGS (logical subject) in spite of the guidelines specifically saying, '[LGS] attaches to the NP object of by and not to the PP node itself.'
(Bies et al., 1995) Each mistagging in the test corpus can cause up to two spurious errors, one in precision and one in recall.
Still another source of difficulty comes when the guidelines are vague or silent on a specific issue.
To return to logical subjects, it is clear that `the loss' is a logical subject in 'The company was hurt by the loss', but what about in 'The company was unperturbed by the loss'?
In addition, a number of the function tags are authorised for 'metaphorical use', but what exactly constitutes such a use is somewhat inconsistently marked.
It is as yet unclear just to what degree these tagging errors in the corpus are affecting our results.
