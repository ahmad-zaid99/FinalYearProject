{"id": "S001048251500342X", "article": "MAIN-TITLE Regularized logistic regression with adjusted adaptive elastic net for gene selection in high dimensional cancer classification   HIGHLIGHTS          The AAElastic showed superior results in terms all evaluation criteria.      The AAElastic selected more correlated genes than the other methods.      The AAElastic performed remarkably well in classification stability test.      In terms of gene selection consistency, AAElastic significantly performed well.          KEYPHRASES   Adaptive elastic net  Oracle property  Regularized logistic regression  Cancer classification  Gene selection   Recently, molecular biology and genetics research has been transformed from the study of individual genes to the exploration of the whole genome. DNA microarrays technology is one such technique to measure the expression levels of thousands of genes in a single experiment [1\u20134]. Cancer classification based on microarray gene expression data has become one of the most active research topics in biomedical research, which is suitable for comparing the gene expression levels in tissues under different conditions, such as normal versus abnormal [5,6].  However, cancer classification with DNA microarray data is a challenging issue because of its high dimensionality and the small samples size. Typically, the number of genes is more than thousands from a hundred or less tissue samples [7,8]. Due to the high dimensionality and the small sample size, gene selection is an important issue for cancer classification and has been extensively studied in recent years. The application of gene selection methods allows the identification of a small number of important genes that can be used as biologically relevant genes of the appropriate cancer [9\u201311]. From the viewpoint of biologists, gene selection can increase the classification accuracy of the classification method by removing irrelevant and noisy genes [12\u201314].  Many gene selection methods have been proposed to select a subset of genes that can have high classification accuracy for cancer classification. Recently, regularization methods, which are capable of conducting efficient gene selection and model estimation simultaneously, have gained popularity [15,16]. From the statistical perspective, regularization methods can control the effects of the overfitting and multicollinearity [17]. Numerous statistical methods have been successfully applied in the area of cancer classification. Among them, logistic regression (LR) is considered to be a powerful discriminative method. LR provides predicted probabilities of class membership and easy interpretation of the gene coefficients [17]. However, LR is neither applicable nor suitable for high-dimensional cancer classification because the design matrix is singular. Thus, the iteration methods, such as Newton\u2013Raphson\u05f3s method cannot work [18]. Regularized logistic regression (RLR) has been successfully applied in high-dimensional cancer classification [6,19\u201323]. The benefits of RLR are that (a) the classification accuracy can often be improved by shrinking the regression coefficients, and (b) selecting a small subset of genes that exhibits the strongest effects provides a classification model with easy interpretation.  An RLR with different regularization terms can be applied. The most widely and popular regularized term is the least absolute shrinkage and selection operator (LASSO) [24]. LASSO imposes the    \u2113   1   \u2212 norm  regularization to the loss function. Because of the    \u2113   1   \u2212 norm  property, LASSO can perform variable selection by assigning some genes coefficients to zero. For this reason, LASSO has gained popularity in high-dimensional data.  Despite the advantage of LASSO, it has three shortcomings [25,26]. First, LASSO has a biased gene selection, which means it is an inconsistent gene selection method because it regularizes all gene coefficients equally [27]. In other words, LASSO does not have the oracle property, which refers to the probability of selecting the right set of genes (with nonzero coefficients) converges to one, and that the estimators of the nonzero coefficients have asymptotically normal distribution with the same means and covariances as if the zero coefficients are known in a prior [28,29]. Related to this limitation of LASSO, concerning the oracle property, Zou [30] proposed the adaptive LASSO in which adaptive weights are used for regularizing different coefficients in the    \u2113   1   \u2212 norm  regularization. Second, it cannot select more genes than the number of samples. Last, in the microarray gene data, there is grouping among genes, where genes that share a common biological pathway have a high pairwise correlation with each other. LASSO tries to select only one gene or a few of them among a group of correlated genes. To overcome the last two limitations, Zou and Hastie [26] proposed the elastic net regularization, for which the regularization is a linear combination of    \u2113   1   \u2212 norm  and    \u2113   2   \u2212 norm  . Similar to LASSO, elastic net lacks the oracle property even though it outperforms LASSO. Zou and Zhang [31] proposed adaptive elastic net to handle grouping effects and enjoy the oracle property simultaneously.  In high-dimensional classification data, however, the adaptive elastic net faces practical problems where a maximum likelihood estimate (MLE), which is usually proposed as an initial weight, is simply infeasible, and, hence, the adaptive elastic net is no longer applicable. Zou and Zhang [31] proposed using the elastic net estimates as an initial weight in adaptive elastic net; however, using this weight may not be preferable for three reasons: First, it is well known that gene selection by elastic net can be inconsistent [31,32]. In other words, this initial weight is biased in selecting genes. Second, elastic net exhibits difficulties when a group of genes is nearly linearly dependent, because it does not take into account the correlation structure among genes [33]. Last, the elastic net does not perform well when the pairwise correlations between genes are not extremely high; El Anbari and Mkhadri [34] stated that if the absolute correlation between genes is slightly less than 0.95, the elastic net may be slightly less reliable.  In this study, a new initial weight inside    \u2113   1   \u2212 norm  regularization in adaptive elastic regularized logistic regression is proposed, which is defined as the ratio of the standard error of the ridge regression estimator to the ridge regression estimator. The main objective behind this new initial weight is to adjust the    \u2113   1   \u2212 norm  regularization in regularized logistic regression by improving the gene selection consistency while still maintaining the grouping effects. To evaluate the effectiveness of the new initial weight, we applied three DNA microarray datasets of cancer classification. Moreover, a comparison is made with other regularization terms and initial weights.  The rest of this paper is arranged as follows: Section 2 displays the regularized logistic regression, the adaptive regularized logistic regression, and the proposed method. While Section 3 covers the real data application results. Finally, the conclusion is covered by Section 4.   METHODS   Logistic regression is a statistical method to model a binary classification problem. The regression function has a nonlinear relation with the linear combination of the genes. In cancer classification, the response variable of the logistic regression has two values either 1 for the tumor class or 0 for the normal class. Assume that we have n observations and p genes. Let    y   i   \u2208 { 0 , 1 }  be the response variable value for observation i,  i = 1 , 2 , .. . , n  and    x   i   =   (   x   i 1   ,   x   i 2   , .. . ,   x   i n   )  T   be the   i   t h   gene vector of the gene matrix X. Then, the response variable is related to genes by  (1)     \u03c0   i   = p (   y   i   = 1 |   x   i   ) =   exp (   x   i   T   \u03b2 )   1 + exp (   x   i   T   \u03b2 )   ,    i = 1 , 2 , .. . , n    where  \u03b2 =   (   \u03b2   0   ,   \u03b2   1   , .. . ,   \u03b2   p   )  T   is a  p \u00d7 1  vector of unknown gene coefficients. The log-likelihood function of the logit transformation of Eq. (1) is defined as  (2)   \u2113 ( \u03b2 ) =  \u2211  i = 1  n   {    y   i    log (   \u03c0   i   ) + ( 1 \u2212   y   i   ) log ( 1 \u2212   \u03c0   i   )  }  .       Regularized logistic regression adds a nonnegative regularization term to the negative log-likelihood function,  \u2113 ( \u03b2 )  , such that the size of gene coefficients in high-dimension can be controlled. Several regularization terms have been discussed in the literature [23,24,26,35]. The    \u2113   1   \u2212 norm  regularization, proposed by Tibshirani [36], is one of the popular regularization terms. The    \u2113   1   \u2212 norm  regularization performs gene selection and estimation simultaneously by constraining the negative log-likelihood function of gene coefficients. Thus, the RLR is defined as:  (3)   R L R = \u2212 \u2113 ( \u03b2 ) + \u03bb P ( \u03b2 ) .       The estimation of the vector \u03b2 is obtained by minimizing Eq. (3)   (4)      \u03b2 ^    R L R   = arg   min   \u03b2    [  \u2212  \u2211  i = 1  n   {    y   i    log (   \u03c0   i   ) + ( 1 \u2212   y   i   ) log ( 1 \u2212   \u03c0   i    }  + \u03bb  P ( \u03b2 )  ]  ,    where  \u03bb  P ( \u03b2 )  is the regularization term that regularized the estimates. The penalty term depends on the positive tuning parameter, \u03bb, which controls the tradeoff between fitting the data to the model and the effect of the regularization. In other words, it controls the amount of shrinkage. For the  \u03bb = 0  , we obtain the MLE solution. In contrast, for large values of \u03bb the influence of the regularization term on the coefficient estimate increases. Choosing the tuning parameter is an important part of the model fitting. If the focus is on classification, the tuning parameter should find the right balance between the bias and variance to minimize the misclassification error. Without loss of generality, it is assumed that the genes are standardized,   \u2211  i = 1  n     x   i j   = 0   and  (   n   \u2212 1   )  \u2211  i = 1  n      x  2   i j   = 1  ,  \u2200 j \u2208  {  1 , 2 , .. . , p  }   . As a result, the intercept   \u03b2   0   is not regularized. The estimation of the vector \u03b2 using the LASSO (    \u2113   1   \u2212 norm  regularization) is defined as:  (5)      \u03b2 ^    L A S S O   = arg   min   \u03b2    [  \u2212  \u2211  i = 1  n   {    y   i    log (   \u03c0   i   ) + ( 1 \u2212   y   i   ) log ( 1 \u2212   \u03c0   i    }  + \u03bb  \u2211  j = 1  p   |    \u03b2   j    |   ]  ,    where \u03bb is a tuning parameter. It reduces to the MLE estimator when  \u03bb = 0  . On the other hand, if  \u03bb \u2192 \u221e  , the regularization term forces all the gene coefficients to be zero. In practice, the value of \u03bb is often chosen by a cross-validation procedure. Eq. (5) can be efficiently solved by the coordinate descent algorithm [37,38].  Elastic net is a regularization method for gene selection, which is introduced by Zou and Hastie [26] to deal with the first two drawbacks of LASSO. Elastic net tries to combine the    \u2113   2   \u2212 norm  with    \u2113   1   \u2212 norm  to deal with the highly correlated genes and to perform gene selection simultaneously. The RLR using elastic net penalty is defined by  (6)      \u03b2 ^    E l a s t i c   = arg   min   \u03b2    [  \u2212  \u2211  i = 1  n   {    y   i    log (   \u03c0   i   ) + ( 1 \u2212   y   i   ) log ( 1 \u2212   \u03c0   i    }  +   \u03bb   1    \u2211  j = 1  p    |    \u03b2   j    |  +   \u03bb   2    \u2211  j = 1  p       \u03b2   j    2     ]  .       As we observe from Eq. (6), elastic net estimator depends on two non-negative tuning parameters \u03bb 1 and \u03bb 2 which leads to regularized logistic regression solution.  According to Fan and Li [28], LASSO does not attain the oracle property. This is because LASSO is equally regularizing all the coefficients, leading the estimation to be biased. To overcome this drawback, Zou [30] proposed the adaptive LASSO where adaptive weights are assigned for regularizing different coefficients in the    \u2113   1   \u2212 norm  penalty. By assigning the small coefficients with large weight and the large coefficients with low weight, it could be possible to reduce the selection bias, and, therefore, it can consistently select the relevant coefficients.  The regularized logistic regression using the adaptive LASSO of \u03b2 is defined by:  (7)      \u03b2 ^    A L A S S O   = arg   min   \u03b2    [  \u2212  \u2211  i = 1  n   {    y   i    log (   \u03c0   i   ) + ( 1 \u2212   y   i   ) log ( 1 \u2212   \u03c0   i    }  + \u03bb  \u2211  j = 1  p     w   j    |    \u03b2   j    |    ]  ,    where  w =   (   w   1   , .. . ,   w   p   )  T   is p\u00d71 weight vector and    w   j   =   ( |    \u03b2 ^    j   | )   \u2212 \u03b3    , where  \u03b3 > 0  , and  \u03b2 ^  is a root n-consistent initial value, which means that it converges to the true estimate \u03b2 with    O   p   (   n   \u2212 1 / 2   )  .  In a similar way to LASSO, the elastic net does not enjoy the oracle property even though it performs much better in classification accuracy [31,32]. Additionally, Zou and Zhang [31] pointed out that the adaptive LASSO outperforms LASSO in terms of achieving the oracle property, even though the grouping effect problem for adaptive LASSO remains. As a result, the adaptive elastic net was introduced by Zou and Zhang [31] and Ghosh [32], which combines the    \u2113   2   \u2212 norm  regularization with the adaptive LASSO. The improved regularization method, adaptive elastic net, outperforms adaptive LASSO in terms of gene selection consistency and grouping effect simultaneously. For fixed \u03bb 2, the regularized logistic regression using the adaptive elastic net (AElastic) of \u03b2 is defined by:  (8)       \u03b2 ^   *   A E l a s t i c   = arg   min   \u03b2    [      \u2212  \u2211  i = 1  n   {    y   i   *    log (   \u03c0   i   ) + ( 1 \u2212   y   i   *   ) l o g ( 1 \u2212   \u03c0   i   )  }        +   \u03bb   1    \u2211  j = 1  p     w   j    |    \u03b2   j    |  +    \u03bb   2    \u2211  j = 1  p     \u03b2   j   2         ]  ,    where    y  *  =   (     y     0     )   ( n + p ) \u00d7 1    is the augmented vector [26], and    w   j   =   ( |    \u03b2 ^    j   | )   \u2212 \u03b3   ,  j = 1 , 2 , .. . , p  is the adaptive weight based on the initial estimator  \u03b2 ^  for a positive constant \u03b3.  In cancer classification, genes exhibit certain natural grouping structures; for example, gene expression profiles may be grouped according to their pathways, and it is often preferable that a group of genes are either kept or eliminated from the classification together. Furthermore, the regularization method that selects the correct subset of genes with probability tending to one is desired. The adaptive elastic net was successfully applied for gene selection in cancer classification [35,39,40].  Choosing the initial weight is crucial in the adaptive elastic net. Ghosh [32] studied the grouping effect in the adaptive elastic net by using the ordinary least squares (OLS) as the initial weight in low-dimension data. In logistic regression, MLE instead of OLS was proposed as an initial weight. In high-dimensional cancer classification, however, using MLE is simply infeasible and hence the adaptive elastic net is no longer applicable. Zou and Zhang [31], on the other hand, proposed using the elastic net as an initial weight either in low-dimensional data or high-dimensional data. Generally, the elastic net estimator is inconsistent in itself. In other words, this initial weight is biased in selecting genes. In addition, the elastic net performs well when the pairwise correlations between variables are very high. El Anbari and Mkhadri [34] stated that if the absolute correlation between genes is less than 0.95, the elastic net may be slightly less reliable. Moreover, the elastic net does not take into account the correlation structure among genes [33].  From these aforementioned drawbacks, using the elastic net estimator in adaptive elastic regularized logistic regression in high-dimensional cancer classification may not be preferable. The ratio of the standard error of the ridge regression estimator to the ridge regression estimator is proposed as the initial weight in the adaptive elastic net. According to the nature of the    \u2113   2   \u2212 norm  , the ridge penalty tries to force the estimated coefficients of highly correlated genes to be close to each other. In particular, this property in the elastic net may help to select or omit the highly correlated genes together if their coefficients are close to each other. However, this property loses the capability for estimating the coefficients of highly correlated genes with different magnitudes, especially with different signs [41]. The advantage of using the standard error of the ridge estimator   s      \u03b2 ^    R i d g e     is to adjust the regularized logistic regression using the adaptive elastic net (AAElastic) when using ridge regression estimates or elastic net estimates as an initial weight. As a result, AAElastic is able to improve genes selection consistently and maintain the grouping effects simultaneously. Cule and De Iorio [42] proposed a procedure to calculate the   s      \u03b2 ^    R i d g e     depending on principal component analysis.  Let    \u03b2 ^   R i d g e   =   (   \u03b2 ^   1 ( R i d g e )   , .. . ,   \u03b2 ^   p ( R i d g e )   )  T   be the vector of ridge regression estimate,    s      \u03b2 ^    R i d g e     =   (   s   1 (    \u03b2 ^    R i d g e )     , .. . ,   s   p (    \u03b2 ^    R i d g e )     )  T   be the vector of the standard error of the ridge regression, then    w   R a t i o   =   (   w   1 ( r a t i o )   , .. . , w  )     p ( r a t i o )     T   be the ratio weight vector where    w   j   =   (   s   j (    \u03b2 ^    R i d g e   )   / |    \u03b2 ^    j ( R i d g e )   | )   \u2212 \u03b3   ,  j = 1 , 2 , .. . , p  . Furthermore, let      x   j     * *   =    x  *  j  /   w   R a t i o , j   , j = 1 , 2 , .. . , p  , then, the regularized logistic regression using AAElastic is defined as:  (9)       \u03b2 ^    * *    A A E l a s t i c   = arg   min   \u03b2    [      \u2212  \u2211  i = 1  n   {    y   i   *    log (   \u03c0   i   ) + ( 1 \u2212   y   i   *   ) ln ( 1 \u2212   \u03c0   i   )  }        +   \u03bb   1    \u2211  j = 1  p     w   R a t i o , j    |    \u03b2   j    |  +    \u03bb   2    \u2211  j = 1  p       \u03b2   j    2        ]  .       Eq. (9) can be effectively solved by the coordinate descent method in glmnet package [38]. After solving Eq. (9), the true vector estimator  \u03b2 ^  is calculated as:  (10)      \u03b2 ^    A A E l a s t i c   = ( 1 +   \u03bb   2   )     \u03b2 ^    * *    A A E l a s t i c   /   w   R a t i o , j   ,   j = 1 , 2 , .. . , p .       In order to prove that our proposed method has the oracle property, the theoretical results were covered in the Supplementary file. Theorem 1 (Oracle property) : Suppose that  A =  {  j :    \u03b2   j   \u2260 0  }   and   A ^  (   \u03bb   1   ,   \u03bb   2   ) =  {  j :      \u03b2 ^    A A E l a s t i c   (   \u03bb   1   ,   \u03bb   2   ) \u2260 0  }   . Under the regularity conditions (R1) \u2013 (R6), the adjusted adaptive elastic net (AAElastic) has the oracle property by satisfying the following:  1. Consistency in variable selection:    lim   n \u2192 \u221e   p (  A ^  (   \u03bb   1   ,   \u03bb   2   ) = A ) = 1     Asymptotic normality:    \u03b7  T    ( 1 +   \u03bb   2   )   \u03a3   A   \u2212 1     2 +   \u03bb   2         \u03a3   A         \u03b2 ^    A A E l a s t i c   (   \u03bb   1   ,   \u03bb   2   ) ~ N ( 0 ,   \u03c3  2  )  , where \u03b7 is a vector of norm 1, and    \u03a3   A   =   X   A   T     X   A    .  For practical applications, one has to decide the values of the tuning parameters. Classically, cross-validation (CV) has been widely used. However, it is computationally intensive for AAElastic, simply because there are three tuning parameters   \u03bb   1   ,   \u03bb   2   and \u03b3 . For simplicity,  \u03b3 = 1  was used for the real data application. The   \u03bb   2   is typically assumed to take values from a range between 0 and 100. For each   \u03bb   2   , the coordinate descent algorithm produces the entire solution path. Then the optimal pair of  (   \u03bb   1   ,   \u03bb   2   )  is obtained using k-fold CV.  To evaluate our proposed method, AAElastic, in the field of cancer classification, three publicly well-known binary cancer classification datasets were used. The first was the prostate cancer dataset published by [43]. It consisted of 102 samples of 52 prostate tumor samples and 50 non-tumor tissues, where each sample has 12600 genes. According to Yang et al. [44], a subset of 5966 genes was adapted in the classification by setting the intensity thresholds at 100\u201316000 units, then filtering out the genes with either max/min \u22645 or max\u2013min \u226450.  The second was the diffuse large B-cell lymphoma (DLBCL) dataset published by [45]. The DLBCL dataset consisted of the gene expression values of 77 samples that were measured by high-density oligonucleotide microarrays of the two most prevalent adult lymphoid malignancies, which comprised 58 samples of diffuse large B-cell lymphomas (DLBCL) and 19 samples of follicular lymphoma (FL). Each sample contained 7129 gene expression values.  The last was the colon cancer dataset published by [46]. It contained gene expression levels of 40 tumor and 22 normal colon tissues for 6500 human genes obtained with an Affymetrix oligonucleotide array. A subset of 2000 genes with the highest minimal intensity across the samples was used. The detailed information of these datasets is summarized in Table 1.  In order to evaluate the performance of our proposed AAElastic method and to compare it with the Elastic, AElastic, and AERidge, three evaluation criteria were used depending on the testing dataset:  Classification accuracy (%) (CA)  (11)   Classification  accuracy =   TP + TN   TP + FP + FN + TN   \u00d7 100 %       Sensitivity (%) (Sen.)  (12)   Sensitivity =   TP   TP + FN   \u00d7 100 %       Specificity (%) (Spe.)  (13)   Specificity =   FN   FP + TN   \u00d7 100 %    where TP is the number of true positive, FP is the number of false positive, TN is the number of true negative, and FN is the number of false negative. Furthermore, we also performed a two-way analysis of variance (ANOVA), to show the statistical difference in the area under the curve (AUC) of the methods in the training dataset.  In order to enable a fair comparison, typically, each dataset was randomly partitioned into a training dataset, which comprised 70% of the samples, and a test dataset, which consisted of 30% of the samples. The partition repeated 50 times for each of the datasets. In order to obtain the best value of the pair  (   \u03bb   1   ,   \u03bb   2   )  , the 10-fold CV was employed using the training dataset. All the applications were conducted in R using the glmnet package. The average number of selected genes, the average classification accuracy, the average sensitivity, and the average specificity are presented in Table 2.  As can be seen from Table 2, AAElastic selected more genes than the other three methods for all the datasets. In DLBCL, for instance, AAElastic selected 61 genes compared to 54, 55, and 49 genes for Elastic, AElastic, and the AERidge, respectively. Importantly, AAElastic had the potential to select more genes than the other three methods, indicating that most of these additionally selected genes were probably not highly correlated.  In terms of classification accuracy, AAElastic achieved a maximum accuracy of 93.04%, 95.04% and 96.40% for prostate, DLBCL, and colon datasets, respectively. Furthermore, it is clear from the results that AAElastic outperformed the AERidge in terms of classification accuracy for all datasets. This improvement in classification accuracy is mainly due to the AAElastic ability in taking into account the standard error of the ridge regression. Moreover, AAElastic slightly improved the classification accuracy compared to AElastic. The improvements were 2.00%, 1.27%, and 2.29% for the prostate, DLBCL, and colon datasets.  It can also be seen from Table 2 that AAElastic has the best results in terms of the sensitivity and specificity. AAElastic has the largest sensitivity of 91.52%, 92.14%, and 92.21% for the prostate, DLBCL, and colon datasets, respectively. This indicated that AAElastic significantly succeeded in identifying the patients who in fact have the cancer with a probability of 0.915, 0.921, and 0.922, respectively.  On the other hand, the results for the specificity represent the probability of an adaptive regularized logistic regression method in identifying the patients who are normal. In terms of the specificity, AAElastic significantly outperformed the AElastic, AERidge, and Elastic for all datasets. In the prostate cancer dataset, for example, AAElastic has the largest probability of 0.928 in identifying the normal patients compared to 0.913, 0.903, and 0.907 for AElastic, AERidge, and Elastic, respectively.  To further highlight the classification stability for the proposed method, the AAElastic seeks to prove that it can classify high-dimensional cancer data with a high degree of accuracy compared to the other three methods used. Depending on the training dataset, a two-way ANOVA was used to check whether the AAElastic, AElastic, AERidge, and the Elastic were statistically significant, and if there was any significant difference between the three datasets used in terms of AUC. Table 3 reports the two-way ANOVA results. From Table 3, the results showed statistically significant differences between the AAElastic and the three other methods used in terms of AUC. In addition, we can see that the prostate, DLBCL, and colon datasets had different area under the curve values.  Furthermore, Duncan\u05f3s multiple range test was used to obtain more detailed information about the differences between the AAElastic and the three other methods used. Table 4 lists the p-value of each compared pair of methods. It is apparent from Table 4 that the AAElastic showed statistical differences compared to the AElastic, AERidge, and Elastic in terms of AUC.  Besides classification accuracy, gene selection consistency is another aspect associated with adaptive regularized logistic regression. To measure the consistency of gene selection,   Figs. 1\u20133 display the frequency of the top selected genes by the AAElastic, AElastic, AERidge, and the Elastic for prostate, DLBCL, and colon datasets, respectively.  As we can see from Fig. 1, only 11 genes were frequently selected by all methods. It is clearly seen that AAElastic showed more consistency in selecting the top genes. For example, it successfully selected the gene index (name) \u00d71074 (H.sapiens cDNA), \u00d71490 (H.sapiens ABC), \u00d7205 (H.sapiens mRNA for RET), \u00d73617 (H.sapiens GSTA4 mRNA), and \u00d74525 (hepatoma mRNA for serine protease hepsin) with probability equal to 1, while the other three methods only selected \u00d71074 and \u00d7205 with a probability equal to 1. By looking at the correlation among the 11 top selected genes from Fig. 4, the correlation between \u00d7205 and \u00d74525 was 0.568, which is not very high, but AAElastic selected these two genes together with 100%. Compared to the AElastic and AERidge, they selected these genes with a percentage of 74% and 62%, respectively. Hence, the ability of AAElastic in selected correlated genes with no high correlation can be inferred.  Similar to the prostate dataset, AAElastic provided consistent gene selection for the DLBCL dataset. Among the top 23 frequently selected genes (Fig. 2), six genes: \u00d71373 (Macrophage migration inhibitory factor (MIF)), \u00d71818 (heat shock 60kDa protein 1 (chaperonin)), \u00d74028 (lactate dehydrogenase A), \u00d74116 (ALDOA Aldolase A), \u00d7438 (T-COMPLEX PROTEIN 1), and \u00d76179 (enolase 1, (alpha)), frequently appeared together in all the methods. It is apparent that AAElastic consistently selected them with a probability of 0.96 compared to 0.84, 0.78, and 0.78 of AElastic, AERidge, and Elastic, respectively. By checking the correlation matrix from Fig. 5, we can observe that the correlations among these six genes range between 0.53 and 0.78. This could be the reason why the AAElastic selected these six genes together more frequently compared to the other three methods.  Again, from Fig. 4, we can see that the AAElastic is more consistent than the other three methods. It selected genes \u00d71772 (Homo sapiens), \u00d7249 (Human desmin gene, complete cds), \u00d7493 (MYOSIN HEAVY CHAIN, NOUMUSCLE), \u00d766 (HUMAN), and \u00d7765 (SMOOTH MUSCLE) together with a percentage of 100%. However, the AAElastic was selected \u00d783 (Human mRNA) less than AElastic, AERidge, and Elastic, where \u00d783 achieved correlation with the range between 0.53 and 0.75 with some selected genes ( Fig. 6). In contrast, AAElastic selected gene x66 with a percentage of 100%, while both AElastic and Elastic failed to selected it, although gene \u00d766 has a correlation equal to 0.52, 0.51, and 0.65 with \u00d7493, \u00d71432 (H.sapiens mRNA for p cadherin), and \u00d7249, respectively.  Overall, it is obvious that the microarray real datasets results demonstrated the use of AAElastic in terms of classification accuracy for both the training and testing datasets, sensitivity, and specificity. Additionally, it outperformed the AElastic, AERidge, and Elastic in terms of gene selection consistency. Furthermore, it is clear from the application results that for the values of the pairwise correlations, AAElastic dominates the other three methods via grouped selection.   CONCLUSION   Cancer classification is one of the most important applications in gene expression data. However, due to the high-dimensionality problem of genes, many computational methods have failed to identify a small subset of important genes. To tackle both estimating the gene coefficients and performing gene selection simultaneously, adaptive regularized logistic regression was successfully applied in high-dimensional cancer classification. In this research, we proposed AAElastic for consistent gene selection and encouraging the grouping effect simultaneously in high-dimensional cancer classification. From the results, which were based on three microarray real datasets, it was proved that AAElastic was competitive, effective, and yielded positive results in terms of (a) classification accuracy, sensitivity, and specificity, and (b) consistency in gene selection. Furthermore, it is clear from the application results that for the values of the pairwise correlations, AAElastic dominates the other three methods via grouped selection. Therefore, we can conclude the effectiveness of the proposed AAElastic method in practice.   SUMMARY   A proposed penalized method as a tool for gene selection, adjusted adaptive regularized logistic regression (AAElastic), is employed in high-dimensional cancer classification. AAElastic can perform consistency selection and deal with grouping effects simultaneously. Compared to other commonly used regularization methods, the results show that not only does AAElastic obtain the best classification ability by consistency selection, but also by encouraging the grouping effects in selecting more correlated genes.  None declared.  Supplementary data associated with this article can be found in the online version at doi:10.1016/j.compbiomed.2015.10.008.      Supplementary material        REFERENCES", "highlights": "Cancer classification and gene selection in high-dimensional data have been popular research topics in genetics and molecular biology. Recently, adaptive regularized logistic regression using the elastic net regularization, which is called the adaptive elastic net, has been successfully applied in high-dimensional cancer classification to tackle both estimating the gene coefficients and performing gene selection simultaneously. The adaptive elastic net originally used elastic net estimates as the initial weight, however, using this weight may not be preferable for certain reasons: First, the elastic net estimator is biased in selecting genes. Second, it does not perform well when the pairwise correlations between variables are not high. Adjusted adaptive regularized logistic regression (AAElastic) is proposed to address these issues and encourage grouping effects simultaneously. The real data results indicate that AAElastic is significantly consistent in selecting genes compared to the other three competitor regularization methods. Additionally, the classification performance of AAElastic is comparable to the adaptive elastic net and better than other regularization methods. Thus, we can conclude that AAElastic is a reliable adaptive regularized logistic regression method in the field of high-dimensional cancer classification."}