{"id": "S0968090X14002186", "article": "MAIN-TITLE Spatio-temporal clustering for non-recurrent traffic congestion detection on urban road networks   HIGHLIGHTS          A non-recurrent congestion (NRC) event detection method is proposed.      Two complementary criteria are introduced to evaluate an NRC detection method.      Each detected NRC can be quantified by representing it in its evolution.      London\u2019s urban road network is analysed consisting of 424 links.      Link journey times that are 40% higher than their expected values belong to an NRC.          KEYPHRASES   Spatio-temporal clustering  Non-recurrent congestion  Urban road network  Link journey time  Weighted product model   Most of the traffic operation centres and researchers classify traffic congestion into two types: Recurrent Congestion (RC) and Non-Recurrent Congestion (NRC) (Dowling et al., 2004; OECD, 2007, p. 14; Ozbay and Kachroo, 1999, p. 1; Varaiya, 2007). RC is the congestion observed at morning or afternoon peak periods, and exhibits a daily pattern. Location and duration of an RC event is usually known by regular commuters and traffic operators. It is mainly caused by excess travel demand, inadequate traffic capacity or poor signal control (Han and May, 1989). On the other hand, NRC can occur at any time of day, and its location and duration usually depends on the local conditions of the road network, as well as travel demand and traffic capacity. An NRC event is mainly caused by unexpected events like traffic accidents or vehicle breakdowns. It can also occur due to planned engineering works, special events (e.g. football matches or concerts) or inclement weather (FHWA, 2012a; Kwon et al., 2006).  The major source of travel time variability is considered to be NRC events (Noland and Polak, 2002). Occurrence of an NRC event frustrates commuters and traffic operators. FHWA (2012b) explains this by stating \u201cMost travellers are less tolerant of unexpected delays because they cause travellers to be late for work or important meetings, miss appointments, or incur extra childcare fees. Shippers that face unexpected delay may lose money and disrupt just-in-time delivery and manufacturing processes\u201d. Therefore, research is required to gain a better understanding of the causes of NRC events and how they are related to incidents. Such an understanding will provide valuable information for traffic operation centres as planned events like engineering works or social events on could be effectively managed (Hallenbeck et al., 2003a).  Most of the existing research focused on determining the total amount of annual congestion that is due to recurrent or non-recurrent congestion (Dowling et al., 2004; Skabardonis et al., 2003). Therefore, existing research have been developed to measure NRC, which limits their usefulness to detect NRC events due to the following two reasons. First, substantial amount of data including travel demand, traffic capacity, incidents and weather is required to measure NRC. However, usually, the required data cannot be feasibly collected over a large road network. Consequently, NRC is measured on a small subset of a road network. Second, existing methodologies provide a generic estimation of the total amount of NRC, which does not allow a traffic operator to study the impact of each NRC event separately.  Detecting NRC events (hereafter NRCs) on an urban road network governs two main challenges. First, evaluating the detected NRCs is difficult due to the absence of ground-truth data. In other words, there is lack of information regarding whether or not a traffic observation belongs to an NRC in reality. This challenge could be addressed by using an incident data set, as traffic incidents are regarded as the main cause of an NRC (Cheu and Ritchie, 1995; Thomas and van Berkum, 2009; Yuan and Cheu, 2003). However, it has been previously been reported that many large delays cannot be explained by an incident data set (Hallenbeck et al., 2003a). Second, detecting NRCs accurately (i.e. without missing to detect any NRC and also not including day-to-day variations in traffic to belong to an NRC) is difficult due to the heterogeneous nature of an urban road network. An urban road network is heterogeneous in terms of link lengths, which may vary from hundreds of metres to tens of kilometres. Handling day-to-day variations on short links and at the same time capturing the variations on longer links which may be due to an NRC is a challenging task. This paper solely focuses on the first challenge.  The aim of this paper is to develop an NRC detection methodology that supports the accurate detection of NRCs on large urban road networks. In order to achieve this aim, three objectives are identified. First is to develop an NRC detection method for a large urban road network. Second is to represent each NRC in its evolution, so that the spatial and temporal extent of the detected NRCs could be identified. Third is to determine criteria to evaluate different NRC detection methods. The methodology is based on Link Journey Time (LJT) data, where an LJT is the estimated journey time through a link at an established time interval. The proposed NRC detection method does not make an assumption regarding the cause of an NRC, and aims to detect NRCs regardless their cause.  This paper is organised as follows. Section 2 discusses the relevant literature on NRC detection, and spatio-temporal clustering. Section 3 describes the proposed NRC detection method. Section 4 describes the proposed criteria to evaluate the performance of an NRC detection method. Section 5 applies the proposed NRC detection methodology on London\u2019s urban road network. Section 6 presents a discussion of the proposed NRC detection methodology emphasizing on the first challenge of developing an NRC detection methodology (i.e. evaluating an NRC detection method in the absence of ground-truth data). Finally, Section 7 concludes this paper by stating the limitations of the proposed methodology and the future research directions.  The importance of NRC detection has been recognised by most traffic operation centres, yet research on this topic is recent. Even though there is lack of research on NRC detection, it is closely linked with incident detection. This is because an incident is usually assumed to cause an NRC. Thus, it is reasonable to consider that detection of an incident necessitates detection of an NRC. Therefore this section explores to what extent Automatic Incident Detection (AID) methods can be used for NRC detection. This section also identifies the relation between spatio-temporal clustering and NRC detection.  Automatic Incident Detection (AID) methods analyse the collected traffic data with the aim of detecting incidents immediately, thus lessening traffic operators\u2019 load. Since the advent of AID, a traffic operator only needs to validate the detected incidents, which enables him/her to spend less time surveilling a road network. Research efforts have resulted in a substantial number of AID methods and several surveys have been written to summarise these methods (Martin et al., 2001; Parkany and Xie, 2005; Weil et al., 1998).  Most of these AID methods have been developed for motorways (freeways), where inductive loops are commonly used to collect occupancy data. Therefore, implementing the existing AID methods for NRC detection on large urban road networks poses two limitations. Firstly, installing inductive loops across a large urban road network is not economically feasible, as inductive loops operate on a short section of a road (Hall, 2001). Secondly, a different theoretical consideration from that used for the well-developed motorway networks is required to understand traffic flows in an urban road network (Kerner, 2004, p. 582). This is because, analysis on an urban road network is more complicated due to the irregular interruptions (e.g. traffic lights, pedestrian crossings or side-street parking) and that the traffic flow is not conserved on urban road networks (Lee et al., 2011; Ozbay and Kachroo, 1999, p. 79). Therefore, new techniques should be investigated for NRC detection on large urban road networks.  Traffic congestion has previously been characterised as a clustering phenomenon, from both theoretical and empirical aspects. Theoretical aspects focus on how a cluster of vehicles moving at low speeds at critical traffic densities could cause congestion, which is referred to as a \u2018phantom jam\u2019 (Helbing, 2001; Kerner and Konh\u00e4user, 1994; Sugiyama et al., 2008). These theoretical findings are developed mainly on motorways where traffic flow is uninterrupted for long distances. Investigation of such traffic congestions in an urban road network still remains as a research challenge. Therefore, empirical research has investigated how spatio-temporal clustering can be used to detect congestion patterns in an urban road network. For example, Li et al. (2007) clustered road segments based on the density of common traffic they share. They have experimented with simulated trajectory data for this purpose. However, their research cannot differentiate whether the detected congestion patterns are due to RC or NRC.  A novel NRC detection method is proposed in this section to detect NRCs on a large urban network based on spatio-temporal clustering of substantially high LJTs. The aim of the proposed method is to identify NRCs accurately (i.e. recognition of all NRCs and no incorrect interpretations of recurrent events in traffic as NRCs) that occurred on a given date of analysis. The identified NRCs should allow a researcher to observe the development of an NRC in space and time. In this way, the dynamic nature of an NRC can be captured, and each detected NRC can be quantified based on its impact. The methodological framework is illustrated in Fig. 1 .  There are four main inputs to the developed methodology for NRC detection in urban road networks. In order to provide a complete understanding of these four inputs, the main data source of this paper should be described, which is Link Journey Time (LJT). The wide usage of Automatic Number Plate Recognition (ANPR) cameras for traffic enforcement purposes allows traffic operation centres to estimate aggregate LJT from the vehicle journey times that are obtained by matching the readings of ANPR cameras. Although the procedure to estimate LJTs is not within the scope of this paper, a brief explanation of this process could be found in Robinson and Polak (2006). Because LJT is a prominent traffic data type in network-wide analysis, this paper uses LJT as the main traffic data type for NRC detection, where NRCs are considered to be a cluster of substantially high LJTs. Thereon, a road network can be characterised by a directed graph that consists of a set of nodes (i.e. N) corresponding to ANPR cameras, and a set of arcs (i.e. A) corresponding to links. Link a is defined with two nodes, na (from) and na (to), where traffic on link a is flowing from na (from) to na (to).  The four main inputs to the proposed NRC detection method are described as follows:   Adjacency matrix (M) is the mathematical representation of a geographical road network. It is a |A | \u00d7 | A | matrix, where | A | corresponds to the number of links. The value M(a, b) denotes the adjacency relationship between the two links, a and b. This paper defines two links, a and b, to be adjacent if the ending node of link a is the beginning node of link b. The adjacency relationship is formally defined as; M(a, b)=1iffna (to)= nb (from), \u2200a, b \u220a A. In addition, a link is considered to be adjacent with itself. Formally, M(a, a)=1, \u2200a \u220a A.  The main reason to define the adjacency relation as aforementioned is related to the cause of an NRC, which is usually considered to be an incident. Because an incident decreases the capacity of the link where it occurred, the reduction in the capacity of that link could cause a vehicular queue to grow towards its upstream link (Ozbay and Kachroo, 1999, p. 8; Weil et al., 1998). Therefore, the impact of an NRC is observed first on the link where the incident occurred, and then a vehicular queue grows towards the upstream of the link.   Historic LJT data are past LJT estimates that can be used for statistical analysis like estimating the expected LJTs. As LJTs are estimated by observations collected from ANPR cameras which work in an automated fashion, there is no extra operational cost to collect historical LJT data.   Congestion factor (c) is a real valued number multiplied with the expected LJTs to determine the threshold to identify whether an LJT is excessive. It is denoted as c, where c \u2a7e1. \u2018Congestion factor\u2019 is a commonly used concept in traffic operation centres to classify LJTs into distinct congestion levels like moderate, serious or severe congestion (OECD, 2007, p. 57; TfL, 2010, p. 95; Transport Canada, 2007, p. 9). Therefore, determining the value of the congestion factor usually depends on expert knowledge.   Date of analysis determines the day on which NRCs will be detected. Any date where LJT data are collected can be used for this input. By this way, the NRCs that occurred within the analysed date would be reported to traffic operators.  Using these inputs, two important measures can be determined:  \u2022  Expected LJT is the mean travel time on a given link a at time interval t under normal traffic conditions. It is denoted as      y   \u00af     a   ( t )  and measured in minutes. Expected LJTs capture the recurrent nature of traffic. Historic LJTs are used to determine the expected LJTs, which allow a researcher to focus on NRC (Hallenbeck et al., 2003b; Varaiya, 2007).   Excess LJT is the difference between the observed and expected LJT if the observed LJT is higher than a threshold which is determined by multiplying the congestion factor with the expected LJT. Thus, if the estimated LJT is higher than the threshold, then it is excessive (substantially high). Formally, given a congestion factor c, the value of the excessive LJT at link a time interval t is defined in Eq. (1):  This section explains the proposed spatio-temporal clustering method to detect NRCs based on LJT data. Substantially high LJTs are clustered to detect NRCs. The proposed method consists of two steps; detecting episodes and clustering episodes, which are discussed in Sections 3.2.1 and 3.2.2 respectively. In Section 3.2.3, the process to determine the evolution of an NRC is introduced, which allows a researcher to identify the spatial and temporal extent of an NRC event; and an example is illustrated to clarify the concepts discussed in this section.  An episode is defined as a maximal interval on a link during which all the LJTs are excessive. Formally, an episode is defined on link a as the time intervals indexed by m and n, as shown in Eq. (2) as follows:  (2)  e { a } =    ( [ mn ] )         \u03b4   a   ( t ) > 0  \u2200 t \u2208 [ mn ] , n \u2a7e m   and       \u03b4   a   ( m - 1 ) =   \u03b4   a   ( n + 1 ) = 0   and     a \u2208 A ; t \u2208 { 1 , 2 , \u2026 , T }             The set of links is denoted with A, and the total number of LJTs within the analysis period is denoted with T. The first requirement states that during an episode all the LJTs are excessive. The second requirement states that an episode is a maximal interval of consecutive excessive LJTs. Thus, the former/latter LJT from the beginning/end of the episode is not excessive, where the episode begins and ends with the time intervals indexed by m and n respectively. The number of episodes on a link varies from zero where no LJT is excessive to T/2 where every other LJT is excessive.  Each episode can be quantified with two measures: duration and severity.  \u2022 The duration of an episode is the time lapse between the beginning and end times of the episode. It is calculated by d(e{a})=(n\u2013m +1)\u00d7 \u03c4, where m, n \u220a e{a} and \u03c4 is the time interval to estimate two consecutive LJTs in minutes. The reason for adding one is that the beginning and end times are included within the definition of an episode inclusively.  The severity of an episode is the total excess LJT that the episode exhibits. It is measured in minutes. The severity of an episode is calculated by  s ( e { a } ) =   \u2211   t = m   n     \u03b4   a   ( t )  , where m, n \u220a e{a}.  In order to illustrate the importance of the congestion factor, suppose that a traffic operation centre evaluates three different congestion factors. These congestion factors are multiplied with the expected LJTs to calculate the thresholds. These thresholds are used to identify whether an LJT is excessive or not. The thresholds defined by the high, medium and low congestion factors are denoted as \u2018High c\u2019, \u2018Medium c\u2019 and \u2018Low c\u2019 respectively in Fig. 2 . Having defined these thresholds, after applying the definition of an episode, all the episodes that occurred on a link would be identified. If the threshold is defined by using the medium congestion factor, then four episodes are identified (denoted with e1, e2, e3 and e4), as shown in Fig. 2.  Two observations can be made regarding the detected episodes:  \u2022 If the congestion factor is increased to the high congestion factor, then no episodes are detected. If the congestion factor is reduced to the low congestion factor, then the second and third episodes would merge. Therefore, the detected episodes depend on the value of the congestion factor, which is usually determined by domain knowledge.  Episodes can be prioritised based on their duration or severity or both. For example, given these four episodes, because the first episode had the longest duration and highest severity, its investigation can be prioritised amongst other episodes.  The example illustrated in Fig. 2 is based on a single link. However, an urban road network usually comprises many links, and it is possible for an NRC event to span multiple links. Therefore, an NRC detection method should be able to detect NRCs that may span multiple links.  Episodes can be used to detect NRCs that span any number of adjacent links. This is because all the LJTs that are included in an episode are much higher than their expected values, indicating that they are likely to belong to an NRC. The extent to which an LJT is high enough to belong to an episode depends on the congestion factor. Therefore, having determined a congestion factor, an episode could be considered to be similar to an NRC event. A major indication of an NRC spanning multiple links, would be to observe episodes on adjacent links at the same times. This is because the reduction in the capacity of a link (e.g. due to an incident) could cause a vehicular queue to grow towards its upstream link. These vehicular queues would be represented by several episodes occurring on adjacent links, and consequently an NRC could be described as a cluster of these episodes.  In order to detect NRCs, spatio-temporally overlapping episodes are clustered. To clarify the concept of \u2018spatio-temporally overlapping\u2019, suppose that two episodes are detected on links a and b, which are denoted with indices i and j as ei {a} and ej {b} respectively. These two episodes are spatio-temporally overlapping if they satisfy the following two conditions:  \u2022 M(a, b)=1, \u2200a, b \u220a A.  \u2203ti , tj |ti  = tj , where ti  \u220a ei {a} and tj  \u220a ej {b} and 1\u2a7d ti , tj  \u2a7d T.  The first condition states that the episodes should occur at adjacent links. The second condition states that there is at least one time interval that is common in both of the episodes. These two conditions are required to cluster two episodes. By clustering spatio-temporally overlapping episodes, NRCs that span multiple links across a time period could be detected. The pseudocode of the \u2018Clustering Episodes\u2019 method is depicted in Fig. 3 . Set E denotes all of the episodes that are detected on the entire road network across the study period by applying the Eq. (1). Set Z denotes the NRCs, where an NRC contains a cluster of spatio-temporally overlapping episodes. The pseudocode illustrated in Fig. 3 is achieved by using the \u2018pseudocode\u2019 Latex package (Kreher and Stinson, 1999).  The set of NRCs (i.e. Z) is initialised to contain no episode. The algorithm starts to search all the episodes with step (1). If the analysed episode i has already been added to an NRC, then the algorithm proceeds with the next episode. If the analysed episode has not been added to an NRC, then a new NRC k is created with the episode i being its first component with step (2). The \u2018while\u2019 loop starting with step (3) ensures that the newly created NRC k contains all the episodes that are spatio-temporally overlapping. In order to achieve this, all the remaining episodes are searched. If an episode j is found to spatio-temporally overlap with an episode contained within the NRC k (assessed with symbol  ), then the episode j is added to NRC k. This operation may extend the spatio-temporal extent of NRC k. Thus, it is necessary to search for all the episodes one more time starting with step (3) in order to ensure that the NRC k reaches its maximum spatio-temporal extent before creating a new NRC.  The NRC detection algorithm described in Fig. 3 determines all NRCs such that no two episodes that belong to different NRCs are spatio-temporally overlapping with one another. However, the detected NRCs do not inform a researcher of the temporal evolution of an NRC. The temporal evolution of an NRC allows a researcher to observe which links were congested and when in an NRC. Determination of the evolution of an NRC allows a researcher to quantify an NRC in terms of its lifetime and severity. Lifetime is the temporal extent of an NRC, and it represents the time period that an NRC contains at least one link. Severity of an NRC is the total impact of the NRC in terms of excessive LJTs. All the excessive LJTs are added throughout the lifetime of an NRC to calculate its severity.  To clarify the aforementioned concepts, suppose a three-link network as shown in Fig. 4 (a), where the direction of traffic flow is indicated with an arrow. The adjacency matrix of this network is illustrated in Fig. 4(b).  Suppose that eight LJTs are estimated for these three links, and those which are excessive are illustrated with a \u2018+\u2019 sign in Fig. 5 . After determining the excessive LJTs, three, two and two episodes are detected for links a 1, a 2 and a 3 respectively. These episodes are illustrated with orange rectangles in Fig. 5(a). Three NRCs are found by clustering these episodes based on the Clustering Episodes method described in Fig. 3. The boundaries of these NRCs are highlighted with red in Fig. 5(b).  Several observations can be made regarding this outcome:  \u2022 The shape and size of NRCs may vary. The shape and size of an NRC cluster depends on the spatio-temporal distribution of the excessive LJTs. The first NRC contains twelve excessive LJTs, whereas the third NRC contains only one excessive LJT. As this paper does not make any assumption regarding the nature of the phenomenon that resulted in an NRC, it is reasonable to observe variations in the shape and size of the detected NRCs.  The second and third NRCs are not merged into one NRC, as none of the episodes within these NRCs spatio-temporally overlap. Although links a 1 and a 2 are adjacent, the time intervals where the episodes occurred do not align; hence, they are not spatio-temporally overlapping.  The first NRC can only be detected through the usage of the \u2018while\u2019 loop in Fig. 3, as it has a concave shape.  Representing the detected NRCs in their evolution would allow a researcher to quantify their lifetime and severity. By this way, each detected NRC could be quantified. The evolutions of the aforementioned NRCs are illustrated in Table 1 .  The lifetime of the first NRC is between time intervals one and five; and the life-times of second and third NRCs are time intervals seven and eight respectively. The severity of the NRCs is calculated by adding the excessive LJTs within an NRC. For example, the severity of the second NRC would be    \u03b4     a   2     ( 7 ) +   \u03b4     a   3     ( 7 )  .  The NRC detection method proposed in Section 3 to detect NRCs based on LJT data do not make any assumption regarding the severity or duration of an NRC event. The method uses excess LJTs as the main indicator of an NRC. Naturally, different values of congestion factor would lead to different episodes, hence, different NRCs. In order to determine the most suitable congestion factor, it is necessary to evaluate the detected NRCs. Therefore, the aim of this section is to seek for an answer to the question \u2018how can the performance of an NRC detection method be evaluated?\u2019  The traditional approach to evaluate the detected NRCs would be to compare them with a ground-truth data (i.e. data indicating whether or not the estimated LJTs do truly belong to an NRC). Lack of ground-truth data is a common issue in many clustering tasks (Dubes and Jain, 1976). Therefore, researchers often propose new criteria to evaluate clustering methods considering the uses of the designated clustering task (Luxburg et al., 2012). Consequently, two criteria that complement each other are proposed within this section. These two criteria are designed to consider the practical uses of an NRC detection method. The first detects NRCs that result in substantial impact that would take a traffic operator\u2019s attention. The second relates the detected NRCs with incidents. These two criteria are referred to as the \u2018High-Confidence Episodes\u2019 and the \u2018Localisation Index\u2019 respectively.  This evaluation criterion assesses to what extent an NRC detection method detects \u2018High-Confidence\u2019 episodes (e \u2217s). A \u2018high-confidence\u2019 episode is an episode on link a that lasts for a minimum duration during which all LJTs are excessive. A \u2018high-confidence\u2019 episode is formally defined on link a between time intervals indexed by m and n in Eq. (3) as follows;  (3)    e   \u2217   { a } =    [ mn ]         \u03b4   a   ( t ) > 0  \u2200 t \u2208 [ mn ] ; m , n \u2208 { 1 , 2 , \u2026 , T }   and       \u03b4   a   ( m - 1 ) =   \u03b4   a   ( n + 1 ) = 0   and     n - m + 1 \u2a7e   t   min               The first two lines of the definition of an e \u2217 satisfies that an e \u2217 is an episode. Hence, it is a maximal interval during which LJTs are excessive. The third line differentiates an episode with an e \u2217. Specifically, an e \u2217 should occur for a minimum duration, which is denoted by t min. The continuous time of the minimum duration is t min \u00d7 \u03c4 minutes, where \u03c4 is the LJT estimation interval. The main reason for proposing this evaluation criterion is to satisfy a practical requirement, as very high LJTs that last a minimum duration demand attention from the traffic operators. Because an e \u2217 commands this attention, an NRC detection method should be able to detect the e \u2217 s that occurred on an analysed date.  Once the values of the unknown parameters of an e \u2217 are determined by historical analysis (described in Appendix A) or by domain knowledge; the LJTs that belong to the detected NRCs and e \u2217s could be compared by using a Venn diagram as shown in Fig. 6 . The number of LJTs that belong to both the e s and the detected NRCs is denoted with \u2018TP\u2019 corresponding to True Positives. The number of LJTs that do not belong to the e \u2217s, but belong to the detected NRCs is denoted with \u2018FP\u2019 corresponding to False Positives. The number of LJTs that belong to the e \u2217s but not the detected NRCs is denoted with \u2018FN\u2019 corresponding to False Negatives. Finally, the number of LJTs that neither belong to the e \u2217s nor the detected NRCs is denoted with \u2018TN\u2019 corresponding to True Negatives.  From this comparison, two important measures could be estimated. False Alarm Rate (FAR) is the proportion of all LJTs that are detected to belong to an NRC but do not belong to an e \u2217. Formally, FAR=FP/(TP+FP). False Negative Rate (FNR) is the proportion of all LJTs that belong to e \u2217 which were not included within the detected NRCs to all LJTs that belong to e \u2217. Formally, FNR=FN/(TP+FN). These two measures assess to what extent an NRC detection method identified and missed e \u2217s, which are considered to be the episodes that should ideally be detected. Of these two measures, FNR is the critical one as it determines the proportion of e \u2217s that are missed. Therefore, an ideal NRC detection method should have low FNR. On the other hand, it is possible for an NRC to last less than the minimum duration defined in the definition of an e \u2217. Hence, an ideal NRC detection model should have some FAR, but its value is unknown due to the lack of ground-truth data.  The reported NRCs should ideally be associated with the reported incidents. In order to associate an NRC with an incident, the detected NRC should consist of links that are adjacent throughout its lifetime. The \u2018Localisation Index\u2019 is an evaluation criterion that quantifies the extent to which the detected NRCs consist of link groups that are adjacent throughout their lifetime. Its calculation depends on the number of connected components that an NRC exhibits throughout its lifetime, which can be described with an example.  Suppose links a1, a2, a3 and a4 are linearly aligned where the traffic is flowing from a1 towards a4, as shown in Fig. 7 . An incident occurs at link a4 and its effect propagates towards its upstream links during the analysis period. The excessive LJTs are illustrated with a \u2018+\u2019 sign in Fig. 7, and the detected NRC is highlighted in red.  The number of connected components is found by investigating the adjacency relationship between the links that were included in the NRC. For the NRC illustrated in Fig. 7, there is only one connected component between times one and five. However, the links that are included within the connected component change over time. For example, at time one the connected component consists of only link a4, whereas at time three the connected component contains links a2, a3 and a4. These three links constitute a single connected component, because a2 is adjacent to a3 and a3 is adjacent to a4. In other words, all the links that were in the NRC at time 3 are connected. Because the detected NRC consist of a single connected component throughout its lifetime, it is possible to relate it with an incident.  On the other hand, for the NRC illustrated in Fig. 8 , it is more difficult to relate the detected NRC with an incident because the number of connected components is two, two and one for times 1, 2 and 3 respectively. The number of connected components is two for times 1 and 2 because the two links included within the NRC at times 1 and 2 (i.e. a1 and a3) are not adjacent.  Calculation of the \u2018Localisation Index\u2019 involves two steps:  1. For each detected NRC event k \u2208 {1, 2, \u2026, |Z|}, where |Z| denotes the number of detected NRCs, calculate the average number of connected components that the k th NRC exhibits throughout its lifetime. For the NRCs illustrated in Figs. 7 and 8, this number would be 1.0 (5/5) and 1.7 (5/3) respectively.  The maximum average number of connected components for all the detected NRCs is the Localisation Index.  The higher the Localisation Index is the harder to relate the detected NRCs with the reported incidents. As the average number of connected components increases, it becomes more likely that each connected component could be related to a different incident, because different connected components are not spatially connected. By this way, an NRC detection method that considers day-to-day variations in traffic to belong to an NRC is penalised. On the other hand, the minimum value of the Localisation Index is one, which is the best value of Localisation Index as the detected NRCs consist of only a single connected component throughout their lifetime.  Last, but not least, the reasoning behind the proposal of \u2018Localisation Index\u2019 is supported within the literature regarding the congestion formation on road networks. It has been reported that traffic congestion events due to an incident (e.g. obstruction) in an idealised road network (i.e. a grid network where the speeds of all vehicles are assumed to be the same) mostly consist of a single connected component. However, it is also possible that such a congestion event may split into two connected components, depending on travel demand (Wright and Roberg-Orenstein, 1999). Once the congestion is split into two connected components, relating these two connected components to the initial cause becomes more difficult (i.e. a higher Localisation Index). Recent research regarding the formation of congestion due to an incident has also focused on grid networks, and research on irregular road networks is left to future studies (Long et al., 2012).  London, the capital city of the United Kingdom, has a long history of urban development which led commuters to experience traffic congestion as early as the 1700s (British Library, 2013). The infrastructure of London consists of an underground (commonly known as \u2018tube\u2019) network covering 402km (TfL, 2013) and a road network of 13,800km (TfL, 2012). However, it still struggles to cope with increasing levels of traffic congestion, which account for almost 20% of the total UK traffic congestion (TfL, 2010, p. 86). Approximately 24.4 million trips have been made to/from or within London within a day, and 58% of these require road network transportation (TfL, 2010, p. 38). These facts illustrate the importance of the road network in London, and indeed make it very interesting case study to investigate NRCs in an urban road network.  A link is usually defined as the route between two Automatic Number Plate Recognition (ANPR) cameras with the highest traffic flow. Two links may spatially overlap with each other due to the lack of ANPR cameras at every junction. However, including spatially overlapping links within the analysis is not feasible, as it creates redundancies, which unnecessarily increases the run time. Hence, Transport for London (TfL) classifies the links into two as core or not. Core links minimise the spatial overlap between the links. Furthermore, core links are generally treated as having higher data quality regarding the estimated LJTs. Therefore, when reporting the performance of road networks, TfL relies on data collected from core links. There are 1,261 ANPR links in total, and 424 of these are core links. In order to illustrate the spatial overlap between these two networks, it is useful to illustrate them on a map as shown in Fig. 9 . Most of central London is covered with core links and most of the non-core links are actually outside central London. As TfL uses core links for their road network performance monitoring, we have also used the core ANPR network. Hereafter \u2018link\u2019 indicates a \u2018core link\u2019.  ANPR links display different lengths and characteristics. Link lengths vary from 175 to 13,487m, and the total length of the network is around 1250km. Links also differ with respect to the traffic flow direction, the number of lanes they contain and the number of junctions through which they pass.  Traffic operators commonly use expected LJTs for road network performance monitoring. This paper uses the expected LJTs that are used in TfL. By this way, it is aimed to have consistency between the results obtained within the analysis of this paper and the practice. TfL classifies the days of a week into two: weekdays (Monday to Friday) and weekends (Saturday and Sunday). For weekdays, expected LJTs are estimated by taking the mean of all the LJTs that happened on weekdays of March 2009 and no data cleaning procedure is applied. For weekends, approximately two months (March and April 2009) of data are used to estimate LJTs. The estimated expected LJTs are currently being used at TfL, as domain experts consider them to reflect the normal (recurrent) traffic conditions.  The analysis in this paper is conducted on the weekdays of October 2010. A tube strike occurred on 4 October, which increased LJTs considerably across the entire road network (Tsapakis et al., 2013). In order to preserve the integrity of the analysis, 4 October has been removed. Hence, a total of 20 weekdays are analysed. The analysis has been conducted between 07:00 and 19:00, as this time interval covers the AM/Inter/PM peak periods in London (TfL, 2010, p. 268). There are a total of 145 LJTs within the analysis period, as each LJT is estimated every \u03c4 =5min (12h\u00d712 LJTs/hour+1, where one is added as the analysis period is inclusive of 07:00 and 19:00).  The main parameter for using the Clustering Episodes method (denoted as \u2018CE\u2019) is the congestion factor. Five different congestion factors are tested (i.e. 1.2, 1.4, 1.6, 1.8 and 2.0) in accordance with the values which are currently being used at TfL (2010, p. 95). These five NRC detection models are compared in the following aspects: firstly, the numbers of detected NRCs are compared. Because the correct number of NRCs is not known due to the lack of ground-truth data, the number of NRCs is not a performance criterion. Nevertheless, considering that traffic operators have a fixed allocated time to analyse the detected NRCs, the higher the number of NRCs, the less time to investigate each NRC. On the other hand, the lower the number of NRCs is the higher the likelihood to miss some NRCs. In addition, run-time of these different models is also compared in order to demonstrate the practical feasibility of the proposed method to detect NRCs (analyses are conducted on a workstation running Windows 7 with a 12GB RAM and 3.2GHz processor; and the code is written in Matlab R2012a. The code would be supplied to interested readers upon request). Finally, these models are compared based on the evaluation criteria described in Section 4. In the results, the number associated with each Clustering Episodes (\u2018CE\u2019) model denotes the congestion factor used to determine whether or not an LJT is excessive in that model.  The boxplot of the numbers of NRCs detected by the five different CE models are illustrated in Fig. 10 . The horizontal axis represents different CE models, whereas the vertical axis represents the number of NRCs. The red horizontal lines in the boxes refer to the median value of the number of NRCs on the analysed dates, while the upper and lower boundaries of the boxes indicate the upper and lower quartiles (i.e. 75th and 25th percentiles, and denoted as Q3 and Q1) respectively. The black lines vertical to the boxes (i.e. whiskers) show the maximum and minimal values that are not outliers. Outliers are indicated with a \u2019+\u2019 sign. The value of an outlier exceeds 1.5 times the interquartile range (i.e. Q3\u2013Q1). The arbitrary value of 1.5 has been commonly used to identify outliers since the advent of boxplots (Tukey, 1977).  The main outcome is that decreasing the congestion factor leads to more NRCs. Initially, this may appear to be trivial as more excessive LJTs would be generated by decreasing the congestion factor; hence, more NRCs. However, it is also possible to merge distinct NRCs, as the likelihood to include day-to-day variations to belong to an NRC increases by decreasing the congestion factor. Another interesting finding is that even when the most conservative model is considered (i.e. CE=2.0), there are approximately 350 NRCs within 07:00 and 19:00. Large number of NRCs that occur within a day indicates the necessity of a dedicated team in a traffic operation centre to investigate these NRCs.  The boxplot of the run-times of the five CE models are illustrated in Fig. 11 , where the horizontal axis represents the CE models. Vertical axis represents the natural logarithm of the run-times in seconds in order to improve the legibility of the results.  Results indicate that the conservative models are much faster than the liberal models. This is reasonable as there would be fewer episodes to cluster in conservative methods; hence, faster run time. The results indicate the practical feasibility of the proposed method, as the CE\u2a7d1.6 models usually complete the analysis less than one minute (i.e. e 4 <60). Only when the most liberal method (CE=1.2) is used, the average run-time increases to approximately one hour.  This section evaluates the detected NRCs based on the two criteria proposed in Section 4. Specifically, \u2018high-confidence episodes\u2019 and \u2018Localisation Index\u2019 are evaluated for the aforementioned NRC detection models.  The empirical evidence provided in Appendix A suggests that a high-confidence episode (e \u2217) occurs on a link when the estimated LJTs are at least 40% higher than their expected values for at least a minimum duration of 25min. High-confidence episodes are considered to be sufficient to attract a traffic operator\u2019s attention, as all the high-confidence episodes would account to the majority of the total delay. Having determined all e \u2217s, this performance criterion assesses to what extent an NRC detection model detected e \u2217s. The more an NRC detection model detects e \u2217s (i.e. low FNR), the better it is.  The average False Negative Rate (FNR) is plotted against the average False Alarm Rate (FAR) in order to illustrate the relationship between the e \u2217s that are detected and missed by different NRC detection models. The FAR and FNR values for each NRC detection model are calculated for all the analysed dates, and these values are then averaged and the results are illustrated in Fig. 12 .  The best models regarding the detection of e \u2217 s are the CE models that use a congestion factor of less than or 1.4 (which is the congestion factor used to define an e \u2217). As can be seen in Fig. 12, the CE=1.4 and CE=1.2 models detected all the e \u2217s, leading to an FNR of zero. This is because the underlying principle used to detect NRCs by CE and e \u2217s relies only on excessive LJTs. Unlike the e \u2217, however, CE models do not make an assumption regarding the duration of an NRC. Therefore, CE models detect NRCs that are not e \u2217s; hence, those models have an FAR value that is greater than zero. It is important to stress one more time that it is reasonable for an NRC detection model not to make an assumption regarding the minimum duration of an NRC, as there could be, for example, accidents which are cleared quickly, yet result in an NRC. Therefore, an ideal NRC detection model should have some FAR when compared with e \u2217s. However, the optimum value of FAR is unknown due to the lack of ground-truth data. On the other hand, increasing the congestion factor for values greater than 1.4 increases the amount of e \u2217s that are missed. For example, when the congestion factor is set to 2.0, approximately 62% of e \u2217s are missed.  Localisation Index values for all the CE models have been calculated for all the analysed dates by applying the procedure described in Section 4.2. The box-plot of the Localisation Index values is illustrated in Fig. 13 .  The results generally validate the theoretical expectation: liberal ways of detecting NRCs result in a worse performance in the \u2018Localisation Index\u2019. This can be observed when the median of the \u2018Localisation Index\u2019 values are investigated. Localisation Index values gradually increase as the congestion factor decreases. This is because the likelihood to consider day-to-day traffic variations to belong to an NRC increases as the NRC detection model becomes more liberal. Deeming day-to-day variations in LJTs to belong to an NRC may result in the merging of two or more distinct NRCs into a single NRC. Therefore, such liberal ways of detecting NRCs are not preferred in terms of relating detected NRCs with incidents.  It should be highlighted that the \u2018Localisation Index\u2019 may also decrease if NRCs are detected in a more liberal way. For example, the maximum value of the \u2018Localisation Index\u2019 for the model CE=2.0 is 3.51, which is actually detected as an outlier amongst the other dates. On the other hand, the maximum \u2018Localisation Index\u2019 for the model CE=1.8 is 3.37. This highlights the possibility that the additional excessive LJTs might merge previously distinct connected components. To clarify, consider the example illustrated in Fig. 8. If the LJT at link a2 at time 2 becomes excessive when the congestion factor is decreased, then the average number of connected components of the NRC will decrease. The results, however, indicate that this is a rare situation, and most of the time liberal ways of detecting NRCs lead to a higher Localisation Index.  The previous two subsections investigated the performance of the different NRC detection models. As the congestion factor increases, the performance of the model degrades regarding the detection of high-confidence episodes; whereas the performance increases regarding the Localisation Index. Therefore, in order to determine the most suitable congestion factor, these two conflicting criteria should be combined into one criterion.  Multi-Attribute Decision Making (MADM, also referred to as multi-criteria decision making) provides the necessary theoretical background to compare different models (also referred to as \u2018alternatives\u2019) based on two or more, often conflicting, criteria. The aim of such a comparison is to determine the best-performing model. MADM consists of many methods, including, but not limited to, Weighted Sum Model (WSM), Weighted Product Model (WPM) and Analytic Hierarchy Process (AHP) (Triantaphyllou and Mann, 1989). The main issue, however, is that it is very difficult to choose the most suitable method from amongst these different MADM methods. This is because such a decision may yield rank reversals, which means that the introduction of a new model that is inferior to the existing models may alter the decision regarding the best-performing model (Leskinen and Kangas, 2005; Zanakis et al., 1998). This is a serious limitation, because it prevents a researcher to correctly identify the strengths and limitations of different NRC detection models with respect to each other. Consequently, identifying the best MADM method is considered to be a paradox (Triantaphyllou and Mann, 1989).  It has been reported that WPM is not susceptible to rank-reversals due to its structure (Triantaphyllou and Mann, 1989), which relies on a pair-wise comparison of different models. For a given criterion, the ratio of the values belonging to the two compared NRC detection models is calculated. The weight for each attribute is considered as an exponent to the corresponding ratio. The calculated ratios are multiplied to obtain the Final Score, which is illustrated by the Eq. (4) as follows:  (4)  Final Score (   S   K   ,   S   L   ) =    \u220f   j \u2208 {   e   \u2217   ,Localisation Index }      (   S   kJ   /   S   Lj   )     w   j     ,where    \u2211   j      w   j   = 1   where SK and SL are the two NRC detection models and j denotes a criterion which can be either the False-Negative Rate in e \u2217 or the Localisation Index. SKj and SLj denote the values of the jth criterion of Kth and Lth NRC detection models respectively, and wj is the weight (i.e. the relative importance) of criterion j. Having determined all Final Score values, the best model is the one that has the smallest Final Score, because the smaller the values of both of the criteria, e \u2217 and the Localisation Index, the better the NRC detection model. Once all NRC detection models are compared with one another, they can be ranked based on their Final Score values.  The WPM method is a simple, yet useful way of comparing different NRC detection models. An important advantage of WPM is that it provides a dimensionless analysis, as it relies on ratios. In this way, attributes with different units could be combined in one measure. On the other hand, as it relies on the calculation of a ratio, it assumes that none of the values of a criterion could be zero. This assumption, however, is not satisfied within this papers\u2019 context, because the FNR values of the two Clustering Episode models (i.e. CE\u2a7d1.4) is zero. Recent studies explicitly indicate that there is no published research evidence to handle this issue (Mela et al., 2012). Because WPM does not suffer from rank reversal, this paper relies on WPM to compare different NRC detection models with the following adjustment: all the models\u2019 FNR values are incremented starting from 0.01 to 0.1 for all the analysed dates, by considering equal weights for both of the proposed criterion (i.e.    w     e   \u2217     =   w   Localisatation Index   = 0.5  ). The median of the calculated Final Scores for the analysed 20 weekdays of October 2010 are illustrated in Fig. 14 .  The following conclusions can be drawn by analysing the Final Scores. Firstly, the most suitable congestion factor is found to be approximately 1.4, as the CE=1.4 model resulted the least Final Score amongst the other models. This suggests that whenever an estimated LJT is higher than 40% of its expected value, then it should belong to an NRC. The difference between CE=1.4 model and the rest decrease when the incremented value increases. This is a theoretically plausible outcome because whenever the same increment is added both to the numerator and denominator, the resulting ratio would be closer to one. In other words, the relative effect of FNR on the Final Scores would gradually decrease with the incremented amount. Due to this very reason, CE=1.2 performs better than CE=1.6 for increment values less than 0.08. When the incremented value is greater than 0.08, CE=1.6 performs better than CE=1.2 because the Localisation Index becomes relatively more important in the calculation of the Final Score. Note that, the Final Scores of CE=1.2 is always one, because it is considered to be the reference model in which all the models are compared with respect to CE=1.2. However, the ranking of the models would be the same regardless the chosen reference model or if we add another model (e.g. CE=1.7) to the existing models as the Final Scores are calculated based on WPM.   DISCUSSION   An important challenge of an NRC detection methodology is to evaluate a method in the absence of ground-truth data. Specifically, a researcher usually does not know the true spatial and temporal extent of NRCs. In order to understand why there is lack of ground-truth data, it is necessary to discuss the three main ways to obtain it:  \u2022  Manual labelling a domain expert can manually label the traffic data and identify the links and times that experienced NRC. Previously, Guralnik and Srivastava (1999) relied on manual labelling to generate ground-truth data for the purpose of detecting the times when traffic patterns change based on traffic flow data. However, they noted that experts may have disagreements regarding how they label the data. Sen et al. (2012) also rely on expert knowledge to generate ground-truth data manually for the task of categorising traffic patterns into two, namely free-flow and congested. However, their research was limited to a single road. This is because manual labelling of traffic data is usually a labour intensive task, which prevents its usage on a large network.   Using traffic simulations simulations use traffic models to generate artificial traffic data. Traffic simulations, however, often make simplistic but unrealistic assumptions like limiting the number of incidents that can occur on a link (Emmerink et al., 1995). In commercial simulation packages, which are usually more acceptable in the scientific domain, these assumptions are even unknown to users. This, in turn, might lead to two simulation packages reporting different outcomes for the same purpose (e.g. estimating travel times in Bloomberg and Dale (2000)). Furthermore, there is a lack of research on modelling a large urban road network by a simulation framework that is capable of generating NRCs.   Collecting it as a separate data set Research on Automatic Incident Detection (AID) is a good example of this, because researchers use an incident data set as ground-truth data to evaluate the performance of their AID methods (Martin et al., 2001). Although incident data sets are commonly used to evaluate the performance of AID methods, they have also been reported to have inconsistencies, especially regarding the beginning and end times of incidents (\u0160ingliar and Hauskrecht, 2009). This suggests that even the ground-truth data may not be completely reliable.  Consequently, there is no feasible way of collecting the ground-truth data regarding the NRCs for a large urban road network. Nevertheless, the proposed two novel evaluation criteria have overcome this limitation, and demonstrated their effectiveness on a large urban road network as they complement each other. Specifically, liberal NRC detection models perform better regarding the detection of high-confidence episodes; whereas they perform poorly regarding the Localisation Index and vice versa for the conservative models. This outcome is theoretically plausible as the aim of NRC detection method is to detect all the NRCs that have occurred within a day without including day-to-day variations in traffic to belong to NRCs.  Even though this paper has only analysed London\u2019s urban road network, the proposed evaluation criteria are generic as long as the following requirements are met:  \u2022 Link Journey Time (LJT) data are available.  The analysed road network could formally be described with an adjacency matrix (M) which is not a diagonal matrix. Formally, for the links a and b, M(a, b)=1, a \u2260 b \u2203 a, b \u220a A, where A denotes the set of links.  An NRC could be observed on several adjacent links.  These requirements may not always be met in real-life scenarios. For example, it might be possible that a researcher or traffic operation centre has only access to data obtained from inductive loop detectors. In this case, because we do not have LJT data, it would not be possible to determine the performance of an NRC detection method regarding the detection of high-confidence episodes. Another case would be the situation where the adjacency matrix is represented by a diagonal matrix. This situation might occur when we are dealing with a sparse road network where links are not connected with each other. In this situation, Localisation Index criterion would be meaningless, because all the NRC detection methods would appear to perform the best regarding this criterion (i.e. the Localisation Index values would be one for all the NRC detection models). Nevertheless, these requirements are not very strict would likely be met in a traffic operation centre managing an urban road network. Therefore, the proposed NRC detection methodology could readily be adapted by a researcher or an urban traffic operation centre.   CONCLUSIONS   Accurate detection of NRCs in large urban road networks is gaining importance due to the growing necessity to manage urban road networks more effectively. This paper presents an initial attempt to address this necessity, and considers NRC detection as a task of spatio-temporal clustering, where substantially high LJTs are clustered. The proposed method clusters spatio-temporally overlapping episodes to detect NRCs. All of the detected NRCs are quantified based on their impact consisting of two characteristics: lifetime and severity. Lifetime of an NRC is its temporal extent, and represents the time period that the NRC contains at least one link. Severity of an NRC is used to determine the impact of the NRC in terms of the total excess LJT it exhibits. These two characteristics are obtained by representing each detected NRC in its evolution. In this way, traffic operators could observe the dynamics of an NRC, including the development of an NRC on a road network and in time. In this way, detected NRCs could be quantified, hence their investigation could be prioritised.  The proposed NRC detection method relies on a single parameter, which is referred to as the \u2018congestion factor\u2019. The usage of a single parameter to identify congestion levels is a common approach in urban traffic operation centres. Therefore, the proposed NRC detection method is compatible with the domain expertise. Different values of this parameter, however, would lead to different NRCs. Therefore, a formal process is required to evaluate the detected NRCs. Due to the lack of ground-truth data, two criteria are proposed considering the uses of NRC detection. The first is to detect NRCs that result in substantial impact. The second is to be able to relate the detected NRCs with incidents. The terms \u2018High-confidence episode\u2019 and \u2018Localisation Index\u2019 are used to refer to these two criteria. The analyses are conducted on London\u2019s urban road network. This road network consists of 424 links, and analysis on such a large urban road network is hardly matched in previous studies. The congestion factor values are varied between 1.2 and 2.0, and these values are determined based on the domain knowledge. The most suitable congestion factor is found to be approximately 1.4 by weighting both of the evaluation criteria equally in a Weighted Product Model (WPM). This finding suggests that, whenever an estimated LJT is at least 40 percent higher than its expected value, then it should belong to an NRC.  There are several limitations of the proposed NRC detection methodology. First, the proposed NRC detection method is indifferent to the variations in the expected LJTs, as it only considers the expected LJTs and the congestion factor to identify excessive LJTs. Considering a single congestion factor across the entire road network may lead to the detection of NRCs that do not truly have a physical significance. For example, day-to-day variations in short links (e.g. variations in the timings of a traffic signal) might be considered to belong to an NRC. Second, the way in which spatio-temporally overlapping episodes is defined may prevent a direct matching of the detected NRCs with incidents. Specifically, if two or more episodes are spatio-temporally overlapping, then they are assumed to belong to the same NRC, even though their underlying causes are different. Furthermore, the flows on a road network are not considered. An incident happening at one link might show its effect at its adjacent link after a time lapse. During the elapsed time the adjacent link might become congested, whereas the link where the incident occurred becomes decongested. As a result, an episode could be observed on both links individually that are not spatio-temporally overlapping. These episodes would be considered to belong to different NRCs, but actually these NRCs are due to a single cause.  There are also limitations regarding the proposed evaluation criteria. First, all high-confidence episodes are considered to be equal in terms of their contribution to the False-Negative Rate (FNR), regardless their severity. Specifically, missing two e \u2217s that have the same duration, but different severities causes the same effect towards the calculation of FNR. Second, the Localisation Index criterion is indifferent to the order of occurrence order of the number of connected components.  In order to explore the full potential of NRC detection on large urban road networks, this paper could be extended in the following directions. First, new NRC detection methods should be investigated that considers the statistical properties of the estimated LJTs. In this way, the heterogeneous nature of an urban road network (e.g. due to differences in link lengths or data quality) might be captured more effectively. This research direction would also necessitate investigating further evaluation criterion, which would be used to assess the extent to which an NRC detection method captured the heterogeneous nature of an urban road network. Second, the way in which the adjacency matrix is defined could be improved by either incorporating domain knowledge or by developing a more systematic approach that considers link lengths. Finally, it would be very interesting to apply the proposed methodology on different urban road networks and investigate how the results vary on different urban road networks. The vision of this future research direction is to develop a formal traffic theory to explain NRCs on an urban road network. In this way, NRCs could possibly be simulated on a large urban road network, which would allow researchers to evaluate the effectiveness of their NRC detection methods in a more comprehensive way.   ACKNOWLEDGEMENTS   This work is part of the STANDARD project \u2013 Spatio-Temporal Analysis of Network Data and Road Developments (standard.cege.ucl.ac.uk), supported by the UK Engineering and Physical Sciences Research Council (EP/G023212/1) and Transport for London (TfL). The first author thanks to Hacettepe University and the Council of Higher Education of Turkey for the PhD scholarship they provided. The contents of this paper reflect the views of the authors, who are responsible for the facts and the accuracy of the data presented herein. The contents do not necessarily reflect the official views or polices of TfL.  This appendix is dedicated to explain the historical analysis conducted to determine the values of the unknown parameters within the definition of a high-confidence episode (e \u2217) as discussed in Section 4.1. Two parameters are required to define an e \u2217. The first is congestion factor c, which determines whether or not an LJT is excessive, and the second is the minimum duration (t min), which ensures that the episode lasted for a substantial amount of time.  Link Journey Times (LJTs) within a \u2018high-confidence\u2019 episode are substantially high enough to attract a traffic operator\u2019s attention. This part aims to provide empirical evidence to quantify what is meant by \u2018substantially\u2019. In this way, the congestion factor that is used to define a \u2018high-confidence\u2019 episode can be estimated.  This paper considers an LJT to be excessive enough to belong to an e \u2217 if it is greater than or equal to its 95th percentile value. This is because 95th percentile values are commonly used in traffic science to indicate unusually high traffic data (Pu, 2011). Once the 95th percentile LJTs are determined, then the congestion factor that would classify them as excessive can be determined. The average of such congestion factors throughout the entire study area is used as the congestion factor to define an e \u2217. This process is summarised as follows:  1. Calculate the 95th percentile of LJTs using a historic data set. The 95th percentile LJT for link a at time interval t is denoted as    y   a   95   ( t )  , which is calculated by linear interpolation as follows (Matlab, 2013). Collect historical ya (t) and sort them in ascending order, and denote this data vector as H, where H(1) and H(h) denotes the lowest and highest ya (t) in the historical data respectively. Find the rank of the 95th percentile in H by, r =(0.95\u00d7 h)+0.5. Hereafter,    y   a   95   ( t )  is calculated as the linearly interpolated value between H(r) and H(r +1), depending on the decimal of (0.95\u00d7 h)+0.5.  Calculate the congestion factor for link a at time interval t that would classify the 95th percentile LJT as excessive. Formally,    c   a   95   ( t ) =   y   a   95   ( t ) /     y   \u00af     a   ( t ) .     Repeat steps 1 and 2 for each LJT, a \u2208 A and t \u2208 [1, 2,\u2026, T], where A denotes the set of links and T denotes the total number of LJTs within the analysis interval.  The median of the    c   a   95   ( t )  values is considered to be the congestion factor that is used to define an e \u2217.  These four steps are applied between 07:00 and 19:00 in London\u2019s urban road network consisting of 424 links. Each LJT is estimated every 5min, so the analysis interval contains T  = 145 LJTs (12h\u00d712 LJTs/hour+1). The    c   a   95   ( t )  values are estimated using all the weekdays between 1st April and 1st August 2010, which constitutes 87 LJTs (i.e. h =87). The box-plot of the estimated    c   a   95   ( t )  values with respect to time is illustrated as shown in Fig. 15 .  These results illustrate that the    c   a   95   ( t )  values varies spatially and temporally. Spatial variation is, however, more substantial than the temporal variation. This is an expected outcome, as some links may experience long-term engineering work during the data collection interval, which would result in high 95th percentile LJTs; hence, higher    c   a   95   ( t )  values. This paper makes the simplifying assumption to use a single congestion factor to define a \u2018high-confidence\u2019 episode. This value is estimated by taking the median of the    c   a   95   ( t )  values that are calculated for all links at all times between 07:00 and 19:00. Median is used instead of mean as it is a more robust estimator of central tendency than mean. The median of the    c   a   95   ( t )  values for all the links across all the times between 07:00 and 19:00 is found to be approximately 1.4. This suggests that whenever an LJT is 40 percent higher than its expected value, then it should be considered as excessive.  The second parameter in the definition of a \u2018high-confidence\u2019 episode is the minimum duration (t min). This parameter ensures a \u2018high-confidence\u2019 episode contains a number of consecutive LJTs that are excessive, implying persistency within the episode. This part aims to determine the t min by using the information obtained from all the episodes during the analysis interval. In order to achieve this, the following question is asked: \u2018what is the maximum value of the duration of an episode in which the severity of all episodes with durations greater than or equal to this value accounts for the majority of total delay?\u2019 Therefore, all the episodes that last at least t min time intervals would correspond to the majority of total delay.  This part builds upon the previous part (i.e. using a congestion factor of 1.4 to determine excessive LJTs) and applies the following steps to determine t min:  1. Find all the episodes within the analysis interval of T =[07:00 19:00].  Calculate the duration and severity of all episodes. The duration is measured in terms of the number of consecutive LJTs that the episode contains. Severity is the total excess LJT of the episode and it is measured in minutes.  Sort all the episodes based on their duration in descending order, so that  d (   e   i   ) \u2a7e d (   e   i + 1   )  , where d(ei ) is the duration of the ith episode.  Find the duration of the episode when the cumulative severities of the episodes explain the majority of the total delay. Formally, find d(ex ), where  x =   min   x   (   \u2211   i = 1   x   s (   e   i   ) / \u03a8 ) \u2a7e 0.5  , and \u03a8 is the total delay observed during the analysis period and s(ei ) is the total severity of the ith episode.  The d(ex ) is the minimum sufficient duration that is required to account for the majority of the total delay during the analysis period.  Apply the steps 1\u20134 for all the days between 1 April and 1 August 2010 and record all the d(ex ) values for different days. The histogram of these d(ex ) values for the analysis period is illustrated in Fig. 6.  The most noticeable outcome in Fig. 16 . is the substantial decrease in the number of days with a d(ex ) value greater than 7. These dates are highly congested; for example, the 25th May 2010, State Opening of Parliament, is one such date. However, because congested days do occur, all of the 87 weekdays between 1 April and 1 August are considered to determine the minimum duration. It is found that, on average, episodes that last at least five consecutive time intervals account for the majority of the total delay (t min =5). Since each LJT is estimated every five minutes, the minimum duration to define an e \u2217 is determined to be 5\u00d75=25min.  These findings suggest that a \u2018high-confidence\u2019 episode (e \u2217) occurs when a link\u2019s estimated LJTs are 40% (i.e. c  = 1.4) higher than their expected values for a minimum of 25min.   REFERENCES", "highlights": "Non-Recurrent Congestion events (NRCs) frustrate commuters, companies and traffic operators because they cause unexpected delays. Most existing studies consider NRCs to be an outcome of incidents on motorways. The differences between motorways and urban road networks, and the fact that incidents are not the only cause of NRCs, limit the usefulness of existing automatic incident detection methods for identifying NRCs on urban road networks. In this paper we propose an NRC detection methodology to support the accurate detection of NRCs on large urban road networks. To achieve this, substantially high Link Journey Time estimates (LJTs) on adjacent links that occur at the same time are clustered. Substantially high LJTs are defined as those LJTs that are greater than a threshold. The threshold is calculated by multiplying the expected LJTs with a congestion factor. To evaluate the effectiveness of the proposed NRC detection method, we propose two novel criteria. The first criterion, high-confidence episodes, assesses to what extent substantially high LJTs that last for a minimum duration are detected. The second criterion, the Localisation Index, assesses to what extent detected NRCs could be associated with incidents. The proposed NRC detection methodology is tested for London\u2019s urban road network. The optimum value of the congestion factor is determined by sensitivity analysis by using a Weighted Product Model (WPM). It is found out those LJTs that are at least 40% higher than their expected values should belong to an NRC; as such NRCs are found to maintain the best balance between the proposed evaluation criteria."}