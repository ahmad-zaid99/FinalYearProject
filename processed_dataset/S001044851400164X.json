{"id": "S001044851400164X", "article": "MAIN-TITLE Retrieval of non-rigid 3D shapes from multiple aspects   HIGHLIGHTS          Multiple aspects are considered for non-rigid 3D shape retrieval.      Two distinctive viewpoints are considered for shape representation.      A new retrieval optimization approach is proposed.      A shape filtering algorithm is designed to remove the junk shapes.          KEYPHRASES   Non-rigid shape retrieval  Multiple aspects  Shape representation  Retrieval optimization  Shape filtering   With the wide application of non-rigid 3D shapes in fields like biomedical, chemistry, and film and game industry, the non-rigid 3D retrieval technology turns out to be quite important in reducing shape construction cost. Isometric variation is one of the most typical non-rigid deformations, which can change the shape of surfaces without tearing and stretching, like bending. Besides, there exist some other shape variations, such as scaling and noise, which bring about more difficulties for shape recognition. Hence, how to realize invariant shape representation is a challenging topic\u00a0 [1\u20134].  One of the major goals of pattern recognition and machine intelligence is the development of efficient methods for shape description or the creation of shape signature to capture shape properties which could distinguish it from shapes belonging to the other classes. Recognition and retrieval in low-level feature space is a basic issue and lots of methods have been proposed [4\u201311]. By studying the prior works, we focus on three aspects for non-rigid 3D shape retrieval, including shape representation, retrieval optimization and shape filtering, where the first one is a basis and the other two are related to retrieval strategies by using additional information besides features (e.g.\u00a0prior knowledge or latent information). Based on these considerations, we study related techniques for non-rigid 3D shape retrieval.  The main research contents of this article are illustrated in Fig.\u00a01 , which consists of the following three topics.  (a) Shape representation. We first study feature extraction and matching methods for non-rigid shape description and retrieval, where two distinctive (local and global) features are studied and evaluated for k -nearest neighbor (KNN) retrieval without using extra information independently.  Retrieval optimization. In feature space, a universal and error-free descriptor is hard to realize, which may inevitably lead to incorrect feature distances. To alleviate the deficiency in retrieval, distance learning based on the averaged local paths (e.g. Q\u2192A\u2192D) is promising to reorder the results. An optimized result is illustrated in the green circle of Fig.\u00a01(b). Though X appeared in the initial retrieval results of Q, it is far from the others (i.e.\u00a0A, B and C) which are close to D. Then, by averaging the local paths, the distance d  ( Q , X )  would become larger while d  ( Q , D )  would become smaller even if there exists a long path (i.e.\u00a0Q\u2192D), which finally results in the removal of X and addition of D.  Shape filtering. Usually, the results of each retrieval can be classified into three types: fully similar shapes (FSS) from the same class, partially similar shapes (PSS) from another class, and fully irrelevant shapes (denoted as junks). In the low-level feature space, it is hard to distinguish FSS and PSS because of feature errors or discriminative ability. But, one thing is certain that the visual difference between FSS and junks is high. Thus, it is feasible to remove the junks from the retrieval results for improvement. The operation does not impose explicit restrictions on PSS because some of the results would provide users with more possibilities for shape reusage or some other useful suggestions.   RELATED WORK   Spectral shape analysis methods have exhibited the efficiency in non-rigid 3D shape analysis. Reuter et\u00a0al.\u00a0 [12] used the truncated sequence of the eigenvalues of Laplace\u2013Beltrami Operator (LBO) as isometry-invariant shape descriptors dubbed shapeDNA. L\u00e9vy et\u00a0al.\u00a0 [13] pointed out that the eigenfunctions of the Laplace\u2013Beltrami differential operator could understand geometry by analyzing the Chladni plates, which has resulted in many useful applications. Then, Rustamov\u00a0 [14] created an interesting global point signature (GPS) by using the eigenvalues and the eigenfunctions of LBO. Although GPS is isometric invariant, it suffers from the problem of eigenfunctions switching when the eigenvalues are close to each other\u00a0 [5]. The problem was lately well handled by heat kernel related methods.  Recently, local features based on heat kernel are widely researched for non-rigid shapes. To cope with the redundance of heat kernel, Sun et\u00a0al.\u00a0 [5] proposed a typical descriptor called heat kernel signature (HKS) with several interesting properties, including multi-scale, concise, robust and isometric invariant. Bronstein et\u00a0al.\u00a0 [4] extended HKS to a scale-invariant version (denoted as SIHKS) for shape retrieval. Although HKS and SIHKS can deal with different shape transformations\u00a0 [4], they performed not as ideal as expected for isometric shapes [1,2]. Then, Aubry et\u00a0al.\u00a0 [7] proposed wave kernel signature (WKS) from another viewpoint to give rise to more accurate matching than HKS, and Fang et\u00a0al.\u00a0 [15] designed a temperature descriptor (TD) for shape retrieval. For further research, many new technologies are incorporated for performance promotion, such as collaborative classification\u00a0 [2] and sparse representation\u00a0 [16]. It is important to note that the above methods are scale dependent and various scales were used. In fact, the (automatical) scale selection is still a difficult problem because (1) the scale range of different shapes is usually unknown, and (2) the same scale maybe not suitable for different parts of a shape due to local differences. Thus, the existing universal scale selection method may be inappropriate for precise shape representation.  In addition, global feature based on intrinsic distance is another research point. Elad and Kimmel\u00a0 [17] introduced a bending invariant method using geodesic distance and Smeets et\u00a0al.\u00a0 [18,1] extended it by a modal representation with a better shape retrieval performance. Liu et\u00a0al.\u00a0 [19] proposed a geodesic-contour-driven tree skeleton extraction method, which was insensitive to surface noise and performed well in the experiment. In spectral embedding space, the diffusion distance (DD)\u00a0 [20,21] and commute time distance\u00a0 [22] are widely considered for shape analysis due to their robustness. Mahmoudi et\u00a0al.\u00a0 [23] proposed to use diffusion distance with a fixed scale for point cloud shape retrieval. In\u00a0 [24], diffusion distance was employed for molecular shape retrieval. In\u00a0 [3], both diffusion distance and commute time distance were discussed for non-rigid shape retrieval, and then an aggregation method in diffusion distance scale space was introduced for further promotion.  Beyond the considerations in shape features, another important aspect for retrieval is to optimize the retrieval results by making full use of the latent structure information in the dataset, which adopts the viewpoints not only from the query shape but also the other shapes relating to it. Affinity propagation (or diffusion) using iterative random walks is one of the most popular solutions and it has produced significant effects in several computer vision and graphics applications. In\u00a0 [8], the authors introduced a self-smoothing (SSO) operator to improve a given similarity result, which has resulted in significant performance gains. Then, Wang et\u00a0al.\u00a0 [10] proposed an extended method of\u00a0 [8] called self-diffusion (SD) for image segmentation and clustering. In\u00a0 [11], an up to date approach (denoted as DP) was designed by uniting the advantages of different works, where a local affinity matrix with K -nearest neighbors was used. In this paper, we call  [ 1 , K ]  the similarity window which means that the similarity values out of this window are set to zero. Although selecting a reasonable K value is quite important, few efficient solutions are available. In our experiment, we find that the window size affects the performance of DP a lot and only a limited number of K values are available to keep good performance, which has restrained its practical applications.  For each retrieval, the result list may contain lots of objects from another class. Hence, shape filtering is promising in removing these incorrect results. Because the information captured by single feature is not enough, feature fusion is considered. Different features extracted from the same object reflect diverse properties, and, sometimes, they can complement each other. By optimizing and uniting these features properly, we can not only improve the discriminative ability but also decrease the amount of redundant information. Although the advantage is obvious, how to combine multiple features in a generalized way is still challenging. In non-rigid 3D field, there exist many fusion examples, such as SRCP-TD\u00a0 [16], CM-BOF\u00a0 [25] and ZFDR\u00a0 [26], which have achieved salient performance gains. In cross media retrieval field, a specific fusion method was used in\u00a0 [9] to filter out the incorrect results of search engines, where a grouping strategy was used and the largest connected component was regarded as the best result. However, the work suffers from two defects: (1) no visual links are available between the query and the search list; (2) taking the largest connected component as the best result is problematic because the correct cluster may be missed if there appeared a big but different cluster.  In this part, we summarize the main contributions of this paper. (1) A new local point descriptor IHKS is proposed based on our defined kernel function, and a corresponding holistic shape representation method is developed by concatenating dense descriptors. The introduced descriptor has multiple interesting properties, including isometric invariant, scale invariant, robust, informative and discriminative. (2) A novel graph diffusion approach is proposed for retrieval optimization, which maps the original similarity distance to an improved version. The created approach has significant advantages over existing algorithms in both the optimized precision and stability. (3) A shape filtering algorithm is devised based on the visual links between the query and the search list to remove the junk shapes, where a couple of features from both the local and global viewpoints are fused by an iterative feature selection procedure. (4) We carry out detailed experiments on three public non-rigid datasets and the results are pleasant that all the studied techniques have achieved a big improvement.  The rest of this paper is organized as follows. First, we describe the shape representation method in Section\u00a0 2. In Section\u00a0 3, we present our retrieval optimization approach. In Section\u00a0 4, we introduce a new shape filtering algorithm. Finally, the experimental results are presented in Section\u00a0 5 and the conclusions are drawn in Section\u00a0 6.  The quality of shape visual content representation plays an important role in non-rigid retrieval. To characterize the visual properties of each shape, we extract two types of features from both the local and global viewpoints: (a) Integrated Heat Kernel Signature (IHKS); (b) Commute Time Feature (CTF). The local feature focuses on the local details of each shape for better discriminative ability. While the global feature could provide a coarser-scale global statistics of the shape appearances.  Methods derived from heat kernel are promising for shape recognition. But, the scale selection has restrained the retrieval performance\u00a0 [1,2,4,5,16]. Hence, we create an integrated heat kernel signature (IHKS) as a new solution.  Mathematically, single point based heat kernel related methods can be regarded as a weighted heat accumulation framework: X \u00d7   \u211c   +   \u2192 \u211c    (1)  h  ( x , T )  =     \u2211   i     \u03d5   i   2    ( x )  k  (   \u03bb   i   , T )      \u2211   i   \u03c0  (   \u03bb   i   , T )    ,   where   \u03d5   i   and   \u03bb   i   are the eigenfunction and the eigenvalue of LBO which satisfies the heat equation   \u0394   X   u  ( x , t )  = \u2212   \u2202   \u2202 t   u  ( x , t )  on shape X , T is the possible scale space, k  (   \u03bb   i   , T )  represents the weighting kernel function, and \u03c0  (   \u03bb   i   , T )  is employed for normalization. When k  (   \u03bb   i   , T )  =   e   \u2212   \u03bb   i   t   and \u03c0  (   \u03bb   i   , T )  =  const  , h  ( \u22c5 , \u22c5 )  becomes HKS\u00a0 [5], and h  ( \u22c5 , \u22c5 )  is equal to wavelet heat kernel signature (WKS)\u00a0 [7] for k  (   \u03bb   i   , T )  =   e   \u2212      (   e   i   \u2212 log  (   E   i   )  )    2     2   \u03c3   2       and \u03c0  (   \u03bb   i   , T )  = k  (   \u03bb   i   , T )  . The function is also suitable for SIHKS\u00a0 [4] which requires a Fourier transformation towards h  ( \u22c5 , \u22c5 )  , where k  (   \u03bb   i   , T )  = \u2212   \u03b1   t   log  ( \u03b1 )    \u03bb   i     e   \u2212   \u03b1   t     \u03bb   i     and \u03c0  (   \u03bb   i   , T )  =   \u03d5   i   2    ( x )    e   \u2212   \u03b1   t     \u03bb   i     .  As to scale selection in T , different from prior works, we define a novel kernel function as the integration of the spectra of heat operator to balance the scale differences  (2)  k  (   \u03bb   i   )  =   lim   T \u2192 \u221e     \u222b   t = 0   T     e   \u2212   \u03bb   i   t   d t =   1     \u03bb   i     .      Then, we set \u03c0  (   \u03bb   i   , T )  =  const  and the resulting function  IHKS   ( x )  = h  ( x , T )  can be interpreted as the integration of heat kernel signature formally. The above operation has avoided the problem of scale selection as well as high dimensionality, which also keeps the information of scale space without waste and reduces the cost for shape matching.  (1) Properties. (a) Intrinsic invariant. IHKS is intrinsic in the sense that it is invariant to non-rigid bendings (or isometric invariant), which is a direct consequence of the invariance of LBO: two manifolds X and Y are said isometric if there exists a mapping \u03a9 : X \u2192 Y , s.t. \u2200 x , y \u2208 X ,   IHKS   ( x , y )  =  IHKS   ( \u03a9  ( x )  , \u03a9  ( y )  )  . (b) Scale-invariant. Suppose shape X is scaled by \u03b1 with   X   \u02c6   = \u03b1 X , the resulting heat of the corresponding point is  IHKS   (   x   \u02c6   )  =   \u2211   i         \u03d5   i     \u02c6     2    ( x )  k  (     \u03bb   \u02c6     i   )  . Based on     \u03d5   \u02c6     i   =   \u03b1   \u2212 1     \u03d5   i   and k  (     \u03bb   \u02c6     i   )  =   \u03b1   2   k  (   \u03bb   i   )  , it is easy to deduce that  IHKS   (   x   \u02c6   )  =  IHKS   ( x )  , which suggests that IHKS is scale-invariant. Different form SIHKS, the integration has solved the scale-invariant of HKS from another aspect. (c) Robustness. Because that the heat value of each point can be regarded as the heat of the convergence status of heat diffusion or it can also be treated as an average heat of each point, it is insensitive to the disturbance of noises. (d) Concise and informative. Because heat kernel contains all of the information about the intrinsic geometry of the shape and could fully characterize shapes up to isometry, the derived IHKS descriptor is informative. Suppose that LBO spectra have no repeated values and \u03a9 is a homeomorphism from X to Y :  Isometric   ( \u03a9 )   \u21d4  IHKS   ( x | X )  =  IHKS   ( \u03a9  ( x )  | Y )  . The removal of scale factors has also reduced the computation and storage complexity of the descriptor, which results in a concise and balanced representation. (e) Discriminative. The heat distribution by concatenating all vertices is determined up to shape, which makes the feature discriminative.  For the purpose of supporting the above properties, we demonstrate the feature maps in Fig.\u00a02 . Intuitively, under several transformations, all the shapes display similar color distributions in each row, which reflects the properties (a)\u2013(d). By comparing shapes from different classes (or rows), it is obvious that their heat components are distinct, which agrees with the discriminative property (e). According to the curves plotted in Fig.\u00a03 , we find that the heat distributions of intra-class shapes are compact while they are separated for inter-class shapes.   Fig.\u00a04 presents an intuitive comparison of our IHKS descriptor with the closely related descriptors: HKS, WKS and SIHKS. It reveals that HKS is sensitive to scale (see the second and third shapes in Fig.\u00a04(a)) and WKS performs not so well for shape deformations. The proposed IHKS descriptor has comparable robustness with SIHKS. We also find that all the compared methods are insensitive to noises and topology.  (2) Holistic shape representation for retrieval. For IHKS descriptors extracted from a shape, although the histogram quantization method could be directly used for holistic representation (denoted as   IHKS   \u2217   ), it is raw and could not describe shapes sufficiently due to the errors caused by the inconsistent quantization ranges of different shapes. Inspired by the bag-of-words (BOW) paradigm [4,27,28], we propose an efficient dictionary voting scheme to create a holistic shape representation using the IHKS descriptor of each vertex. The first step is to train a dictionary with C centers by  (3)   DIC   ( l )  =     \u2211   i     \u03bd   i   l      size   {   \u03bd   l   }    ,  1 \u2264 l \u2264 C   where   \u03bd   i   l   represents the i th element in cluster l and the dictionary clusters are obtained by dividing the IHKS dataset \u00a3 (extracted from the whole shape dataset  { X }  ) evenly into C partitions between min  { \u00a3 }  and max  { \u00a3 }  . Then, each shape can be represented by IHKS distributions on the obtained dictionary, which results in a uniform C sized feature. For simplicity, we also call the dictionary as linear BOW (LBOW) representation.  Unlike point based shape matching method\u00a0 [1], the proposed method avoids feature detection by using dense descriptors of a shape. And a statistical weighting is used to reduce the influence of trivial points. Different from the clustering based BOW methods [4,27,28], a simple but efficient partition method is used for the low dimensional linear data on the whole dataset. For clarity in experiment, \u201cIHKS\u201d\u00a0is used to denote the holistic LBOW based IHKS feature, and \u201cLIHKS\u201d\u00a0is used to denote the local IHKS descriptor of each point.  Shape distributions method\u00a0 [17,14,15,19] is very typical for shape retrieval and it has inspired us a lot. For non-rigid shape description in intrinsic domain, the intrinsic distance is more appropriate. We employ the commute time distance\u00a0 [22] because it reflects the combined effect of all possible weighted paths of pairwise points. Compared with geodesic distance\u00a0 [5], the distance performs more robustly than short circuit topology changes\u00a0 [14]. Hence, it can lead to a measure of similarity which is insensitive to edge deletions and insertions than the simple use of edge weight alone. Then, the k th item of the feature is defined as  (4)   CTF   ( X , k )  =   #  {  | p  (   x   i   )  \u2212 p  (   x   j   )  |  \u2208  bin   ( k )  }      \u2211   k = 1   B   #  {  | p  (   x   i   )  \u2212 p  (   x   j   )  |  \u2208  bin   ( k )  }    ,   where p  (   x   i   )  represents the surface embedding position of vertex   x   i   and #  {  | p  (   x   i   )  \u2212 p  (   x   j   )  |  \u2208  bin   ( k )  }  is the number of distances between p  (   x   i   )  and p  (   x   j   )  in the k th bin.  Based on the definition of commute time distance, it is easy to induce that CTF has various properties: isometric invariant, scale invariant and robust. Although the performance of commute time distance was discussed on dataset containing various types of transformed shapes in\u00a0 [3], no further evaluations were provided on datasets with large intra-class variations (e.g.\u00a0isometry). Besides, the diffusion distance\u00a0 [20,21] can also be used instead of commute time distance, because it performs well with its multi-scale property\u00a0 [3]. But, the scale selection problem needs to be further discussed. In our research, we prefer to use the parameter-free commute time distance which is an aggregated version of diffusion distance over scale space. And the actual retrieval difference using both distances is not very obvious.  For feature comparison, although many dissimilarity functions are available, such as L1 and L2 distance, we recommend to use the Tanimoto coefficient\u00a0 [29]   (5)  d  ( X , Y )  = 1 \u2212   F  ( X )  F  ( Y )       \u2016 F  ( X )  \u2016    2   +    \u2016 F  ( Y )  \u2016    2   \u2212 F  ( X )  F  ( Y )      because it can provide an intuitive illustration of the overlaps between two descriptors, which can be used as a quantization of similarity extent. Then, we use   d   i    ( X , Y )  and   d   c    ( X , Y )  to represent the pairwise feature distances for IHKS and CTF, respectively.  As a matter of fact, both IHKS and CTF are closely related by GPS embedding which is defined as\u00a0 [14]   (6)   GPS   ( x )  =  (     \u03d5   1    ( x )        \u03bb   1       ,     \u03d5   2    ( x )        \u03bb   2       , \u2026 ,     \u03d5   i    ( x )        \u03bb   i       , \u2026 )  .   Then, IHKS can be rewritten as the self-inner product of GPS coordinates  (7)   IHKS   ( x )  =  \u3008  GPS   ( x )  ,  GPS   ( x )  \u3009  ,   where  \u3008 \u22c5 , \u22c5 \u3009  represents the inner product space, and it corresponds to Green\u2019s function as well\u00a0 [14]. As to commute time distance, it can be expressed as the Euclidean distance in embedding space between surface vertices\u00a0 [22]   (8)        ( x , y )  =    \u2016  GPS   ( x )  \u2212  GPS   ( y )  \u2016      L   2     ,   where the embedding position in Eq. (4) satisfies p  (   x   i   )  =  GPS   (   x   i   )  .  Based on the above discussion, it is clear that the aims of the two features are different although they have the same embedding basis. IHKS highlights the local shape information, while CTF focuses on spatial relationship of pairwise shape vertices (i.e.\u00a0global property). Therefore, we are able to describe shapes from complementary aspects.  In feature space, even if two objects belong to the same class, their similarity distance may be very large due to errors and noises, which may lead to some correct shapes that failed to rank on the top of the retrieval list. One important clue that could be used freely to address the problem is to consider both the retrieval results and their similarity contexts in the database. According to the intra-class similarity relationships, we can reorder the results by updating the distance values, which is illustrated in Fig.\u00a01(b). Then, the aim of this section is to introduce a mapping  (9)         to learn new distance values to improve the retrieval without supervision. For short, we denote the proposed graph diffusion method in this section as GD.  Given a set of objects X =  {   x   1   ,   x   2   , \u2026 ,   x   N   }  and a distance function \u1e10 : X \u00d7 X \u2192 R for each pair of objects. Suppose   x   q   \u2208 X is a query and the other objects act as the database objects. By sorting \u1e10  (   x   q   ,   x   i   )  in ascending order for i = 1 , 2 , \u2026 , N , we get a ranking r  ( i )  ,  i = 1 , 2 , \u2026 , N of database objects based on their distance to the query. Usually, the first K \u226a N objects are returned as the most similar to the query, where  [ 1 , K ]  is called the similarity window which could provide much reliable information with fewer noises. The main problem is that the distance function \u1e10 is not perfect, which may lead to wrong results. We introduce a new approach to improve it by similarity learning.  At first, the distances are converted to similarity values by using Gaussian Kernel defined as  (10)  W  (   x   q   ,   x   i   )  =  {    exp  ( \u2212     \u1e10   2    (   x   q   ,   x   i   )      \u03c3   2     )    if  r  ( i )  \u2264 K     0   otherwise\u00a0,       where we define \u03c3 as the maximum value of \u1e10  (   x   q   ,   x   i   )  and the resulted matrix W is sparse. From the notion of random walks, W is then normalized by  (11)  P  (   x   q   ,   x   i   )  =   W  (   x   q   ,   x   i   )    D  (   x   q   )      where D  (   x   q   )  =   \u03a3   i \u2260 q   W  (   x   q   ,   x   i   )  is the degree of node   x   q   such that p  (   x   q   ,   x   i   )  \u2208  [ 0 , 1 ]  and   \u03a3   i   P  (   x   q   ,   x   i   )  = 1 .  Because that the random walk based algorithm\u00a0 [11] (which is state-of-the-art) is quite sensitive to K , we would not adopt the power iteration process for P . It is important to note that K has resulted in a graph with local similarity relationships of the database objects. The graph contains much less noises and it is also favored in geometry understanding by using sparse matrix\u00a0 [13]. As a result, we regard it as a high dimensional manifold and the similarity values are reconsidered as the understanding of this geometry. Thus, we propose a different method by constructing a graph Laplacian matrix  (12)  L = I \u2212 P ,   where I is a diagonal matrix with I  (   x   q   ,   x   q   )  =   \u03a3   i   P  (   x   q   ,   x   i   )  . Then, we obtain the solution of Eq. (9) by spectral embedding  (13)  \u1e11  (   x   q   ,   x   i   )  = F\u0327  (   x   q   ,   x   i   | \u03a6 , \u039b )  ,   where \u039b and \u03a6 represent the eigenvalue and the eigenfunction matrix of L , and F\u0327  ( \u22c5 , \u22c5 )  is some embedding function. Note that L is dissymmetric due to the usage of similarity window on W . To get a real solution, we recover its symmetry by directly setting W  (   x   q   ,   x   i   )  = max  { W  (   x   q   ,   x   i   )  , W  (   x   i   ,   x   q   )  }  instead of using the normalized matrix of L .  The above resulted distance measure interprets the new similarity relationships from geometry viewpoints by using the eigenbase of the manifold. It is important to note that the individual eigenfunction of the Laplacian matrix is less stable. But, by considering the subspace of the scaled eigenfunctions, functions based on them could reach a much stable representation\u00a0 [30]. Then, our suggestion for F\u0327  ( \u22c5 , \u22c5 )  is to formulate an average similarity measure of pairwise nodes as a distance locally, which can resist noises to a large extent. Besides, this setting is superior to the shortest path related approach\u00a0 [31] which is problematic if there exist short paths between dissimilar shapes.  As stated in\u00a0 [11] that the search result is quite noisy, the similarity window  [ 1 , K ]  could be helpful for rejecting noises and outliers. One defect of the prior works is that K is manually selected. To find an appropriate K which is usually unknown in unsupervised situations, we propose a global method to estimate an approximate value.  For each query shape x , the best class x \u2208  cluster   ( x )  is found by Affinity Propagation (AP) clustering algorithm\u00a0 [32], and a search list  top   ( k )  is chosen by setting k =  median   {  cluster   ( x )  | \u2200 x \u2208 X }  . We define a candidate set  (14)  \u03b6  ( x )  =  {  cluster   ( x )  \u2229  top   ( k )  }  ,   where its elements have a high probability to be similar to x . Then, a cumulative counter of \u03b6  ( x )  is defined by  (15)  c  ( i )  =  {    c  ( i \u2212 1 )  + 1   if  \u03c1  ( i )  \u2208 \u03b6  ( x )      c  ( i \u2212 1 )    otherwise     ,  1 \u2264 i \u2264 k ,   where c  ( 0 )  = 0 and \u03c1  ( i )  represents the i th element in  top   ( k )  . Next, a subset of \u03b6  ( x )  is accepted as the similarity window by using the maximum ratio  (16)  r  ( x )  = arg   max   i    {  ratio   ( i )  =   c  ( i )    i   , 1 \u2264 i \u2264 k }  ,   and then K on the whole dataset is averaged by  (17)  K =   1   N     \u2211   x \u2208 X   r  ( x )  .      As a matter of fact, several typical spectral distances\u00a0 [3] in spectral manifold space work equally well for F\u0327 , like diffusion distance and commute time distance. In our experiment, we choose the commute time distance\u00a0 [22]   (18)  F\u0327  (   x   q   ,   x   i   )  =   \u2211   j        (   \u03a6   j    (   x   q   )  \u2212   \u03a6   j    (   x   i   )  )    2       \u03bb   j     ,   because it is parameter-free and insensitive to the perturbations in graph structure (e.g.\u00a0adding or removal of an edge or a node).  In this section, we introduce a new algorithm to remove the junks from the retrieval results with a filtering strategy. We first give some observations and assumptions. (1) It is interesting to find that the retrieval results of IHKS and CTF have clustering properties based on their visual similarities. Thus, shape clustering is very attractive for improving the retrieval performance. The clusters can be classified into three types: the top group (TG), partially similar groups (PSG), and irrelevant groups (IG). With a high probability, TG is completely a subset of the class of the query, while objects in PSG may belong to the query class or be partially similar to the query in feature space. A large margin is witnessed between TG and IG, which indicates that the category border for them is quite clear. Hence, we have sufficient evidence to filter out these groups. But, as for PSG, the same operation is difficult. Based on the similarity distances to TG or IG, we create a model to classify objects in PSG into two clusters: positive and negative. At last, the negative set as well as IG are removed. (2) The retrieval results of different features may have strong correlations on the top few items, which can be combined to verify whether an object is similar to the query. (3) As to multi-features, each has its merits in some perspective. Thus, a feature selection method by kernel weight determination would be helpful.  Then, a new filtering algorithm is described as follows.  (1) Initialization. (a) The weighted kernel model. For shapes X and Y , their similarity is defined by a mixed kernel model of multiple features (IHKS and CTF)  (19)  W  ( X , Y )  =   \u2211   i = 1   l     \u03b1   i     W   i    ( X , Y )  ,    \u2211   i = 1   l     \u03b1   i   = 1 ,    \u03b1   i   \u2265 0 ,   where l  ( = 2 )  is the number of kernels and   \u03b1   i   represents the corresponding weight for each kernel   W   i   . Based on the distances (   d   i   and   d   c   ) described in Section\u00a0 2.3, two kernels are defined by local (   W   1   ) and global (   W   2   ) similarities  (20)    W   1    ( X , Y )  =   e   \u2212   d   i    ( X , Y )  / \u03c3   ,    W   2    ( X , Y )  =   e   \u2212   d   c    ( X , Y )  / \u03f1   ,   where \u03c3 and \u03f1 are the mean values of   d   i   and   d   c   , respectively. (b) Positive and negative set. For each query shape X , two visual retrieval lists p and q are obtained by using IHKS and CTF features. Then, the AP clustering algorithm\u00a0 [32] is used with p and q separately. And, \u2201  ( p )  is used to represent the best AP cluster of p , where a cluster is best if most of its elements rank in the front of the retrieval list. We initialize a positive group P\u0327 by an intersection\u2013union operation between \u2201  ( p )  and \u2201  ( q )     (21)  P\u0327 =  {    \u2201  ( p )  \u2229 \u2201  ( q )    if  n c \u2265 min  ( n 1 , n 2 )  / 3     \u2201  ( p )  \u222a \u2201  ( q )    otherwise\u00a0,       where n c =  size   ( \u2201  ( p )  \u2229 \u2201  ( q )  )  , n 1 =  size   ( \u2201  ( p )  )  and n 2 =  size   ( \u2201  ( q )  )  . The negative group is defined as  (22)  \u0145 =  ( p \u222a q )  \u2212 P\u0327\u00a0.   And the full set is denoted as U = P\u0327 \u222a \u0145 .  (2) Feature fusion by weight selection. For a given grouping result   G   j   (   G   1   = P\u0327 and   G   2   = \u0145 ), the aim of kernel weight selection is to minimize the inter-class similarity \u03b7  (   G   j   )  =   Z   j   T    ( D \u2212 W )    Z   j   and the intra-class differences \u03b8  (   G   j   )  =   Z   j   T   W   Z   j   by  (23)    min   \u03b1 =  (   \u03b1   1   ,   \u03b1   2   )     {   \u2211   j = 1   l     \u03b7  (   G   j   )    \u03b8  (   G   j   )    }  ,   where  (24)    Z   j    (   X   i   )  =  {    1   if    X   i   \u2208   G   j       0   otherwise       is a category indicator for shape   X   i   \u2208 U , and each element of the degree matrix D is defined as   (25)  D  ( X , X )  =   \u2211   i = 1   l     \u03b1   i     D   i    ( X , X )  ,    (26)    D   i    ( X , X )  =   \u2211   Y \u2208 U     W   i    ( X , Y )  .    Then, based on Eq. (23), the optimal weights \u03b1 are determined automatically by solving the objective function:  (27)    min   \u03b1    {   \u2211   i = 1   l     \u03b1   i     \u2211   j = 1   l       Z   j   T    (   D   i   \u2212   W   i   )    Z   j       Z   j   T     W   i     Z   j     }  .   If \u03b1 converge or the system has reached the maximum iteration steps, stop and return the positive set P\u0327 .  (3) Grouping. Once the weight \u03b1 is determined by step (2), shapes from U are partitioned into two sets by minimizing the following objective function  (28)    min   Z    {   \u2211   j = 1   l     \u03b7  (   G   j   )    \u03b8  (   G   j   )    =   \u2211   j = 1   l       Z   j   T   D   Z   j       Z   j   T   W   Z   j     \u2212 l }  ,   which results in finding the eigenvectors     Z   j     \u02c6   with the largest eigenvalues   \u03bb   j   of the eigen-decomposition problem  (29)    D   \u2212 1 / 2   W   D   \u2212 1 / 2       Z   j     \u02c6   =   \u03bb   j       Z   j     \u02c6   .   The new groups are determined by using kmeans algorithm on   Z   \u02c6   and we update the positive and negative classes: P\u0327,\u00a0\u0145 and U . Then, go to step (2) for iteration.  With the help of multiple kernels and the grouping method, our approach could kick out the unrelated shapes for each retrieval. By maximizing the inter-class difference and intra-class similarity simultaneously, a promising and good group is obtained as the positive result. Although some discrete noises that have random chances to be grouped in P\u0327 may degrade the overall precision of the returned results, the recall rate stays high and the errors are most likely to rank in the end.   EXPERIMENTS   To verify the non-rigid shape retrieval performance, the following three well-known benchmarks (which contain various deformations for evaluation) are employed.  (1) Non-rigid world (NRW) dataset \u00a0 [16] is a basic non-rigid dataset for shape analysis and recognition, which contains 148 shapes with different poses unevenly categorized into 12 classes: 9 cats, 11 dogs, 3 wolf shapes, 17 horses, 15 lions, 21 gorillas, 1 shark, 24 females, two different male shapes with 15 and 20 poses, and 6 centaurs and 6 seahorses for partial similarity experiments.  (2) SHREC10 Non-rigid dataset contains 200 shapes evenly distributed into 10 classes with 20 shapes in each\u00a0 [33]. Different from NRW dataset, the shapes in the same class contain lots of rigid noises, which have contributed to the difficulty for shape description.  (3) SHREC11 Non-rigid dataset is a publicly admitted evaluation dataset for shape retrieval and recognition\u00a0 [1,2], which consists of 600 watertight triangle mesh shapes evenly classified into 30 categories. In each class, there are 20 shapes with one null shape and nineteen deformations derived from the original model. The extent of rigid noises in each category is relatively low.  Considering the computation speed, all the shapes are down sampled to 2000 faces which still preserve most of the salient features of each shape. To comprehensively evaluate the retrieval results, the following popular performance metrics are employed, including Nearest Neighbor (NN), First Tier (FT), Second Tier (ST), and Discounted Cumulative Gain (DCG). To present more intuitive demonstrations about the results, precision\u2013recall (PR) curves are also computed. The range of all these items is  [ 0 , 1 ]  , the higher the better. More details about these standard evaluation metrics can be found in\u00a0 [34] which has provided the reliable source code to carry out the evaluation.  We evaluate the performance of IHKS and CTF on the above mentioned datasets. We choose a small dimension for IHKS and CTF: B = C = 64 . The reason we do not use large parameter values is that the storage space would increase a lot while the accuracy would stay almost the same. For easy comparison, we perform a leave-one-out KNN retrieval experiment, where each deformed shape is queried against the remaining intra-class shapes. For evaluation, matches are regarded correct between different deformations of the same shape (or class).  (1) Results on NRW dataset. On this dataset, we compare the studied features with the top methods based on LBO: HKS\u00a0 [5], SIHKS\u00a0 [4], WKS\u00a0 [7], TD\u00a0 [15,16], and SRCP-TD\u00a0 [16]. Table\u00a01 lists the evaluation measures for the studied features. Expectedly, the proposed IHKS method performs better than HKS, and it also outperforms SIHKS, WKS and (SRCP-)TD. Besides, we have also tried another shape representation method   IHKS   \u2217   (see Section\u00a0 2.1) and find that the adopted holistic shape representation method is superior. We also find that CTF performs quite well just following IHKS with a promising accuracy for shape recognition, which indicates that intrinsic shape distributions are also promising for non-rigid shapes.  (2) Results on SHREC10 Non-rigid dataset. For the purpose of illustrating the retrieval performance on this dataset, we compare with the advanced methods in this field: HKS, SIHKS, WKS, TD [5,4,7,15]. By observing the results in Table\u00a02 , it is clear that IHKS and CTF outperform the related methods (HKS, SIHKS, WKS, TD and   IHKS   \u2217   ), which is consistent with the results on NRW dataset.  (3) Results on SHREC11 Non-rigid dataset. To compare fairly on this popular dataset, we choose the following excellent methods for contrast: HKS\u00a0 [5], SIHKS\u00a0 [4], WKS\u00a0 [7], TD\u00a0 [15], SRCP-TD\u00a0 [16], CP-HK\u00a0 [2]. Based on the results in Table\u00a03 , we find that our IHKS approach evidently improves the original retrieval performance of HKS for non-rigid 3D model retrieval, and it performs better than the other versions and variations of HKS: SIHKS, WKS, TD, SRCP-TD and CP-HK. In addition, both the performances of CTF and IHKS are at the same level with good performance. In all, by comparing with various related approaches, we find that IHKS and CTF have reached the highest overall accuracy.  As can be seen from the above experiments, the shape retrieval methods based on IHKS and CTF have reached the advanced performance with high accuracy, especially that IHKS has practical advantages over HKS, SIHKS, WKS and TD. To illustrate the results intuitively, we then plot the precision\u2013recall curves in Fig.\u00a05 . Further, we test the noise resistance of IHKS by constructing a noise query dataset consisting of 90 shapes taken from SHREC11 Non-rigid dataset, where there are three shapes in each class with different levels of hardness which are more severe than that in Fig.\u00a02. The evaluation results shown in Fig.\u00a06 reveal that the proposed method could resist noises well. We then use the retrieval examples shown in Fig.\u00a07 to demonstrate the influences of noises and we find that, when the noise increases, the difference between appearance similar shapes from diverse classes would become small. According to the result, we see that IHKS feature could deal with unsharp shape noises well.  In this section, we verify the efficiency of the introduced GD method, and the average precision (AP) is adopted to illustrate the retrieval robustness (or quality), which is approximated as the area under the Precision\u2013Recall curves using the number of neighbors from the same category. Because the top items would attract more attention, the First Tier (FT) values are also admitted in this evaluation. To highlight the ability of GD, we compare it with the diffusion process (DP) approach by\u00a0 [11] which is the best reported method. The distance matrices based on IHKS and CTF are used as the input for the algorithm. As a consequence, the results are represented as IHKS-GD, IHKS-DP, CTF-GD and CTF-DP. The proposed method has several advantages, which would be demonstrated next.  According to Fig.\u00a08 , it is clear that our GD method is quite stable for the varying sizes of similarity window. On the one hand, the blue curves for DP go down very quickly with narrow peaks, while our GD method (the red curves) could keep a high precision for a long time. On the other, both GD and DP could produce positive and negative results for the original method (or baseline), but GD offers a larger space for errors of similarity windows with much lower probabilities to reduce the precision. In addition, it is easy to observe that different choices of parameter K affect the retrieval performance a lot. If K is too small, the information is not enough for optimization and the result would be easily influenced by small noises. When K gets too large, large amount of noises are incorporated, which requires that the optimization algorithm should have an excellent noise resistance ability. Thus, the selection of appropriate K values is very important.  The improved results of IHKS and CTF with automatic similarity window (i.e. K ) are listed in Table\u00a04 . It is important to note that GD has salient advantages (e.g.\u00a0NN for IHKS-GD is 12.7% higher than IHKS-DP on SHREC11 Non-rigid dataset) over DP on different datasets. And, IHKS-GD outperforms CTF-GD in most cases despite some exceptions, like NN on NRW dataset (100% versus 99.3%). The reason is that IHKS has a better discriminative ability (or larger inter-class intervals) with a BOW voting scheme. Compared with the baseline in Tables\u00a01\u20133, we find that our method has worked efficiently for retrieval, such as IHKS on SHREC10 Non-rigid dataset (improve: 12.0% for FT, 12.2% for ST, and 3.6% for DCG).  To assess the effectiveness of the proposed SFR method, our evaluation focuses on precision and recall, where the precision is defined as the percentage of correct shapes in the returned list and recall is defined as the ratio of correct returned shapes with respect to the number of shapes in the same category. The top 50 results are returned for both IHKS and CTF. As to SFR, the combined list size is nearly 70 which is used as the baseline for comparison.  As shown in Fig.\u00a09 , it is obvious that our algorithm has very competitive precision against the original results, where the recall rates are approximately at the same level: 98% on SHREC11 Non-rigid dataset; 90% on SHREC10 Non-rigid dataset; 98% on NRW dataset. Our algorithm has significant advances for retrieval precision on multiple shape classes, like glasses and lioness, which suggests that the proposed method can filter out the junk shapes effectively. Besides, we also give the average precision values of SFR on each dataset in Table\u00a05 . To intuitively demonstrate the shape filtering performance, we present some representative filter results in Fig.\u00a010 .  In this part, we first analyze the computation complexity. (1) For shape representation in Section\u00a0 2.1, both IHKS and CTF are based on eigen-decomposition of LBO and the complexity of this operation is O  ( M \u03b8 )  [3], where M is the number of vertex of a shape and \u03b8 is the number of eigenvalues. The costs of computing LIHKS and CTF are O  ( \u03b8 )  and O  (   M   2   \u03b8 )  , respectively. The complexity of training dictionary on \u00a3 is O  (  size   ( \u00a3 )  )  and the voting cost of each shape is O  ( M C )  . (2) For GD in Section\u00a0 3, the overall complexity is O  (   N   2   )  , where N is the size of database. (3) As for SFR in Section\u00a0 4, the main cost lies in the iterative feature selection and clustering with complexity O  (   n   2   )  , where n is the size of the query list.  Then, on an Intel(R) Xeon(R) Dual CPU W3530@3 GHz with 2\u00a0GB memory, the average running time of each part of this paper is reported in Table\u00a06 . We see that the time cost of IHKS is much less than CTF. As for GD, the algorithm can run offline to construct similarity contexts between dataset shapes. And some other techniques can also be used to speed up. Although the complexity for SFR is high O  (   n   2   )  for each query, the cost would reduce a lot by adopting the top few retrieval results, like the time consumption of SFR in Table\u00a06.   CONCLUSIONS   In this paper, multiple aspects are considered for non-rigid 3D shapes retrieval. Firstly, for shape representation, both of the local and global features (IHKS and CTF) are studied for shape description. Specifically, the heat accumulation descriptor IHKS has several interesting properties, including isometric invariant, scale invariant, robust, informative and discriminative. Then, a new graph diffusion approach is devised for retrieval optimization and, compared with the iterative random walks based method, our approach has a superior stability for various similarity windows because it adopts the averaged local paths. Additionally, we propose a shape filtering technique to remove the junk shapes. The visual links between the query and the retrieval list are used for grouping and then the visually unrelated shapes are removed. Finally, the presented experiment results show that our research is promising and the proposed techniques have produced positive influences on non-rigid shape retrieval.   ACKNOWLEDGMENTS   This work is partly supported by National Natural Science Foundation of China (Grant No. 61379106), the Scientific Research Foundation for the Excellent Middle-Aged and Youth Scientists of Shandong Province of China (Grant No. BS2010DX037), the Shandong Provincial Natural Science Foundation (Grant Nos. ZR2009GL014, and ZR2013FM036), the Open Project Program of the State Key Lab of CAD & CG (Grant No. A1315), Zhejiang University, the Fundamental Research Funds for the Central Universities (Grant Nos. 10CX04043A, 10CX04014B, 11CX04053A, 11CX06086A, 12CX06083A, 12CX06086A, 13CX06007A, and 14CX06010A, 14CX06012A).   REFERENCES", "highlights": "As non-rigid 3D shape plays increasingly important roles in practical applications, this paper addresses its retrieval problem by considering three aspects: shape representation, retrieval optimization, and shape filtering. (1) For shape representation, two kinds of features are considered. We first propose a new integration kernel based local descriptor, and then an efficient voting scheme is designed for shape representation. Besides, we also study the commute times as shape distributions, which grasp the spatial shape information globally. Both of them capture shape information from different viewpoints based on the same embedding basis. (2) We then study the typical problem of retrieval optimization. Prior works show poor stability under different similarity windows. To deal with this deficiency, we propose to model the problem as a distance mapping on a graph in spectral manifold space. (3) Usually, for each retrieval input, a list is returned and there may be lots of irrelevant results. We develop an algorithm to filter them out by combining multiple kernels. Finally, three public datasets are employed for performance evaluation and the results show that the studied techniques have contributed a lot in promoting the recognition rate of non-rigid 3D shapes."}