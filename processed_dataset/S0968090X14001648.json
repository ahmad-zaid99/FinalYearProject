{"id": "S0968090X14001648", "article": "MAIN-TITLE Local online kernel ridge regression for forecasting of urban travel times   HIGHLIGHTS          Local online kernel ridge regression (LOKRR) is developed for forecasting urban travel times.      LOKRR takes into account the time varying characteristics of traffic series through the use of locally defined kernels.      LOKRR outperforms ARIMA, Elman ANN and SVR in forecasting travel times on London\u2019s road network.      The model is based on regularised linear regression, and clear guidelines are given for parameter training.          KEYPHRASES   Forecasting  Travel time  Prediction  Time series  Kernel method  Machine learning   The short term forecasting of traffic variables such as speeds, flows and densities is one of the primary goals of Intelligent Transportation Systems (ITSs), with applications in dynamic signal control, advanced traffic management systems (ATMSs) and advanced traveller information systems (ATISs) (Vlahogianni et al., 2004). To date, a wide range of methods have been used for short term traffic forecasting, which can be broadly separated into two categories: (1) parametric methods, and; (2) machine learning (ML) methods. 1 ML methods are often referred to as nonparametric methods (Vlahogianni et al., 2004) or computational intelligence (CI) methods (Karlaftis and Vlahogianni, 2011).   1 The former type includes statistical (space) time series methods such as the (space\u2013time) auto-regressive integrated moving average (ST)ARIMA model family (Billings and Yang, 2006; Cheng et al., 2010; Kamarianakis and Prastacos, 2005; Williams and Hoel, 2003), state space models (Okutani and Stephanedes, 1984; Stathopoulos and Karlaftis, 2003), and Bayesian networks (Anacleto et al., 2013a; Fei et al., 2011; Sun et al., 2004, 2005, 2006; Zheng et al., 2006). Comprehensive reviews can be found in Vlahogianni et al. (2004) and, more recently Vlahogianni et al. (2014). These methods typically assume that the data being described are stationary. That is, they must have constant mean and variance. If this assumption is not satisfied, then the data must be transformed through differencing, or some other transformation (Kendall and Ord, 1990). It is often found that these assumptions are difficult to satisfy, leading standard parametric methods to perform poorly. This has led to the recent development of local parametric model specifications that attempt to model the local characteristics of traffic data in time and/or space (Ding et al., 2010; Kamarianakis et al., 2012; Min and Wynter, 2011; Min et al., 2009, 2010).  An alternative approach is to model the data directly in a nonlinear machine learning (ML) framework. ML methods typically make minimal explicit assumptions about the data generating process, and instead try to learn the characteristics of the data through exposure to examples (Mitchell, 1997). ML methods have often been shown to outperform parametric methods in the literature, although a recent comparison study by Chen et al. (2012) suggests that data preprocessing is just as important as model choice. The most widespread ML method in the short term traffic forecasting literature is the artificial neural network (ANN). ANNs have a long history of successful implementation in traffic forecasting, and readers are directed to Dougherty (1995) for a review of the early work. As the power of computers has increased, researchers have developed increasingly sophisticated ANNs for forecasting traffic variables both on highways (see, e.g., Chen and Grant-Muller, 2001; van Lint et al., 2005; van Hinsbergen et al., 2009; Huang and Sadek, 2009; Li and Rose, 2011) and on urban networks (see, e.g. Ledoux, 1997; Yin et al., 2002; Vlahogianni et al., 2005, 2007). The most successful implementations circumvent the traditional black box problem of neural networks by explicitly incorporating domain knowledge into the model structure. For example, the topological structure of the road network can be explicitly represented in the nodes of the hidden layer of an ANN, making the internal function of the model more transparent (van Lint et al., 2005; van Lint, 2006). In a similar vein, Vlahogianni et al. (2005) frame their genetically optimised modular ANN as a multivariate non-linear time series model, fed with spatially and temporally lagged data. The recent work of (Chan et al., 2013a,b; Chan et al., 2012a,b) has further enhanced the position of ANNs at the forefront of the traffic forecasting literature. Other ML methods that have been applied to traffic forecasting include fuzzy rule based systems (Dimitriou et al., 2008) and hybrid models (Van Der Voort et al., 1996; Hofleitner et al., 2012). Karlaftis and Vlahogianni (2011) summarise the main differences (and similarities) between ML methods and parametric methods.  Recent developments have seen the application of kernel methods (KMs) to traffic forecasting. The term kernel method is an umbrella term for a broad set of techniques that share a common characteristic. They comprise two components: (1) a function that maps the input data into a high (possibly infinite) dimensional space, known as a feature space, and; (2) a learning algorithm capable of discovering linear patterns in that space (Shawe-Taylor and Cristianini, 2004). Mapping to the feature space is accomplished efficiently using a kernel function, hence the term KM. Because linear relations are sought in the feature space, a broad range of theoretically well founded and efficient linear algorithms can be used. To date, many linear algorithms have been kernelised including ridge regression (Hoerl and Kennard, 1970; Saunders et al., 1998), the generalised portrait (Vapnik and Lerner, 1963; Boser et al., 1992), principal components analysis (Sch\u00f6lkopf et al., 1997) and canonical correlation analysis (Hotelling, 1936; Hardoon et al., 2004) amongst many others. KMs are modular in nature, meaning that any kernel algorithm can be applied using a particular kernel, and vice versa (Shawe-Taylor and Cristianini, 2004). This gives them great flexibility as a tool for solving a wide range of practical problems. KMs are an attractive approach for modelling nonlinear and nonstationary data because they combine the advantages of principled, linear learning algorithms such as ordinary least squares (OLS) with nonlinear solutions.  The most widely used KM in traffic forecasting is Support Vector Regression (SVR). Wu et al. (2004) first used SVR for the forecasting of travel times on Taipei\u2019s freeway system. Travel times over three distances for 28 consecutive days are used to train a model to forecast the following 7days\u2019 travel times in a one-step-ahead scenario. The results are compared to a current\u2013time predictor and a historical mean predictor and are found to be superior in all cases. The performance of SVR compares favourably with that of ANNs. Vanajakshi and Rilett (2007, 2004) compare the performance of SVR and ANNs in forecasting travel times on San Antonio\u2019s freeway system using a forecast horizon varying from 2min to 1h ahead. It is found that SVR performs better than ANNs when the size of the training set is small. Zhang and Xie (2008) used \u03bd-SVR for highway traffic volume forecasting, and found the method to outperform a multi-layer feedforward neural network (MLFNN). Other KMs have been applied to traffic forecasting with similar success. Xie et al. (2010) apply Gaussian processes regression (GPR) to highway traffic volume forecasting, and the results are compared with the \u03bd-SVR model of Zhang and Xie. The results are found to be similar, but the GPR model has the advantage of providing error bounds on the forecasts.  One of the main challenges in applying KMs lies in deciding what information to include in the kernel. In KMs a kernel induced feature space is constructed from a database of historical data patterns to store the relevant information about a particular problem. This feature space must contain sufficient richness of patterns in order to produce accurate forecasts, while not being so large as to sacrifice computational efficiency. Wang and Shi (2013) designed an SVR model in which the input space is defined using chaos theory and wavelet theory, namely the Chaos-Wavelet Analysis-Vector Machine (C-WSVM). It is demonstrated that selection of the correct input space to an SVR model can improve forecasting performance. However, the number of data points required in the kernel is not considered, and time varying input spaces are not investigated. In long traffic series, it is not feasible to use the entire historical dataset to forecast at each point in time because this would involve the construction of very large kernels. Therefore, typically one seeks to select a subset of the data that will be most informative. The most common approach taken in the literature is to use a small subset of the most recent observations, which makes the unrealistic assumption that a sufficiently diverse range of traffic states have been observed recently. For example, Wu et al. (2004) use just 5weeks of data to train their SVR model, while (Hong, 2010, 2011, 2012; Hong et al., 2011) use just 1month of data. While the ability to produce strong performance on smaller training data sets is one of the advantages of SVR (Vanajakshi and Rilett, 2004), the use of such a short period of training data means that there is unlikely to be sufficient richness of unusual traffic patterns. Furthermore, as time passes the training data will lose its relevance due to the yearly seasonal trends in traffic data.  One way to address the latter problem is to use online or sequential training in KMs. For example, Castro-Neto et al. (2009) implemented an online SVR algorithm for short term traffic flow forecasting. Instead of retraining the model every time new data become available, which is computationally expensive, the model is iteratively updated three samples at a time and the solution support vectors are changed accordingly. The advantage of this approach is that new information is incorporated into the existing structure of the solution. Empirical results show that the method outperforms Holt\u2019s exponential smoothing, and multi-layer perceptron (MLP) ANNs. Gaussian Maximum Likelihood (GML) produces better results under typical conditions but the OL-SVR model performs well under non-recurrent conditions due to the fact it adapts well to new data. Given that non-recurrent events are more difficult to predict this is a significant benefit. However, it does not address the issue that only very recent traffic patterns are included in the kernel, in this case 15days.  It is well known that road traffic exhibits strong cyclic patterns, usually characterised by a peak period in the morning and evening, with intervening periods of lower traffic. This phenomenon is described statistically as seasonal temporal autocorrelation. Moreover, traffic conditions in the peak periods are more variable than those in the intervening periods, a phenomenon known as heteroskedasticity (Fosgerau and Fukuda, 2012). From the perspective of pattern analysis, this temporal locality implies that data pertaining to the period between peaks will be largely ineffective in forecasting the peak periods, and vice versa, motivating the development of models that produce forecasts based on similar traffic states. The simplest way to achieve this is to arbitrarily divide the data into time periods, such as AM peak and PM peak, and construct a separate model for each (e.g. Hong et al., 2011; Hong, 2012; Stathopoulos and Karlaftis, 2003). A more considered approach is to attempt to identify traffic states based on historical data and construct a model for each one (Kamarianakis et al., 2010, 2012; Min and Wynter, 2011). In operation, the models are switched according to the current traffic state. One can also make the assumption of smooth transition of traffic states, and construct a model in which the parameters vary smoothly by time of day (Anacleto et al., 2013a, 2013b). The latter model type is designed to model the seasonal autocorrelation and heteroskedasticity in traffic data.  In this study, a novel local kernel based algorithm is developed for the forecasting of traffic series, namely local online kernel ridge regression (LOKRR). LOKRR has its roots in OLS, making it relatively simple to interpret and implement compared with other KMs. The structure of the paper is as follows: In Section 2, the LOKRR algorithm is described, beginning with a motivation for the model. In Section 3, a case study is introduced, in which LOKRR is used for forecasting Unit Travel Times (UTT, seconds/metre) collected using automatic number plate recognition (ANPR) on London\u2019s road network. In Section 4, the results are presented and analysed. Finally, in Section 5, some conclusions and directions for future research are given.  To motivate the model in the current context, the example of forecasting travel times on a single road section (link) is used. Assume a stream of observations of a traffic variable such as flow, density or travel time [z 1, z 2,\u2026, zT are observed at times t =1,2,\u2026, T, obtained at an interval of \u03c4 =5min over a number of days. The task is to make use of observations of the process up to time t to forecast the value of zt  +1. Each day, (60/5)*24=288 observations are made. Now assume that because of computational constraints the maximum kernel size of a kernel based model is set to 10,000*10,000, which is a reasonable upper limit. The standard approach is to use the most recent observations of the process to construct a single kernel. Using this approach entails that a maximum of 10,000/288=34.7days of data could be included in the kernel. This approach has two clear drawbacks. Firstly, 34.7days is a relatively short time in this context, and it is unlikely that sufficient variation in traffic patterns will have been observed over this period in order to produce reliable forecasts, particularly under abnormal conditions. Secondly, because of the strong cyclic pattern present in traffic data, historical traffic patterns recorded at temporal lags that are either close to zero or close to divisible by 288 are likely to be much more informative than those at other lags. For example, when forecasting the level of flow at 9AM on a Tuesday morning, historical data pertaining to midnight on a Saturday is unlikely to be useful. Furthermore, the data with which the model is trained is important: If the model is trained on data from August and applied for forecasting traffic in December, it is likely to encounter problems due to the differences in traffic between August and December.  Consider as an alternative, the extreme example where zt  +1 is forecast as a function of only those travel times that were observed at the same time of day on previous days. With the same maximum kernel size, one could build a kernel containing the travel time patterns obtained on the previous 10,000days. Of course, in practice, one would not have access to 10,000days (27.4years) of historical data, and the nonstationarity in long term travel patterns would render data collected beyond a certain time threshold irrelevant. However, this example serves to highlight the fact that if knowledge of the cyclic nature of traffic is directly incorporated into the model structure, then there is potential for having access to a much richer source of information about the time point to be forecast without increasing the kernel size. In fact, it is possible to include much more informative data in a far smaller kernel. Using the same example, one could capture the behaviour of the link at a single time point over the course of a year in a kernel of size 365*365.  The advantage of the formulation outlined above is that it directly incorporates seasonality into the model structure. However, it has the drawback that it may exclude a significant amount of data that may be of interest. Returning to the example of forecasting some traffic variable on a road link, if one were to commute to work on the same road at approximately the same time each day, one may observe that the road tends to become congested at approximately the same time each day, and may be able to make statements such as \u201cif I leave after 9am there is always too much traffic\u201d, or \u201cif I set off before 8 am my journey is usually pretty quick\u201d. However, there is usually significant variation around such trends. For instance, on some days a link may become congested earlier or later than usual; or the congestion may be slightly more or less severe, and one might find oneself making a statement such as \u201cIt\u2019s especially busy today, something must have happened\u201d, or \u201cwow, it\u2019s really quiet, it\u2019s usually really busy by now\u201d. These intuitive observations summarise the variability inherent in traffic data, and a successful traffic forecasting model should be capable of modelling this variability.   Fig. 1 shows an illustration of the variability in Unit Travel Times (UTT, seconds/km) on a road link in London, UK. There are two recurrent peaks on this link, a small one in the morning and a larger one in the evening. Although they occur at roughly the same time each day, the time at which they begin and end, as well as their magnitude, differs within a time window. Within this window, the observed historical patterns are likely to be informative and should be included in the kernel. From the perspective of kernel based methods, this means that there is a temporal window extending in both directions around each point within which past information is informative and a kernel can be constructed. Based on this observation, a local kernel based model of traffic data is proposed, in which local kernels are constructed around each time point. This model is described in the following subsections.  The LOKRR model is based on a simple form of regularised linear regression, called ridge regression (RR) (Hoerl and Kennard, 1970). Beginning with the well-known case of multiple linear regression, one seeks to solve a system of equations of the following form:  (1)  y = Xw + \u03b5   where X is (n \u2217 p) and of rank p, n is the number of observations, p is the number of variables, w is (p \u22171) and unknown , E[\u03b5]=0, and  E    \u03b5   \u03b5   \u2032      =   \u03c3   2     I   n    . The solution of this system for w is:  (2)  w =   (   X   \u2032   X )   - 1     X   \u2032   y   This solution is viable when X\u2032X is nearly a unit matrix. However, if this is not the case then this solution is sensitive to the number of errors in the data. To overcome this problem, a regularisation constant  \u03bb  can be introduced:  (3)  w =   (   X   \u2032   X + \u03bb   I   p   )   - 1     X   \u2032   y ;  \u03bb \u2265 0      \u03bb  is called the ridge parameter and the resulting algorithm is known as ridge regression. The matrix  (   X   \u2032   X + \u03bb   I   p   )  is always invertible if  \u03bb > 0  , allowing the solution of ill-posed regression problems. If  \u03bb = 0  , Eq. (3) is equivalent to Eq. (2).  Linear RR can be converted to a nonlinear algorithm. Eq. (3) can be rearranged in terms of w as follows (Shawe-Taylor and Cristianini, 2004):  (4)  w =   \u03bb   - 1     X   \u2032   ( y - Xw ) =   X   \u2032   \u03b1    w can be written as a linear combination of the training data points,  w =   \u2211   i = 1   n     a   i     x   i    , with  \u03b1 =   \u03bb   - 1   ( y - Xw )  . Therefore, \u03b1 can be computed as:  (5)     \u03b1 =   \u03bb   - 1   ( y - Xw )     \u21d2 \u03bb \u03b1 = ( y -   XX   \u2032   \u03b1 )     \u21d2 (   XX   \u2032   + \u03bb   I   n   ) \u03b1 = y     \u21d2 \u03b1 =   ( G + \u03bb   I   n   )   - 1   y      where G = XX\u2032, and is known as the Gram matrix. It can be seen from Eq. (5) that the solution is now written in terms of the data points and the weight vector w need not be solved explicitly. This formulation is computationally more efficient than Eq. (3) when p > n, which is often the case in machine learning applications. However, the main benefit of expressing the algorithm in this way is that, given a valid kernel function K, G can be replaced with a kernel matrix K as follows:  (6)  \u03b1 =   ( K + \u03bb   I   n   )   - 1   y   This algorithm is known as kernel ridge regression (KRR). A forecast for a new data point can be computed as:  (7)  g ( x ) =   y   \u2032     ( K + \u03bb   I   n   )   - 1   k   where K is a kernel matrix of inner products between training vectors and k = k(x  i , x) is a vector of inner products between the test vector x  i and the training vectors x. KRR is called a regularization network in the ANN literature (Poggio and Girosi, 1990). The method is also strongly related to Kriging (Krige, 1951), which is termed GPR in the machine learning community (Rasmussen and Williams, 2006). There are many kernels that one could use, but the most widely used and generally applicable kernel is the Gaussian radial basis function (RBF) kernel (Keerthi and Lin, 2003):  (8)  K ( x , z ) = exp    -   \u2016 x - z   \u2016   2     2   \u03c3   2          where \u03c3 is the kernel bandwidth, and controls the smoothness of the function. In the following description of the methodology, use of the Gaussian RBF kernel is assumed, and the parameter selection described in Section 2.6 is specific to this kernel type.  To take into account the long term seasonality and trend in traffic data, the LOKRR model described here is online. Online model training involves processing the data one example at a time, making a forecast, and then updating the model based on the predictive error (Shawe-Taylor and Cristianini, 2004). Online learning allows new data to be incorporated into the model as they arrive, and old data to be removed, enabling the model to adapt to changes in the distribution of the data.  LOKRR is based on the sliding window kernel recursive least squares (KRLS) algorithm of Van Vaerenbergh et al. (2006), which is equivalent to an online KRR (OKRR) algorithm. Given a stream of training data points {(x 1, y 1),(x 2, y 2),\u2026}, the training data at time t is constructed as y  t  =[yt , yt  \u22121,\u2026, yt  \u2212  N  +1]\u2032 and X  t  =[x  t , x  t  \u22121,\u2026, x  t  \u2212  N  +1]\u2032, where yt and x  t are the values of the dependent and independent variables at time t, respectively, N is the size of the window and t =1,2,\u2026, N. In the context of traffic forecasting, an autoregressive model form is usually specified: Given a series of observations of a traffic variable z 1, z 2,\u2026, zT , yt  = zt  +1 and x  t  =[zt , zt  \u22121,\u2026, zt  \u2212  m  +1], where m is the embedding dimension of the series. However, it is possible to include other variables such as a time varying mean, observations of another traffic variable, or other data such as precipitation or incidents.  At time t, a regularised kernel matrix    K   t   \u03bb   = (   K   t   + \u03bb   I   t   )  is constructed from X  t . At the following time step, the data window slides along by one point to time t +1 and the most recent observation is added to the training data and the oldest training point is removed. A new matrix    K   t + 1   \u03bb    is constructed. Thus, at each time point the kernel matrix is constructed from only the previous N training examples. The main computational burden of standard KRR lies in calculating the inverse of the regularised kernel matrix in Eq. (7). OKRR requires the inverse to be recalculated at each time step, which is infeasible in an online setting. Fortunately, methods exist for updating    K   t   \u03bb - 1    without computing the inverse from scratch. To remove a row and column, a kernel matrix K and its inverse K \u22121 can be partitioned as follows (Van Vaerenbergh et al., 2006, p. 8):  (9)  K =       a     b   \u2032       b   D          ,    K   - 1   =       e     f   \u2032       f   G            The inverse of the submatrix D is required, which can be calculated from the submatrices of K \u22121 as follows:  (10)    D   - 1   = G -   ff   \u2032   / e   Therefore, the updated inverse is calculated from the known elements of K \u22121. To add a row and column to the inverse, K \u22121 and K are partitioned as follows:  (11)  K =       A   b       b   \u2032     d          ,    K   - 1   =       E   f       f   \u2032     g            Then one can take advantage of the following formula:  (12)    K   - 1   =         A   - 1   ( I +   bb   \u2032     A   - 1 H   g )   -   A   - 1   b g     -   (   A   - 1   b )   \u2032   g   g            where  (13)  g =   ( d -   b   \u2032     A   - 1   b )   - 1     Here, the superscript H of A  H denotes the conjugate transpose of A. A \u22121 is the inverse kernel matrix obtained at the previous time step, demonstrating that the inverse kernel matrix does not need to be recalculated from scratch. These steps reduce the computational complexity of inverting the matrix from O(N 3) to O(N 2), where N is the number of data samples in the kernel.  OKRR has the advantage over standard KRR that it can incorporate new information in the model structure without a significant increase in computation time. However, it has two limitations that limit its application to traffic series. Firstly, a single kernel is constructed using only the data of the N time points immediately preceding time t. This is reasonable in cases where time series exhibit sudden regime changes such as the Wiener system reported in Van Vaerenbergh et al. (2006). However, in seasonal data, this is likely to be insufficient since the most informative data comes not just from the immediately preceding time points, but from the same times on previous days, weeks or months. The second drawback is that it uses a single set of parameters (ridge parameter  \u03bb  and kernel parameter(s)) to describe the behaviour of the system across all times. This limits the ability of the model to account for heteroskedasticity. To address these two limitations, a local version of OKRR is proposed, namely Local (L)OKRR, in which a separate kernel is defined for each time of day, each with its own set of parameters.  Consider, again, a stream of training data points {(x 1, y 1),(x 2, y 2),\u2026}. The goal is to forecast y  t using a subset of the observed patterns. Assume the dataset has a regular seasonal component with order S. The training dataset is constructed as y  st  =[y 1,  t , y 2,  t ,\u2026, yn  ,  t ]\u2032 and X  st  =[x 1,  t , x 2,  t ,\u2026, x  n  ,  t ]\u2032, where s =1,2,\u2026, N is the index of the seasonal component, for example the day index, and t =1,2,\u2026, S is the time of day index. This formulation is shown diagrammatically in Fig. 2 .  The rows of the table indicate the successive days in the training dataset. The columns represent the successive time intervals in a day. N is the total number of days in the training dataset. In the example outlined above of forecasting travel times, S =288. Under this formulation, S kernels K 1, K 2,\u2026, K  S are constructed from the columns of Fig. 2. For example, to forecast the travel time at t 2, the first column of training examples is used. Compared with a single kernel model, this would appear to be a large number of kernels, however, each of them can have a much smaller dimension than a single kernel model. Furthermore, a smaller kernel needs to be updated at each time step, which is computationally more efficient than updating a single large kernel. Eq. (14) shows a local temporal kernel constructed in this way.  (14)    K   st   =       k (   x   1 , t   ,   x   1 , t   )   k (   x   1 , t   ,   x   2 , t   )   \u22ef   k (   x   1 , t   ,   x   N , t   )     k (   x   2 , t   ,   x   1 , t   )   k (   x   2 , t   ,   x   2 , t   )   \u22ef   k (   x   2 , t   ,   x   N , t   )     \u22ee   \u22ee   \u22f1   \u22ee     k (   x   N , t   ,   x   1 , t   )   k (   x   N , t   ,   x   2 , t   )   \u22ef   k (   x   N , t   ,   x   N , t   )               The model form shown in Fig. 2 restricts the model to include only those patterns that were observed at exactly the same time of day. However, it was argued in Section 2.2 that this is overly restrictive. To address this, a local temporal window is formed around each time t, which is denoted as w and the training data is constructed as:  (15)    y   stw   =   [ {   y   1 , t - w   , \u2026 ,   y   1 , t - 1   ,   y   1 , t   ,   y   1 , t + 1   , \u2026 ,   y   1 , t + w   } , \u2026 , {   y   n , t - w   , \u2026 ,   y   n , t - 1   ,   y   n , t   ,   y   n , t + 1   , \u2026 ,   y   n , t + w   } ]   \u2032        (16)    x   stw   =   [ {   x   1 , t - w   , \u2026 ,   x   1 , t - 1   ,   x   1 , t   ,   x   1 , t + 1   , \u2026 ,   x   1 , t + w   } , \u2026 , {   x   n , t - w   , \u2026 ,   x   n , t - 1   ,   x   n , t   ,   x   n , t + 1   , \u2026 ,   x   n , t + w   } ]   \u2032     Based on this, kernels K 1, K 2,\u2026,  K  S are defined according to:  (17)    K   t   =         K   1 , 1       K   1 , 2     \u22ef     K   1 , n         K   2 , 1       K   2 , 2     \u22ef     K   2 , n       \u22ee   \u22ee   \u22f1   \u22ee       K   n , 1       K   n , 2     \u22ef     K   n , n              where  (18)    K   1 , 1   =       k (   x   1 , t - w   ,   x   1 , t - w   )   \u22ef   k (   x   1 , t - w   ,   x   1 , t - 1   )   k (   x   1 , t - w   ,   x   1 , t   )   k (   x   1 , t - w   ,   x   1 , t + 1   )   \u22ef   k (   x   1 , t - w   ,   x   1 , t + w   )     \u22ee   \u22f1   \u22ee   \u22ee   \u22ee   \u22f1   \u22ee     k (   x   1 , t - 1   ,   x   1 , t - w   )   \u22ef   k (   x   1 , t - 1   ,   x   1 , t - 1   )   k (   x   1 , t - 1   ,   x   1 , t   )   k (   x   1 , t - 1   ,   x   1 , t + 1   )   \u22ef   k (   x   1 , t - 1   ,   x   1 , t + w   )     k (   x   1 , t   ,   x   1 , t - w   )   \u22ef   k (   x   1 , t   ,   x   1 , t - 1   )   k (   x   1 , t   ,   x   1 , t   )   k (   x   1 , t   ,   x   1 , t + 1   )   \u22ef   k (   x   1 , t   ,   x   1 , t + W   )     k (   x   1 , t + 1   ,   x   1 , t - w   )   \u22ef   k (   x   1 , t + 1   ,   x   1 , t - 1   )   k (   x   1 , t + 1   ,   x   1 , t   )   k (   x   1 , t + 1   ,   x   1 , t + 1   )   \u22ef   k (   x   1 , t + 1   ,   x   1 , t + w   )     \u22ee   \u22f1   \u22ee   \u22ee   \u22ee   \u22f1   \u22ee     k (   x   1 , t + w   ,   x   1 , t - w   )   \u22ef   k (   x   1 , t + w   ,   x   1 , t - 1   )   k (   x   1 , t + w   ,   x   1 , t   )   k (   x   1 , t + w   ,   x   1 , t + 1   )   \u22ef   k (   x   1 , t + w   ,   x   1 , t + w   )         Including a window of patterns around t increases the amount of local temporal information available to the model. However, it also increases the kernel size. The kernel defined in Eq. (14) is n * n, whereas the kernel defined in Eq. (17) is (n *((2* w)+1))*(n *((2* w)+1)). As w increases, the dimension of K increases by 2n. Therefore, it is preferable to keep w small. However, it should be noted that, in an online setting, this formulation is still efficient compared with using a kernel defined on all the data.  The formulation outlined above allows for local tuning of the model parameters. Given a model with S =288, 288 separate kernels are defined, each of which has its own set of parameters. The training of these parameters has no added computational cost over the training of a single set of parameters since the model still requires the inversion of a single kernel matrix at each time step. Therefore, each of the 288 models can be trained simultaneously. This enables the model to capture temporal heteroskedasticity by allowing the kernel bandwidth and the ridge parameter to vary by time of day. Parameter selection in kernel based models is an active research area, and various methods, mainly heuristic in nature, have been used to improve the parameter selection process. For example, in SVMs, genetic algorithms (\u00dcst\u00fcn et al., 2005; Wu et al., 2009; Cai et al., 2009; Li and Yang, 2008), chaotic immune algorithms (Wang et al., 2009a), particle swarm optimisation (Li et al., 2010), immune particle swarm optimisation (Wang et al., 2009b) differential evolution (Lahiri and Ghanta, 2008; Li and Cai, 2008) and ant colony optimisation (Zheng et al., 2008) have been used to select parameters, amongst others. Although this research has been instrumental in improving the speed of training of machine learning algorithms, in an applied setting it is desirable to have a parameter selection process that is interpretable and simple to implement, particularly when the model is to be applied in a large number of cases. In the following subsections, the selection method is described for each of the model parameters in the context of using a Gaussian RBF kernel, which are \u03bb, \u03c3 and w. The parameter space is derived from the data, making parameter selection straightforward in any application.  The regularisation constant  \u03bb  is a free parameter that needs to be tuned. Usually,  \u03bb  is determined by cross-validation over a space of values. It is sensible to try to limit this space of values in order to limit the computation time required to train models. Exterkate (2013) showed that appropriate values of  \u03bb  can be found by relating  \u03bb  to the signal to noise ratio \u03c6. First the R 2 is obtained from an OLS fit of y on X. If the OLS fit were the true model, then \u03c6 0 = R 2/(1\u2212 R 2). From this,    \u03bb   0    can be determined for a Gaussian kernel from the signal to noise ratio as    \u03c6   0   = 1 /   \u03bb   0    . Exterkate recommends the following grid be used to train  \u03bb  :  {   1   8     \u03bb   0   ,   1   4     \u03bb   0   ,   1   2     \u03bb   0   ,   \u03bb   0   , 2   \u03bb   0   }  . In the study, Monte Carlo simulation was used to compare the mean squared prediction error (MSPE) obtained from this grid with that obtained from the true values of  \u03bb  , and it was found that estimating  \u03bb  resulted in an increase in MSPE of only around 0.4%. It follows that the added computational effort associated with training a larger grid of values for  \u03bb  is not justified. The method of Exterkate is therefore recommended for tuning  \u03bb  in LOKRR.  The second free parameter in the LOKRR model is the kernel parameter. In the case of the Gaussian RBF kernel defined in Eq. (8), this is the kernel bandwidth \u03c3, which determines the smoothness of the forecasting function. A large value of \u03c3 results in a smooth function while a small value results in a more complex function. There is a wealth of literature on the training of \u03c3 in Gaussian kernels, particularly in the context of SVMs, and some of the methods that have been used to date were listed in Section 2.6. However, it is important to recognise that there is a small range of values within which \u03c3 can vary, which is based on the variance of the data in a given kernel. Caputo et al. (2002) showed that the optimal values of \u03c3 lie between the 0.1 and 0.9 quantiles of  | | x -   x   \u2032   |   |   2    and so can be estimated from the Euclidean distance between the data points in the kernel. This is the scheme used for automatic \u03c3 estimation for SVM models in the R package Kernlab (Karatzoglou et al., 2004, 2006). Exterkate (2013) proposed a similar method for selecting \u03c3 in the context of KRR.  While it is likely that more sophisticated training schemes will yield slightly more accurate results, the size of the improvement is unlikely to justify the increased computational burden of training extra parameter combinations. Therefore, in this study, the approach of Caputo et al. is used to estimate the median, 0.25 and 0.75 quantiles of each kernel, and these three values of \u03c3 are used to train each model. Not only does this reduce the size of the parameter space that needs to be searched, it also provides a principled way of training LOKRR models on other datasets.  The window size w is an important parameter that decides how much local temporal information to include in the model. The larger w gets, the more information is included in the model pertaining to each day of the training data. However, as mentioned in Section 2.5.2, an increase in w of 1 results in an increase in the dimension of the kernel of 2n. Therefore, it has a significant effect on the computation time required to train and update models. Holding the overall size of the kernel static, an increase in w necessitates a 2w +1-fold decrease in n. Therefore, a smaller w is desirable in order to maintain computational efficiency. The value of w is determined in the model training process and depends on the application. However, it is recommended to restrict its maximum value. The values tested here are w =1,2,3. Because of strong seasonal temporal autocorrelation in the data it is assumed that a maximum window size of 3 is sufficient. In other applications, a strategy of holding the kernel size static, and reducing the training length as w increases is recommended in order to examine the tradeoff between w and training data length.  Data normalisation, also referred to as scaling, is important in kernel methods because it brings all of the independent variables into the same range. This ensures that no single variable dominates over the rest of the variables when the distance between the training vectors is calculated in the kernel. There are various types of normalisation that could be used, including standardization to produce z-scores, scaling by domain (i.e. [0,1]) and minmax normalisation (Juszczak et al., 2002). Each kernel is normalised separately. This enables each kernel to capture the distribution of the data around time t, and brings the kernel parameters for each time point into the same range. This also enables the heteroskedasticity in the data to be examined through the model parameters. In the context of traffic data, it would be expected that the variance in \u03c3 would be higher during the peak periods than in the inter peak periods.  The data used in this study are Unit Travel Times (UTTs, seconds/metre) collected on London\u2019s road network, as part of the London Congestion Analysis Project (LCAP) coordinated by Transport for London (TfL). LCAP TTs are observed using automatic number plate recognition (ANPR) technology. Cameras operate in pairs, which are called links. The camera at the start of a link is called its start camera, and the camera at its end its end camera. A diagram of an ANPR link is shown in Fig. 3 .  Individual vehicle TTs are aggregated at 5min intervals to produce a regularly spaced time series with 288 observations per day. Due to the comparatively poor quality of data collected during the night time period, only data collected between 6AM and 9PM are used in the analysis (180 observations per day). To test the performance of the LOKRR model, the ten LCAP links with the lowest levels of missing data are selected. These links are shown in Table 1 , along with the average data frequency (number of vehicles per observation). The spatial location of the links in the network is shown in Fig. 4 . Links with low levels of missing data are selected in order to minimise its effects on the results.   Fig. 5 shows the time series of each of the test links over the course of 10weeks (weekends included). Evidence of the seasonal component in the data is present in the majority of links. However, there is significant variation from day to day. Higher unit journey times are observed in the AM and PM peak periods to varying degrees in all of the links, but there is clear heterogeneity in the magnitude (height) and the duration of the peaks from day to day. Between links there is also considerable variation in the observed traffic patterns. Links 26, 442, 2448 and 881 appear to have fairly stable traffic patterns. However, other links such as 1815 and 1799 are characterised by few very large peaks.  In total there are 154days for which data are available, collected between January and July 2011. To test the models, the data are divided into three sets; a training set, a testing set and a validation set, which are 80days (52%), 37days (24%) and 37days (24%) in length, respectively. The testing set runs from Tuesday 28th April to Wednesday 1st June 2011 and the validation set runs from Thursday 2nd June to Friday 8th July 2011. The sizes of the training and testing sets are fixed in this experiment for simple comparison of results with the benchmark models described in Section 3.6.  A sliding window validation approach is used to train the models. At the first time step, the training set is used to build a model to forecast the first day of the testing set. Following this, the first day of the training set is removed and replaced with the first day of the testing set. A model is then built using the new training set to forecast the second day of the testing set, and so on and so forth. The selected model is the one that minimises the average training error across the days in the testing set. To measure the training error, the root mean squared error (RMSE) index is used. This is defined as follows:  (19)  RMSE =   1      n   \u2211   t = 1   n     (   y   t   -     y   \u02c6     t   )   2       where yt and      y   \u02c6     t    are the observed and forecast values, respectively.  An autoregressive model structure is used to train the models. In an autoregressive model, m previous observations of the series are used to forecast the next value, where m is the embedding dimension. Assume a stream of input/output pairs {(x 1, y 1),(x 2, y 2),\u2026} is to be arranged into a vector of dependent variables y  t  =[yt , yt  \u22121,\u2026, yt  \u2212  N  +1]\u2032 and a matrix of independent variables X  t  =[x  t , x  t  -1,\u2026, x  t  -  N  +1]\u2032. In an embedded time series, the independent variable vector at time t is constructed as follows 2 An intercept term is also added in the LOKRR model, but is omitted from the description both for clarity and because it is not required for all time series models.   2 :  (20)    x   t   = [   y   t   ,   y   t - 1   , \u2026 ,   y   t - ( m - 1 )   ]   In this formulation, the temporal ordering of the data is not made explicit beyond the definition of the window w. Therefore, all data patterns in the kernel are considered equally and a forecast is made based only on the similarity of the data pattern observed at time t with the patterns in the historical data. It has been demonstrated in previous studies that including a time varying average as a variable in the model can improve the predictive performance of pattern based models (Smith and Demetsky, 1996). The sample time varying average      \u03bc   \u02c6     t    can be calculated as:  (21)      \u03bc   \u02c6     t   = 1      n    \u2211   i = 1   n      y   i , t     where n is the number of days in the training data. The mean is appended to the vector defined in Eq. (20) to give:  (22)    x   t   = [   y   t   ,   y   t - 1   , \u2026 ,   y   t - ( m - 1 )   ,     \u03bc   \u02c6     t   ]   Including      \u03bc   \u02c6     t    in the model introduces the notion of temporal ordering into the feature space defined by the kernel. Essentially, this has the effect of making a prior assumption that those patterns observed at near times to the current pattern are likely to be more informative than those observed at different times. At longer forecasting horizons and larger values of w, the effect of including the average variable is greater.  The performance of the LOKRR model is assessed in terms of its ability to forecast travel times at four forecast horizons, which are 15, 30, 45 and 60min ahead. The data are not aggregated to achieve these forecasts because this reduces the temporal granularity of the data. Instead, forecasts are made of 5min aggregated UTTs, at 3, 6, 9 and 12 steps into the future.  There are two strategies that can be used to make this type of forecast, which are iterated and direct. In the first strategy, a one step ahead forecast is made and the forecast point is fed back into the model and used to forecast the subsequent time step. This process is iteratively repeated until the desired number of time steps have been forecast. In the second strategy, forecasts are made directly. This is achieved by sampling the series at the desired temporal interval, and using this series to construct a model for each time interval. For example, given 5min averaged data, every second point is used to forecast at the 10min interval and every third point to forecast at the 15min interval. In this study, the second strategy is used as preliminary testing revealed that direct forecasting was more effective than iterated forecasting, both with the LOKRR model and the comparison models described in Section 3.6. Given an embedding dimension m and a time delay \u03c4, the embedded, delayed series x  t (\u03c4) is defined as:  (23)    x   t   ( \u03c4 ) = [   y   t   ,   y   t - \u03c4   ,   y   t - 2 \u03c4   , \u2026 ,   y   t - ( m - 1 ) \u03c4   ,     \u03bc   \u02c6     t   ]   A separate delayed, embedded series is constructed for each of the forecast intervals.  Along with RMSE, three additional performance measures are used. The NRMSE is defined as:  (24)  NRMSE =   RMSE     y   max   -   y   min       where y max and y min are the maximum and minimum values of the series, respectively. NRMSE is scale free, allowing comparison of performance between links. A feature of the RMSE is that it varies with the variability within the distribution of error magnitudes, the square root of the number of errors, and the average-error magnitude. It has been argued that an absolute error measure is more appropriate when comparing the performance of models (Willmott and Matsuura, 2005). In this case, the mean absolute percentage error (MAPE) is used as percentages are easy to interpret conceptually. The MAPE is defined as:  (25)  MAPE = 1      n    \u2211   t = 1   n           y   t   -     y   \u02c6     t       y   t          The mean absolute scaled error (MASE) is a generally applicable measurement of forecast accuracy (Hyndman and Koehler, 2006). The MASE compares the absolute errors in the forecast values with the absolute errors of a na\u00efve forecast, where the previous value of the series is used as the forecast of the next value:      y   \u02c6     t   =   y   t - 1    . As the na\u00efve model is the simplest possible model, any more complicated time series model should outperform it as a minimum requirement. The MASE is defined as:  (26)  MASE = 1      n    \u2211   t = 1   n         |   e   t   |     1   n - 1     \u2211   i = 2   n   |   y   i   -   y   i - 1   |        where et is the forecast error for a given period and the denominator is the average forecast error of the na\u00efve forecast method. A MASE value less than 1 indicates that the forecast model outperforms the na\u00efve model, while a value greater than 1 indicates the opposite.  The ARIMA model is a linear statistical time series model that has been widely used in many time series forecasting applications, including traffic forecasting. ARIMA is well described in the literature, and readers are directed to the texts of Box et al. (1994) and Hamilton (1994) for detailed treatments. ARIMA is commonly used as a benchmark model against which to judge the performance of time series forecasting models. An ARIMA(p, d, q) model is described by its autoregressive order p, its moving average order q, and the number of differences d. In time series with cyclical trends, a seasonal (S)ARIMA(p, d, q)*(P, D, Q) model is often used, where P is the seasonal autoregressive order, D is the order of the seasonal difference, and Q is the seasonal moving average order. To maintain consistency with the LOKRR approach, the multi-step forecasts are made in a single step, rather than recursively. 3 This approach was found to produce markedly superior results to the iterated approach in preliminary testing.   3 As the model building procedure of ARIMA requires a continuous time series, \u03c4 embedded, delayed series are constructed and concatenated to form a single series on which the model is trained. For example, consider a time series with its origin at t =0. If \u03c4 =3, three separate series are constructed, one beginning at t =0, one at t =1 and one at t =2, which are merged into a single series, under the reasonable assumption that each series will have the same properties.  The function auto.arima in the R package forecast is used to train the model (Hyndman and Khandakar, 2007). The Akaike Information Criterion (AIC) is used to determine the parameters of the ARIMA models, which is a common alternative to the Box\u2013Jenkins procedure (Ozaki, 1977). Nonseasonal and seasonal model specifications are considered in the model building procedure.  SVR is a kernel based machine learning method that has demonstrated strong performance in forecasting of traffic variables in recent years. An excellent introduction to SVR is given in (Smola et al., 2004). SVR is similar to KRR, but has the advantage of maintaining a sparse representation of the solution. The \u03b5-insensitive variant of SVR (\u03b5-SVR) is used here. There are a minimum of three parameters to train in an \u03b5-SVR model, which are:  (1) The kernel parameter(s).  The regularisation constant C (similar to \u03bb in KRR).  The width of the tube \u03b5.  In this case, to maintain consistency, the same RBF kernel is used as shown in Eq. (8) and the kernel parameter \u03c3 is determined using the same method as the LOKRR model. The value of C is varied in the range C ={0.1,1,10,100}. \u03b5 is varied in the range \u03b5 ={0.0001,0.001,0.01,0.1}. After initial feasibility testing, the kernel size is limited to 70days to ensure training times are acceptable. This means a kernel of size 180*70=12,600 is created for each link. SVR is implemented using the kernlab package in R (Karatzoglou et al., 2004).  The ANN is the most widely applied traffic forecasting method in the literature. In time series forecasting, ANN models with recurrent architectures have displayed the best results. The Jordan (1986) and Elman (1990) networks are two examples of these. Here, the Elman network is selected as the comparison model as a preliminary analysis revealed superior performance over the Jordan network on this dataset. It must be noted, however, that different ANN structures may yield different results to the Elman network presented here. There is one parameter to be determined in the Elman network, which is the number of input layers. Here, the number is varied between 1 and 10. To train the Elman network, the RSNNS library in R is used, which is based on the Stuttgart Neural Network Simulator (Bergmeir and Ben\u00edtez, 2012).   RESULTS    Table 2 shows the training errors for each of the links at each of the forecast intervals. RMSE is the error criterion used for model selection. The NRMSE, MAPE and MASE are shown to aid interpretability of the results. The results in terms of NRMSE and MAPE reveal considerable variation in forecast accuracy between links. For instance, at the 15min interval, the best MAPE for link 24 is 24.91, whereas the best MAPE for link 1815 is 4.7. The errors increase as the forecasting interval increases, which is to be expected as there is less local temporal information available at longer forecast intervals. However, even at the 60min interval, most of the links exhibit MAPE values of between 10 and 20.  Examining the performance in terms of MASE, it can be seen that all of the models meet the minimum requirement of outperforming the na\u00efve model at each of the forecast horizons, with the exception of link 1815. In general, the performance of LOKRR in terms of MASE improves as the forecasting horizon increases. This implies that the model is successful in capturing the cyclic variation in the data, which is not captured by the na\u00efve model at increasing forecast horizons.  The parameters obtained in the training phase are used to forecast the remaining 37days in the validation set. Table 4 shows the testing results. In terms of RMSE, the training and testing errors are broadly similar. At the 15, 30 and 60min intervals, testing performance is better than training performance on 4 of the 10 links. At the 45min interval, testing performance is better than training performance on 5 of the links. The reason for the differences in the training and testing errors may reflect the relative forecastability of the data in each period. The MASE gives a good indication of performance in this regard. For each of the links, the MASE is <1, with the exception of link 1815. The pattern of decreasing MAPE with increasing forecast interval observed in the training phase is repeated in the testing phase. These results indicate that the model generalises well to the unseen validation data.   Table 5 shows the average errors of the LOKRR model and the comparison models in the training and testing periods at each of the forecast intervals. In terms of RMSE, the LOKRR model is the best performing model in both the training and testing phases at each forecast horizon.  In terms of MAPE, LOKRR outperforms each of the comparison models at the 15min, 30min and 45min horizons. The SVR model performs marginally better at the 60min horizon in terms of average MAPE. Of the comparison models, ARIMA performs best at shorter forecast horizons, but its performance decreases rapidly as the forecast horizon increases. The ANN and SVR models outperform ARIMA at the 45min and 60min horizons.   Fig. 6 shows the RMSE of each of the models at the four forecasting horizons. LOKRR is the best performing model in terms of RMSE on 5, 9, 9 and 10 of the links at the 15, 30, 45 and 60min forecasting horizons respectively, highlighting the consistency in the performance of the model. The ARIMA model exhibits the best performance on 4, 1 and 1 of the 10 links at the 15, 30 and 45min forecast horizons respectively. The SVR and ANN models exhibit best performance on a single model each.   Fig. 7 shows the MAPE of each of the models at the four forecasting horizons. In terms of MAPE, LOKRR is the best performing model on 7, 6, 6 and 5 of the links at the 15, 30, 45 and 60min horizons respectively. SVR also performs strongly, achieving the lowest MAPE on 3, 3, 1 and 3 of the links at the 15, 30, 45 and 60min horizons respectively. ANN is the best performing model on 1, 3 and 2 of the links at the 30, 45 and 60min intervals respectively. The ARIMA model does not perform well in terms of MAPE, and is not the best performing model in any of the cases.  An intuitive way to judge the performance of a forecasting model is by visual inspection of the observed series against the forecast series. For brevity, we focus on a selection of links to demonstrate the strengths and weaknesses of the LOKRR model compared with the benchmark models. Fig. 8 shows the observed versus forecast data for the LOKRR model and each of the benchmark models on Link 454, over the course of a week, at the 15min forecast interval. On first glance, the performance of each model appears similar. However, on closer inspection, the LOKRR model performs significantly better in forecasting the onset of the congested afternoon peak period. The comparison models all exhibit a 15min lag in forecasting the rise in UTT. This is because they are reactive models. In contrast, LOKRR is often able to forecast the onset of congestion with little or no time lag. This improved performance results from the temporal locality of LOKRR.  Weekends and weekdays have not been differentiated in the model specification described in this paper. Therefore, it is important to examine the performance of the model on weekend days. Fig. 8(e) and (f) shows the performance of the model on the weekend. On both Saturday and Sunday, LOKRR forecasts no peak in the afternoon, although there is a small spike to a UTT of around 0.3 on the Saturday (compared with around 0.8 on weekdays). This result demonstrates that, at the 15min interval, LOKRR is able to forecast both weekdays and weekends without explicitly accounting for their different characteristics in the model specification.   Fig. 9 ((a)\u2013(g)) shows the performance of the model on the same link and days at the 60min forecast interval (30 and 45min intervals are not shown for brevity). As expected, both the LOKRR model and the comparison models perform less well as the forecast interval increases. Most notably, none of the models are able to forecast the peaks that occur in the mornings of the Wednesday and the Sunday, shown in Fig. 9(c) and (f), respectively. However, the LOKRR model is still able to forecast the onset of congestion in the afternoon peak with little or no delay, whereas the comparison models forecast congestion 60min later, when it has already begun to decrease. Moreover, the comparison models greatly underestimate the level of congestion in the peak. This result clearly illustrates the advantage of incorporating temporal locality in the model structure. In general, the weekend forecasting performance of the LOKRR model degrades at longer forecast intervals. It can be seen in Fig. 9e that, in contrast to the result at the 15min interval, LOKRR erroneously forecasts a peak on the Saturday. This problem could be overcome by building separate kernels for weekends and weekdays, or by incorporating a mean variable that varies with day of week as well as time of day.  Accurate forecasting is of particular importance during periods of non-recurrent congestion, where the level of traffic is unexpectedly high. Fig. 10 shows the performance of the LOKRR model compared with the benchmark models on two separate days on link 442. Fig. 10(a) demonstrates the performance in forecasting a typical weekday at the 15min forecast horizon. It can be seen that the LOKRR, SVR and ARIMA models exhibit broadly similar performance in this case, while the ANN model underestimates the peaks. Fig. 10(b) shows the performance of the models during an abnormal congestion event taking place on a weekend, where the UTT is roughly twice what it would normally be. This event may be due to an incident, or a special event taking place in the vicinity of the link. In this case, the LOKRR model is better able to forecast the evolution of the congestion event than the comparison models. ARIMA is the best performing competitor in this case as it forecasts only the residual series.   Fig. 11 shows the forecasting performance on the same link and same days as Fig. 10 at the 60min forecast horizon. Fig. 11(a) shows the relative performance of each of the models on the typical weekday shown in Fig. 10(a). The important feature to note is that, again, the LOKRR model is capable of forecasting the afternoon peak with less of a delay than the other models. This is consistent with the result for link 454, and is due to the fact that knowledge of the recurrent nature of traffic is built into the model. Essentially, LOKRR balances prior knowledge of traffic phenomena with observations of local conditions.   Fig. 11(b) shows the performance of the models during the abnormal congestion event. All four of the models exhibit a delay in forecasting the onset of congestion of 1h (12 points in the figure). This is to be expected because in the univariate setting is not possible to forecast congestion events that begin after a forecast has been made. However, the LOKRR model forecasts the size and duration of the peak more accurately than the other methods. The SVR model fails in this instance, with the most likely reason being that the set of support vectors determining the solution eliminates congestion of this magnitude. LOKRR performs particularly well in forecasting the end of the congestion event. The time delay is much less than the 1h delay exhibited by the other models. This is because LOKRR only uses those traffic patterns that were observed around the same time on previous days.   Table 3 summarises the best parameters for each link and forecast interval. All of the models exhibit best performance with  \u03bb = 1 / 8   \u03bb   0    . This suggests that the method outlined in Section 2.6.2 for choosing  \u03bb  may overestimate the noise in the data, and that smaller values of  \u03bb  could be used (for example:  \u03bb =   1   16   \u03bb   0     ,   1   32   \u03bb   0      ). The best values of \u03c3 vary between the 0.25, 0.5 and 0.75 quantiles of  | | x -   x   \u2032   |   |   2    . As the forecasting horizon increases, there is a tendency towards larger kernel bandwidths. This reflects the fact that the uncertainty in the forecasts increases with forecasting interval, so the models tend towards the historical average. With sufficiently large \u03c3, each forecast would reduce to the average of the observations contained within w.  The model parameters \u03c3 and  \u03bb  of the LOKRR are local and vary throughout the day. Each time point has a different value of \u03c3 and  \u03bb  which is determined from the data. Fig. 12 shows the calculated values of the 0.25, 0.5 and 0.75 quantiles of \u03c3 at each time point and each link.  The median value of \u03c3 tends to be higher in the morning and evening peak periods than other times of day. This is particularly evident on links 442, 1798 and 1799. In general, \u03c3 is not static in time, which reflects the fact that the distribution of the data is conditional on the time of day. The finding of larger values of \u03c3 in the peak periods is consistent with empirical studies of traffic data, which have demonstrated that the mean/median and variance/interquartile range of travel times is higher in peak periods (e.g. (Fosgerau and Fukuda, 2012). However, not all of the links exhibit the same two peak profile. This is because the traffic patterns of urban road links depend on many factors, including whether they are arterial (inbound or outbound) or radial routes. The LOKRR model is able to capture these differences.   Fig. 13 shows the average value of    \u03bb   0    by time of day for each of the test links. The estimated values of    \u03bb   0    are higher around the peak periods. Recalling that the values of    \u03bb   0    are obtained from the signal to noise ratio \u03c6 derived from the OLS fit of the data, this result indicates that there is a higher level of noise in the data around the peak periods. This reflects the basic intuition that traffic conditions are more difficult to predict in peak times. This is particularly marked on link 1815, which is characterised by rare, but very extreme peaks in the morning rush hour period. In this case, the value of    \u03bb   0    rises to around 80 because OLS cannot model the highly nonlinear input output relationship in this period. The variation in travel times is interpreted as noise. The implication of the higher values of    \u03bb   0    in the peak periods is that forecasts will tend to be closer to the average because high values of  \u03bb  penalise large weights. This is undesirable when forecasting non-recurrent congestion events. However, searching the grid of multiples of    \u03bb   0    in a cross-validation approach circumvents this problem. On other links, the values of    \u03bb   0    are higher during the early morning and late evening periods. In these cases, the parameter values correctly identify the increased noise in the data at times of lower flow that is caused by fewer vehicles being observed by the ANPR cameras.  The sensitivity of model parameters is an important concern in forecasting models because it determines how easily they can be trained. In this context, the meaning of parameter sensitivity is the effect that a change in the value of a parameter has on the performance of the model. As mentioned in Section 2.6, there is an extensive literature on parameter selection in kernel methods such as SVR and KRR. However, in practice one may be willing to accept a suboptimal model if the advantage gained in training time reduction or ease of implementation outweighs the potential advantage of increased accuracy. This is a feasible option if the parameters are insensitive, and good results can be guaranteed within a small range of values.   Fig. 14 shows the effect of varying \u03c3 through its range, while keeping  \u03bb  fixed in a forecasting situation. The data shown here are on link 442 over the course of a single week. In the figures, the large peak in UTT on the Friday represents an abnormal congestion event. The effect of varying  \u03bb  on model performance is significant. Choosing a large value of  \u03bb  results in an overestimation of the noise in the data and an overly smooth solution. Consequently, models using large values of  \u03bb  do not perform strongly in forecasting abnormally high peaks in travel times. This result, combined with the model parameters of the fitted models shown in Table 3, indicates that it is not necessary to test larger values of  \u03bb  , supporting the intuition of Exterkate (2013, p. 6) that  \u03bb  should be less than    \u03bb   0    because one expects to obtain a better fit using nonlinear models.   Fig. 15 shows the effect of holding \u03c3 fixed and varying  \u03bb  . Fig. 15((a)\u2013(c)) all appear broadly similar, indicating that the effect of varying \u03c3 is not as great as the effect of varying  \u03bb  . This supports the statement of Caputo et al. (2002) that any kernel bandwidth between the 0.1 and 0.9 quantiles of the kernel will produce good results. The implication of this is that the performance of LOKRR is relatively insensitive to Gaussian kernel bandwidth choice when the method described in Section 2.6.2 is used. Similar results are found on the other links in the network.  Some conclusions can be drawn from the sensitivity analysis that make model selection more straightforward. Firstly, small values of  \u03bb  always yield greater accuracy than large values, meaning it is unnecessary to test large multiples of    \u03bb   0    . Secondly, although performance varies with choice of \u03c3, a good fit to the data can be obtained with the limited parameter range suggested.  In this paper, a temporally local kernel based forecasting model, namely LOKRR, has been developed for forecasting traffic series. To incorporate the cyclic patterns inherent in traffic data, a separate kernel with individual parameters is defined for each time point. This structure has two main benefits. Firstly, it enables smaller kernels to be accessed each time a forecast is made, making the algorithm efficient. Secondly, it allows the model to capture the seasonality and heteroskedasticity in the series. The empirical case study demonstrates the strong performance of the model in forecasting up to 1h ahead. This ability makes the model suitable for real time application, where accurate forecasts of future traffic conditions are important.  The strength of LOKRR lies in its ability to model the cyclic variation in travel times throughout the day. Each forecast is a weighted nonlinear combination of previously observed travel times from within the temporal window w. This constrains the model to only produce forecasts based on what has happened during this window in the past. Therefore, it naturally captures the cyclic pattern in the data. At shorter forecasting intervals, the model is able to use recent traffic patterns to accurately forecast the variation above or below the average conditions. As the forecast horizon increases and more uncertainty is introduced to the forecasts, forecasts tend towards the historical average. However, LOKRR is still able to produce more accurate forecasts than the historical average, indicating that it captures the local conditions even at longer forecast horizons. The ability to choose parameters from a small range of initial values determined from the data is also advantageous.  The model has a number of limitations that it shares with the benchmark models. Firstly, it cannot forecast the onset of abnormal congestion before it is observed in the data. This problem can be addressed either by using a spatio-temporal approach or by including additional information such as incident data. KMs are inherently suited to incorporating data from various sources, and this will form the focus of future research. Currently, a spatio-temporal extension of LOKRR, namely space\u2013time (ST) LOKRR is being developed. The main challenge in this regard lies in determining what spatio-temporal information is relevant for forecasting traffic on a given link at a particular time. This is a problem of spatio-temporal neighbourhood selection (Haworth and Cheng, 2014), and the authors have tackled this in the context of statistical modelling (Cheng et al., 2014). Secondly, the method for online removal and addition of data from the kernel is arbitrary, and future research will focus on methods for selective data addition/removal. This problem has been addressed recently in the context of the KRLS algorithm, and could be applied to LOKRR (Van Vaerenbergh et al., 2010, 2012). Finally, the time of day varying structure specified in this paper can be relaxed to include many types of time varying structure. For example, separate kernels can be defined for weekends and weekdays, or longer periods such as peak and non-peak. Alternatively, traffic patterns could be grouped according to states such as congested and free flowing. This would provide a less rigid temporal structure, but would require an additional classification step.  At the time of writing, the LOKRR model has not yet been transferred into operation. This step necessitates calibrating the model locally for many hundreds or possibly thousands of measurement locations, LOKRR is computationally efficient for a single location because it requires small kernels to be evaluated at each time point. However, when implemented serially, the computational time scales linearly with the number of locations to be forecast. It is necessary, therefore, to take a parallel approach to model training and implementation. The LOKRR model has the embarrassingly parallel property, meaning that the model for each location is independent of the models for other locations (Harris et al., 2010). This makes LOKRR suitable for parallel implementation. In this study, and in our other work on this network, the UCL Legion High Performance Computing Facility (Legion@UCL) has been used for model training, enabling many locations to be modelled simultaneously. The way in which parallelism is exploited depends on the hardware available to the practitioner, but cloud computing resources and graphics processing units (GPUs) are becoming available at decreasing cost. Routines optimised for parallel technology that can carry out many of the required computations are readily available. Currently, the authors are working on a GPU based implementation of LOKRR.   ACKNOWLEDGEMENTS   We thank Transport for London for providing the travel time data. The research for this article was carried out under the STANDARD project, sponsored by the U.K. Engineering and Physical Sciences Research Council (EP/G023212/1). Comments from the editor and the anonymous reviewers are greatly appreciated and improved the content and readability of the article. The authors bear sole responsibility for any mistakes that may appear.   REFERENCES", "highlights": "Accurate and reliable forecasting of traffic variables is one of the primary functions of Intelligent Transportation Systems. Reliable systems that are able to forecast traffic conditions accurately, multiple time steps into the future, are required for advanced traveller information systems. However, traffic forecasting is a difficult task because of the nonlinear and nonstationary properties of traffic series. Traditional linear models are incapable of modelling such properties, and typically perform poorly, particularly when conditions differ from the norm. Machine learning approaches such as artificial neural networks, nonparametric regression and kernel methods (KMs) have often been shown to outperform linear models in the literature. A bottleneck of the latter approach is that the information pertaining to all previous traffic states must be contained within the kernel, but the computational complexity of KMs usually scales cubically with the number of data points in the kernel. In this paper, a novel kernel-based machine learning (ML) algorithm is developed, namely the local online kernel ridge regression (LOKRR) model. Exploiting the observation that traffic data exhibits strong cyclic patterns characterised by rush hour traffic, LOKRR makes use of local kernels with varying parameters that are defined around each time point. This approach has 3 advantages over the standard single kernel approach: (1) It allows parameters to vary by time of day, capturing the time varying distribution of traffic data; (2) It allows smaller kernels to be defined that contain only the relevant traffic patterns, and; (3) It is online, allowing new traffic data to be incorporated as it arrives. The model is applied to the forecasting of travel times on London\u2019s road network, and is found to outperform three benchmark models in forecasting up to 1h ahead."}