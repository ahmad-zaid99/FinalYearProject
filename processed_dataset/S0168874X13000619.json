{"id": "S0168874X13000619", "article": "MAIN-TITLE Eigensensitivity analysis of damped systems with distinct and repeated eigenvalues   HIGHLIGHTS          A combined normalization which combines two traditional normalizations is presented.      Design sensitivity analysis of viscously damped eigenproblems is studied.      A compact and well-conditioned algebraic method in case of distinct eigenvalues is proposed.      An N-space algorithm for computing eigensensitivity with repeated eigenvalues is proposed.          KEYPHRASES   Design sensitivity analysis  Eigensensitivity  Damped systems  Repeated eigenvalues  Floating raft   Design sensitivity analysis of engineering structures deals with the computations of the rate of performance measure from changes in the design parameters describing the structure. A significant body of research has been devoted to the computation and application of design sensitivity analysis (see, e.g., van Keulen et al. [1], Adelman and Haftka [2], Haftka et al. [3], Haug et al. [4] or Choi and Kim [5] or Chen [6]). Computational methods for design sensitivity analysis have received much attention over the past four decades, particularly those related to the eigenvalue problems. The eigensolution sensitivities of structural and mechanical systems with respect to structural design parameters have become an integral part of many engineering design methodologies including structural design optimization, structural health monitoring, structural reliability, dynamic model updating, structural dynamic modification, approximate reanalysis techniques and many other applications. Although computing eigenvalue sensitivity is straightforward, finding eigenvector derivatives raises several challenges, due in part to the singularity problem and repeated eigenvalues.  The modal method [7,8] evaluated the derivative of each eigenvector as a linear combination of the whole eigenvectors. Adhikari [9,10] and Adhikari and Friswell [11] derived N-space modal methods of viscously damped systems without using 2N-space (state-space) formulation where N is the system dimension. Although these 2N-space methods are exact in nature, the 2N-space approach usually needs heavy computational cost for real-life multiple degree-of-freedom (DOF) systems due to the size of system matrices of state-space equations is double. Adhikari [12] first extended the modal method to the more general symmetric and asymmetric systems with nonviscous (viscoelastic) damping. To guarantee the accuracy of the derivative of each eigenvector, the modal method needs a linear combination of all the eigenvectors, which is a significant computational task. Often only the lower order frequencies and associated mode shapes are calculated. That is, approximated sensitivity may be evaluated depending on the number of modal terms. Corrections to the model truncation problem have been investigated by several authors [6,13\u201317].  Fox and Kapoor [7] also suggested a direct algebraic method, which calculates the eigensolution derivatives by assembling the derivatives of eigenproblems and the additional constraints obtained from the derivative of normalization condition into a linear system of algebraic equations. Garg [18], Rudisill [19], Rudisill and Chu [20] investigated the algebraic method to address the problem of asymmetric eigensystems. Lee and Jung [21,22] derived an algebraic method with symmetric coefficient matrices to solve the eigensolution derivatives. But these methods [18\u201322] are only restricted to the first-order representation of the equation of motion. Later, Lee et al. [23,24] further extended their algebraic method to symmetric systems with viscous damping. Choi et al. [25] developed their method for the second- and higher-order derivatives of eigensolutions for symmetric systems with viscous damping. Unfortunately, Wu et al. [26] pointed out that these methods [22,24,25] were not correct since a mistake was made in the computation of derivatives of the normalization condition for the eigensystems with repeated eigenvalues. Choi et al. [27], Guedria et al. [28], Chouchane et al. [29] and Xu et al. [30] further extended the algebraic method to compute the eigensolution sensitivities of asymmetric viscously damped systems. Recently, Li et al. [31] extended the algebraic method to the more general nonviscous damped systems. Li et al. [32] developed an algebraic method for asymmetric nonviscously damped systems, which can evaluate the sensitivities of eigenvalues and eigenvectors without using the left eigenvalues.  Another efficient method proposed by Nelson [33] finds the eigenvector derivatives with distinct eigenvalues for undamped eigensystems by expressing the derivative of each eigenvector as a linear combination of a particular solution and a homogeneous solution of the singularity problem. The particular solution can be found by identifying the element of the corresponding eigenvector with the largest absolute value and constraining its derivative to zero. It is worth mentioning that each eigenvector can always satisfy the singularity problem and is therefore considered as a homogeneous solution of its derivative. The arbitrary constraining the derivative of the largest absolute value can be compensated by the computation of the coefficient of the homogeneous solution. Sutter et al. [34] pointed out Nelson's method is more efficient than the modal method for the reason that the modal method needs all or most of the eigenvectors to find the eigenvector derivatives. Friswell [35] developed Nelson's method to compute the second- and higher-order eigensolution derivatives for undamped systems. Later, Friswell and Adhikari [36] extended Nelson's method to viscously damped systems. Guedria et al. [37] extended Nelson's method to second-order derivatives of eigensolutions of viscously damped systems. Recently, Adhikari and Friswell [38] extended Nelson's method to nonviscously damped systems. More recently, Omenzetter [39] extended Nelson's method to general nonlinear eigensystems. Although Nelson's method mentioned above gives exact results and only needs the eigenpairs of interest, these methods are clumsy for programming.  Jankovic [40] gave the exact analytical solutions for the first- and higher-order derivatives of eigensolutions of linear and nonlinear eigenproblems. Murthy and Haftka [41] surveyed the methods for sensitivity analysis of the eigenproblem with general non-Hermitian matrices. Mottershead et al. [42] pointed out that the eigensensitivity-based method is probably the most successful approach to model updating. Other methods have been developed for the calculation of the sensitivity of mode shape including the iterative method [43,44], QR-based method [45,46], Davidson-based method [47], the combination method [48], the finite difference method [49,50] and the substructuring method [51\u201353].  The methods [7\u201353] to compute eigensensitivity are only restricted to the case of distinct eigenvalues. Choi and Kim [5] pointed out that repeated eigenvalues are far more likely to happen in optimized structures. Thompson and Hunt [54] paid attention to optimal designs that are constructed with repeated eigenvalues. Olhoff and Rasmussen [55] showed that a repeated buckling load may occur in an optimized clamped-clamped column. Ojalvo [56], Mills-Curran [57], Dailey [58] developed Nelson's method for solving the derivatives of eigensolutions of undamped eigensystems with repeated eigenvalues. Methods [56\u201358] are based on deleting rows and columns of the singular system to form a reduced coefficient matrix. Unfortunately, the methods [56,58] may fail in some circumstances (see Mills-Curran [59] for details). Shaw and Jayasuriya [60] generalized the methods [56\u201358] to compute the eigensolution derivatives in the case of repeated eigenvalues with repeated first-order derivatives. Tang et al. [61,62] investigated the eigensolution derivatives of general asymmetric systems with repeated eigenvalues. Later, Xu and Wu [63] presented a new normalization and developed a method for the computation of eigensolution derivatives of asymmetric viscously damped systems with distinct and repeated eigenvalues. More recently, Li et al. [64] suggested a new normalization for the left eigenvectors, from which the left and right eigenvector derivatives can be computed separately and independently for asymmetric viscously damped systems with distinct and repeated eigenvalues. Li et al. [65] showed the undamped, viscously or nonviscously damped eigenproblems can be considered as a degenerated case of general nonlinear eigenproblems and developed a unified eigensensitivity method for both distinct and repeated eigenvalues. However, most of the existing methods to obtain the derivatives of the complex eigensolutions with repeated eigenvalues employ first-order (2N-space) formulation.  In this study, the symmetric eigenproblems with viscous damping is considered. To simplify the computation of eigensolution derivatives, a new combined normalization, which combines two traditional normalizations, is presented. The aim of this paper is to propose a compact method to calculate eigenvector derivatives directly by constructing the coefficient of the homogeneous solution to be zero in the case of distinct eigenvalues. In the case of repeated eigenvalues, an N-space algorithm is presented for computing the eigensolution sensitivity such that it is easy to be implemented and need lower computational cost.  Consider the formulation for structural modal analysis described by a linear viscously damped symmetric system with N DOF  (1)  (   \u03bb   i   2   ( p ) M ( p ) +   \u03bb   i   ( p ) C ( p ) + K ( p ) )   \u03c6   i   ( p ) = 0  for  i = 1 , 2 , \u2026 , N   where M ( p ) , C ( p ) and K ( p ) \u2208   \u211d   N \u00d7 N   are the mass, damping and stiffness matrices, respectively, whose components are assumed to be continuously differentiable with respect to the design parameter p. \u03bb  i is the ith eigenvalue and   \u03c6   i   is the ith eigenvector. For underdamped systems, the eigenvalues associated with the above eigenproblem appear in complex conjugate pairs. Often the following normalization is adapted  (2)    \u03c6   i   T   ( p ) ( 2   \u03bb   i   ( p ) M ( p ) + C ( p ) )   \u03c6   i   ( p ) = 1   The arbitrariness of eigenvectors can be removed by this normalization condition which has been widely applied in the design sensitivity analysis of symmetric damped eigenproblems.  The paper is concerned with the derivatives of eigenvalues and associated eigenvectors at p=p 0, and hereafter \u201cp 0\u201d is omitted for variables evaluated at p=p 0. The eigensolutions of viscously damped symmetric systems and the derivatives of system matrices (the mass, damping and stiffness matrices) are assumed to be known. For convenience, the following notation is adopted     ( \u2022 )   , p   \u2261   \u2202 ( \u2022 )   \u2202 p   ,    ( \u2022 )   , p p   \u2261     \u2202  2  ( \u2022 )   \u2202   p  2         Differentiating Eq. (1) with respect to the design parameter p and evaluating them at p=p 0 yields  (3)    F   i     \u03c6   i , p   =   h   i     where     F   i   \u2261   \u03bb   i   2   M +   \u03bb   i   C + K         h   i   \u2261 \u2212 ( 2   \u03bb   i   M + C )   \u03c6   i     \u03bb   i , p   \u2212 (   \u03bb   i   2     M   , p   +   \u03bb   i     C   , p   +   K   , p   )   \u03c6   i        Premultiplying each side of Eq. (3) by   \u03c6   i   T   , the eigenvalue derivatives can be obtained [11]   (4)    \u03bb   i , p   = \u2212     \u03c6   i   T   (   \u03bb   i   2     M   , p   +   \u03bb   i     C   , p   +   K   , p   )   \u03c6   i       \u03c6   i   T   ( 2   \u03bb   i   M + C )   \u03c6   i          By utilizing normalization (2), this formula can be simplified as [9,10]   (5)    \u03bb   i , p   = \u2212   \u03c6   i   T   (   \u03bb   i   2     M   , p   +   \u03bb   i     C   , p   +   K   , p   )   \u03c6   i        The derivatives of the eigenvectors   \u03c6   i , p   cannot be determined directly by utilizing Eq. (3), because the matrix F  i is singular due to the fact that the rank of the matrix F  i of order N is (N\u22121) for systems with distinct eigenvalues (often it is called the singularity problem).  Eigensensitivity analysis for viscously damped systems has received much attention over the past two decades. However, previous works have focused primarily on eigenvectors normalized by Eq. (2) alone. Alternatively, one can choose other kinds of normalizations, e.g.  (6)    {    \u03c6   i   ( p )  }     n   i     = 1   Here   {    \u2022   i    }   e   denotes the eth component of vector   \u2022   i   and   n   i   is selected by finding   {    \u03c6   i   ( p )  }     n   i     which is the largest element in absolute value on the column of   \u03c6   i   ( p ) . Following the procedures of Nelson's method [36] by using normalization (6), it is easy to verify that the eigenvector derivatives can be determined directly due to the fact that the coefficient of the homogeneous solution is equal to zero. Caution must be exercised here, the eigenvalue sensitivities must be evaluated from Eq. (4), and the compact formula (5) does not hold.  This section proposes a new normalization which attempts to determine the eigenvector sensitivities directly and holds the compact formula (5) of the eigenvalue sensitivities. For p=p 0, we assume the new normalization is same with the traditional normalization Eq. (2). To express it clearly, we give the form  (7)    \u03c6   i   T   ( 2   \u03bb   i   M + C )   \u03c6   i   = 1   The normalization is usually presumed to be a differentiable function, at least in the neighborhood of the current design point, i.e., the point at p near p 0. Here we suggest that the new normalization satisfies the following form for p near p 0   (8)    {    \u03c6   i   ( p )  }     n   i     =   {    \u03c6   i    }     n   i       It means that the largest element in absolute value on each eigenvector is a constant for p near p 0. When p=p 0, normalization Eq. (8) is always satisfied, and normalization Eq. (7) can be imposed to guarantee the unique solutions of eigenvectors at p=p 0. So the combined normalization is consistent. Compared to the traditional normalization Eq. (2), the combined normalization not only preserves the compact formula of eigenvalue sensitivities expressed by Eq. (5), but also determines the eigenvector derivatives directly (the details can be seen in the following subsections).  For structural optimization, optimal solution usually need be obtained by calculating the derivatives many times. If the combined normalization is used, the eigenvectors should be renormalized at the beginning of each iteration, which seems to be clumsy for programming. However, it is clear that the complexity of implementation and the computational effort of renormalization are less than those of the computation of the homogeneous solution of Nelson's method [36,37]. Consequently, design sensitivity analysis based on the combined normalization, will be also efficient and easy to be implemented in optimal design.  This section assumes that all eigenvalues and eigenvectors are distinct, that is, they are not repeated. A separate section will be devoted to repeated eigenvalues.  For the foregoing discussions, the derivatives of eigenvectors   \u03c6   i , p   cannot be found directly by utilizing Eq. (3). Therefore, an additional constraint should be imposed to uniquely determine the derivatives of eigenvectors. It is customary to obtain the constraint by differentiating normalization Eq. (2). Here, the constraint can be derived by differentiating normalization Eq. (8)   (9)    {   \u03c6   i , p   }     n   i     = 0      To apply the above constraint, construct    F \u02dc    i   by zeroing out row n  i and column n  i of F  i , but keep the n  i th diagonal element of F  i , and construct    h \u02dc    i   by zeroing out the n  i th elements of h  i . The linear system of algebraic equations for solving the eigenvector derivatives can be given by  (10)     F \u02dc    i     \u03c6   i , p   =    h \u02dc    i     Note that the coefficient matrix of the above system is a symmetric matrix of dimension (N\u00d7N), and has exactly the same band structure as the original system. The formula of Eq. (10) is similar with the computation of the particular solution of Nelson's method [33,36], but the proposed method need not calculate the homogeneous solution. That is, the proposed method can find the eigenvector sensitivities directly. Hence the presented method is compact and easy to be implemented. In addition, Nelson's method [33,36] suggests the n  i th diagonal element of    F \u02dc    i   is set to unity, under such operations, sometimes the components in the coefficient matrix    F \u02dc    i   will be not all of the same order of magnitude, which often causes a big condition number. Under the circumstances, there is small perturbation of the input which leads to a big change in the output (similar study can be seen in Li et al. [66]). To reduce the condition number, we keep the n  i th diagonal element of    F \u02dc    i   . In this sense, the presented method can be more robust than Nelson's method. The similar formula is also presented in Li et al. [64] for asymmetric systems with viscous damping. One of the remarkable characteristics of the proposed method is that its numerical stability is also proved, as demonstrated in Section 4.  Differentiating Eq. (3) with respect to the design parameter p, the second-order sensitivities of eigensolutions satisfy  (11)    F   i     \u03c6   i , p p   =   h   i , p   \u2212   F   i , p     \u03c6   i , p     where     F   i , p   \u2261   \u03bb   i   2     M   , p   +   \u03bb   i     C   , p   +   K   , p   + ( 2   \u03bb   i   M + C )   \u03bb   i , p           h   i , p   \u2261 \u2212 ( 2   \u03bb   i   M + C )   \u03c6   i     \u03bb   i , p p   \u2212 (   \u03bb   i   2     M   , p   +   \u03bb   i     C   , p   +   K   , p   )   \u03c6   i , p   \u2212 (   \u03bb   i   2     M   , p p   +   \u03bb   i     C   , p p   +   K   , p p   )   \u03c6   i   \u2212 2 ( 2   \u03bb   i     M   , p   +   C   , p   )   \u03c6   i     \u03bb   i , p   \u2212 ( 2   \u03bb   i   M + C )   \u03c6   i , p     \u03bb   i , p   \u2212 2 M   \u03c6   i     \u03bb   i , p   2        Eq. (11) can be reformed as  (12)    F   i     \u03c6   i , p p   = \u2212 ( 2   \u03bb   i   M + C )   \u03c6   i     \u03bb   i , p p   \u2212   b   i     where  (13)    b   i   = (   \u03bb   i   2     M   , p p   +   \u03bb   i     C   , p p   +   K   , p p   )   \u03c6   i   + 2 (   \u03bb   i   2     M   , p   +   \u03bb   i     C   , p   +   K   , p   )   \u03c6   i , p   + 2 ( 2   \u03bb   i     M   , p   +   C   , p   )   \u03c6   i     \u03bb   i , p   + 2 ( 2   \u03bb   i   M + C )   \u03c6   i , p     \u03bb   i , p   \u2212 2 M   \u03c6   i     \u03bb   i , p   2        Premultiplying Eq. (11) by   \u03c6   i   T   , in a similar way, the second-order derivatives of eigenvalues can be obtained  (14)    \u03bb   i , p p   = \u2212   \u03c6   i   T   (   \u03bb   i   2     M   , p p   +   \u03bb   i     C   , p p   +   K   , p p   )   \u03c6   i   \u2212 2   \u03c6   i   T   (   \u03bb   i   2     M   , p   +   \u03bb   i     C   , p   +   K   , p   )   \u03c6   i , p   \u2212 2   \u03c6   i   T   ( 2   \u03bb   i     M   , p   +   C   , p   )   \u03c6   i     \u03bb   i , p   \u2212 2   \u03c6   i   T   ( 2   \u03bb   i   M + C )   \u03c6   i , p     \u03bb   i , p   \u2212 2   \u03c6   i   T   M   \u03c6   i     \u03bb   i , p   2     or  (15)    \u03bb   i , p p   = \u2212   \u03c6   i   T     b   i     The right-hand side of Eqs. (14) or (15) is a function of the ith eigenvalue and corresponding eigenvector and their first-order sensitivity as well as the first and second sensitivities of system matrices.  Again, the constraint of the second-order derivatives of eigenvectors can be derived by differentiating normalization Eq. (8) twice  (16)    {    \u03c6   i , p p    }     n   i     = 0      To apply the above constraint, in a similar way, the second-order derivatives of eigenvectors can be obtained using Eq. (12) and take the form  (17)     F \u02dc    i     \u03c6   i , p p   =    h \u02dc    i   ( 2 )     where     h   i   ( 2 )   = \u2212 ( 2   \u03bb   i   M + C )   \u03c6   i     \u03bb   i , p p   \u2212   b   i     Here vector    h \u02dc    i   ( 2 )   can be constructed by zeroing out the   n   i   th elements of   h   i   ( 2 )   .  As can be seen, the second-order sensitivities of eigenvectors only require the calculation of vector    h \u02dc    i   ( 2 )   because the symmetric coefficient matrix    F \u02dc    i   and its matrix decomposition (e.g. LDL T or LU decomposition) are available from the computation process of the first-order sensitivities of eigenvectors. For this reason, the computational cost of the second-order sensitivities of eigenvectors will be reduced remarkably once the first-order sensitivities are obtained (the first-order derivatives obtained by solving Eq. (10) need the number of operations O(N 3), however, only O(N 2) is required for finding the second-order derivatives). It is worth mentioning that b  i has been calculated in the computation process of the second-order sensitivities of eigenvalues. Therefore, the second sensitivities of eigenvectors are also compact, simple and easy to be implemented.  The complete algorithm for computing the first- and second-order sensitivities of eigensolutions with distinct eigenvalues is summarized below:  (1) Solve eigenproblem (   \u03bb   i   2   M +   \u03bb   i   C + K )   \u03c6   i   = 0 and obtain the eigenvalues and eigenvectors.  Normalize the eigenvectors   \u03c6   i   by the normalization   \u03c6   i   T   ( 2   \u03bb   i   M + C )   \u03c6   i   = 1 . Note this normalization is only for the design point at p=p 0. In the neighborhood of the current design point, i.e., the point at p near p 0, the eigenvectors should be normalized by   {    \u03c6   i   ( p )  }     n   i     =   {    \u03c6   i    }     n   i     .  Calculate the eigenvalue sensitivities by the compact formula,   \u03bb   i , p   = \u2212   \u03c6   i   T   (   \u03bb   i   2     M   , p   +   \u03bb   i     C   , p   +   K   , p   )   \u03c6   i   .  Compute   F   i   \u2250   \u03bb   i   2   M +   \u03bb   i   C + K .  Compute   h   i   \u2250 \u2212 ( 2   \u03bb   i   M + C )   \u03c6   i     \u03bb   i , p   \u2212 (   \u03bb   i   2     M   , p   +   \u03bb   i     C   , p   +   K   , p   )   \u03c6   i   .  Find n  i by selecting   {    \u03c6   i    }     n   i     which is the largest element in absolute value on the column of   \u03c6   i   .  Construct    F \u02dc    i   by zeroing out row n  i and column n  i of F  i , but keep the n  i th diagonal element of F  i .  Construct    h \u02dc    i   by zeroing out the n  i th elements of h  i .  Solve    F \u02dc    i     \u03c6   i , p   =    h \u02dc    i   and obtain the first-order sensitivities of eigenvectors,   \u03c6   i , p   .  Compute     b   i   = (   \u03bb   i   2     M   , p p   +   \u03bb   i     C   , p p   +   K   , p p   )   \u03c6   i   + 2 (   \u03bb   i   2     M   , p   +   \u03bb   i     C   , p   +   K   , p   )   \u03c6   i , p   + 2 ( 2   \u03bb   i     M   , p   +   C   , p   )   \u03c6   i     \u03bb   i , p   + 2 ( 2   \u03bb   i   M + C )   \u03c6   i , p     \u03bb   i , p   \u2212 2 M   \u03c6   i     \u03bb   i , p   2        Calculate the second-order sensitivities of eigenvalues using the formula   \u03bb   i , p p   = \u2212   \u03c6   i   T     b   i   .  Compute   h   i   ( 2 )   = \u2212 ( 2   \u03bb   i   M + C )   \u03c6   i     \u03bb   i , p p   \u2212   b   i   .  Construct    h \u02dc    i   ( 2 )   by zeroing out the n  i th elements of   h   i   ( 2 )   .  Solve    F \u02dc    i     \u03c6   i , p p   =    h \u02dc    i   ( 2 )   and obtain the second-order sensitivities of eigenvectors,   \u03c6   i , p p   .  Note that the algorithm of this method contains only a few steps, so it is very compact, simple and easy to be implemented.  Assume eigensystem Eq. (1) has ( 1 < m \u2264 N )  m repeated eigenvalues. Without loss of generality, we denote  (18)    \u039b   m   = d i a g (    \u03bb \u02dc    1   ( p ) ,    \u03bb \u02dc    2   ( p ) , \u22ef ,    \u03bb \u02dc    m   ( p ) )      (19)  X ( p ) = [       \u03c6   m   1   ( p )       \u03c6   m   2   ( p )    \u22ef      \u03c6   m   m   ( p )     ]   where    \u03bb \u02dc    k   ( p ) for k=1,2,\u2026,m are equal.  \u03bb \u02dc  ( p ) =    \u03bb \u02dc    k   ( p ) is the eigenvalue of multiplicity m  ( 1 < m \u2264 N ) for the eigenspace spanned by the columns of X ( p ) . The associating eigenproblem can be given by  (20)  M ( p ) X ( p )   \u039b   m   2   ( p ) + C ( p ) X ( p )   \u039b   m   ( p ) + K ( p ) X ( p ) = 0      (21)    X  T  ( 2  \u03bb \u02dc  M + C ) X =   I   m     Similarly, normalization Eq. (21) is only for p=p 0. Here   I   m   is the identity matrix of order m.  Actually, an infinite number of linear combinations of the eigenvectors   \u03c6   m   k   ( p ) will also satisfy Eqs. (20) and (21). A unique eigenvectors  X \u02dc  ( p ) called adjacent eigenvectors for which derivatives can be found need to be determined from the given eigenvectors X ( p ) . The adjacent eigenvectors  X \u02dc  ( p ) can be expressed in terms of X ( p ) as  (22)   X \u02dc  ( p ) = X ( p ) \u03b1   where \u03b1 is an orthogonal transformation matrix of dimension (m\u00d7m). Thus, the adjacent eigenvectors also satisfy the orthogonal normalization for p=p 0:  (23)     X \u02dc   T  ( 2  \u03bb \u02dc  M + C )  X \u02dc  =   \u03b1  T    X  T  ( 2  \u03bb \u02dc  M + C ) X \u03b1 =   I   m        Combining M ( p ) X ( p )   \u039b   m   ( p ) \u2212 M ( p ) X ( p )   \u039b   m   ( p ) = 0 and Eq. (20) can easily obtain the state-space eigenproblem:  (24)  A ( p ) Z ( p ) = B ( p ) Z ( p )   \u039b   m   ( p )   where  (25)  A =  [      \u2212 K    0     0   M     ]  ,  B =  [     C   M     M   0     ]   and  Z =  {     X      X   \u039b   m        }    And eigenvectors  Z \u02dc  of the 2N eigensystem can be given by  (26)   Z \u02dc  = Z \u03b1 =  {      X \u03b1       X \u03b1   \u039b   m        }  =  {      X \u02dc        X \u02dc    \u039b   m        }       Post-multiplying each side of Eq. (24) by \u03b1, and using the above equation yields another equivalent eigenproblem:  (27)  A ( p )  Z \u02dc  ( p ) = B ( p )  Z \u02dc  ( p )   \u039b   m   ( p )      The differentiation of the previous eigenproblem with respect to the design parameter p for p=p 0 yields  (28)  ( A \u2212  \u03bb \u02dc  B )    Z \u02dc    , p   \u2212 B  Z \u02dc    \u039b   m , p   = \u2212 (   A   , p   \u2212  \u03bb \u02dc    B   , p   )  Z \u02dc       The eigenvector sensitivities    Z \u02dc    , p   can be obtained in a Nelson form:  (29)     Z \u02dc    , p   = V +  Z \u02dc  c   where V is the particular solution and c is a coefficient matrix. Based on the idea of the modal method, the eigenvector sensitivity can be expressed as a linear combination of all the eigenvectors. The particular solution V may be expressed by a linear combination of the eigenvectors of the 2N damped symmetric system except those in eigenvectors  Z \u02dc  . The eigenvector sensitivity    X \u02dc    , p   can also be expressed by  (30)     X \u02dc    , p   = P +  X \u02dc  c   where P is the particular solution of the N-space damped system. One has   V =  [     P     Q     ]    where Q \u2208   \u2102   N \u00d7 m   is an undetermined matrix. Note the particular solution P is a linear combination of the eigenvectors of the N-space system except those in eigenvectors  X \u02dc  .  Based on the orthogonality condition, one can obtain  (31)     Z \u02dc   T  B V = \u03b1   Z  T  B  [     P     Q     ]  = 0   which can be rewritten in the form of the original space  (32)    X  T  ( C +  \u03bb \u02dc  M ) P +   X  T  M Q = 0   Substitute    Z \u02dc    , p   from Eq. (29) into Eq. (28) yields  (33)   [      \u2212 K \u2212  \u03bb \u02dc  C     \u2212  \u03bb \u02dc  M       \u2212  \u03bb \u02dc  M    M     ]   [     P     Q     ]  \u2212  [     C   M     M   0     ]   [     X       \u03bb \u02dc  X      ]  \u03b1   \u039b   m , p   =  [        K   , p   +  \u03bb \u02dc    C   , p        \u03bb \u02dc    M   , p          \u03bb \u02dc    M   , p       \u2212   M   , p        ]   [     X       \u03bb \u02dc  X      ]  \u03b1      The lower equation from Eq. (33) can be given by  (34)  M Q =  \u03bb \u02dc  M P + M X \u03b1   \u039b   m , p        Substitute this equation into Eq. (32), one can then obtain  (35)    X  T  ( C + 2  \u03bb \u02dc  M ) P +   X  T  M X \u03b1   \u039b   m , p   = 0   This expression gives the relation between the particular solution P and the eigenvalue sensitivity and can be considered as a constraint condition. An important property of Eq. (35) is that the relation does not depend on the undetermined matrix Q. One idea frequently come up is whether or not to have a method can deal with the derivatives of eigensolutions of damped systems in the form of the original space.  Let us consider the eigenproblem of the original space to find the eigenvector sensitivities    X \u02dc    , p      (36)  M ( p )  X \u02dc  ( p )   \u039b   m   2   ( p ) + C ( p )  X \u02dc  ( p )   \u039b   m   ( p ) + K ( p )  X \u02dc  ( p ) = 0      The differentiation of this eigenproblem with respect to p for p=p 0 yields  (37)  (    \u03bb \u02dc   2  M +  \u03bb \u02dc  C + K )    X \u02dc    , p   + ( 2  \u03bb \u02dc  M + C )  X \u02dc    \u039b   m , p   = \u2212 (    \u03bb \u02dc   2    M   , p   +  \u03bb \u02dc    C   , p   +   K   , p   )  X \u02dc       Substituting    X \u02dc    , p   from Eq. (30) into the previous equation and post-multiplying each side by   \u03b1  T  yields  (38)  (    \u03bb \u02dc   2  M +  \u03bb \u02dc  C + K ) P   \u03b1  T  + ( 2  \u03bb \u02dc  M + C ) X \u03b1   \u039b   m , p     \u03b1  T  = \u2212 (    \u03bb \u02dc   2    M   , p   +  \u03bb \u02dc    C   , p   +   K   , p   ) X      The above expression also gives the relation between the particular solution P and the eigenvalue sensitivity.  Combine Eqs. (35) and (38), one can obtain a linear system of algebraic equations  (39)   [         \u03bb \u02dc   2  M +  \u03bb \u02dc  C + K     \u03be ( 2  \u03bb \u02dc  M + C ) X         X  T  ( C + 2  \u03bb \u02dc  M ) \u03be       \u03be  2    X  T  M X      ]   [      P \u02dc         \u03be   \u2212 1   G      ]  = \u2212  [      (    \u03bb \u02dc   2    M   , p   +  \u03bb \u02dc    C   , p   +   K   , p   ) X      0     ]    where \u03be is a non-zero constant,  P \u02dc  \u2208   \u2102   N \u00d7 m   , G \u2208   \u2102   m \u00d7 m   are unknown matrices which can be determined by solving the above system. Obviously, P =  P \u02dc  \u03b1 and G = \u03b1   \u039b   m , p     \u03b1  T  . One can obtain  (40)  G \u03b1 = \u03b1   \u039b   m , p        The derivatives of repeated eigenvalues can be determined by solving this subeigenproblem. The transform matrix \u03b1 is normalized as   \u03b1  T  \u03b1 =   I   m   such that it satisfies the assumption that \u03b1 is an orthogonal matrix. Once matrix  P \u02dc  and the transform matrix \u03b1 are determined, the particular solution P and adjacent eigenvectors  X \u02dc  can be obtained. For simplification, the linear system (39) can be written in the following form  (41)    A   m     \u03b7   m   =   b   m     with   A   m   a (N+m)\u00d7(N+m) coefficient matrix,   \u03b7   m   a (N+m)\u00d7m matrix and   b   m   a (N+m)\u00d7m matrix. It should be pointed out that   A   m   is a symmetric matrix and the linear system can be solved by the LDLT decomposition method [67]. Sometimes the elements in the coefficient matrix are not all of the same order of magnitude which may cause a big condition number (e.g., see Li et al. [66] for similar study). To reduce the condition number, we determine the non-zero constant \u03be by finding the largest element in absolute value of matrix    \u03bb \u02dc   2  M +  \u03bb \u02dc  C + K and dividing the largest element in absolute value of matrix   X  T  ( C + 2  \u03bb \u02dc  M ) . Note the coefficient matrix   A   m   is a full rank matrix, as demonstrated in Section 4. Substituting the non-zero constant \u03be into system (39) or (41), leading to a nonsingular, numerically well-conditioned linear system of algebraic equations.  This section will show that the homogeneous solution can also be carried out in the form of the original space. Due to the fact that the particular solution has been found, it only remains to compute the coefficient matrix, c. Following the operations in these methods [58,63,64], differentiating Eq. (27) twice yields  (42)  c   \u039b   m , p   \u2212   \u039b   m , p   c + 0.5   \u039b   m , p p   = R   where   R =    Z \u02dc   T  (   A   , p   \u2212  \u03bb \u02dc    B   , p   ) V \u2212    Z \u02dc   T  (   B   , p    Z \u02dc  + B V )   \u039b   m , p   + 0.5    Z \u02dc   T  (   A   , p p   \u2212  \u03bb \u02dc    B   , p p   )  Z \u02dc       Because the particular solution V satisfies the orthogonality condition (31), the right hand of Eq. (42), namely, R, can be simplified as   R =    Z \u02dc   T  (   A   , p   \u2212  \u03bb \u02dc    B   , p   ) V \u2212    Z \u02dc   T    B   , p    Z \u02dc    \u039b   m , p   + 0.5    Z \u02dc   T  (   A   , p p   \u2212  \u03bb \u02dc    B   , p p   )  Z \u02dc       By using Eq. (25) , R can be expressed in the form of the original space   R = \u2212    X \u02dc   T  [ (    \u03bb \u02dc   2    M   , p   +  \u03bb \u02dc    C   , p   +   K   , p   ) P + ( 2  \u03bb \u02dc    M   , p   +   C   , p   )  X \u02dc    \u039b   m , p   + 0.5 (    \u03bb \u02dc   2    M   , p p   +  \u03bb \u02dc    C   , p p   +   K   , p p   )  X \u02dc  ]      Let R = [   r   j k   ] , c = [   c   j k   ] . If we assume that the derivatives of repeated eigenvalues are distinct, the off-diagonal elements of matrix c can be determined without requiring the second-order eigenvalue derivatives,   \u039b   m , p p   since   \u039b   m , p p   is a diagonal matrix. Thus, the off-diagonal elements of c can be obtained:  (43)    c   j k   =     r   j k        \u03bb \u02dc    k , p   \u2212    \u03bb \u02dc    j , p      if  j \u2260 k      Note the diagonal elements of matrix c   \u039b   m , p   \u2212   \u039b   m , p   c are equal to zeros, the second-order eigenvalue derivatives   \u039b   m , p p   can also be found  (44)    \u039b   m , p p   = 2   r   j k    if  j = k   It is interested to note that the second-order eigenvalue sensitivities of damped systems can also maintain N-space formula.  To determine the diagonal elements of matrix c, just like the case of distinct eigenvalues, an additional constraint should be imposed. Often the additional constraint can be derived by differentiating the normalization. In this study, a new normalization is proposed. For p=p 0, the adjacent eigenvectors satisfy the orthogonal normalization (23). Let  X \u02dc  = [    x \u02dc    j k   ] for j,k=1,2,\u2026,m. In the neighborhood of design point p 0, namely, p near p 0, we assume that the new normalization satisfies  (45)     x \u02dc      n   k   k   ( p ) =    x \u02dc      n   k   k        Find   n   k   such that    x \u02dc      n   k   k   is the largest element in absolute value on the kth column of  X \u02dc  . It means that the largest element in absolute value on each eigenvector is a constant. The additional constraint can be obtained by differentiating the above normalization with respect to parameter p that yields  (46)     x \u02dc      n   k   k , p   = 0   which means the element of row   n   k   and column k of    X \u02dc    , p   is set to zero. Let P = [   p   j k   ] . The diagonal elements of matrix c can be calculated by  (47)    c   j j   = \u2212   (   \u2211   k = 1 , k \u2260 j   m      x \u02dc      n   j   k     c   k j   +   p     n   j   j   )      x \u02dc      n   j   j      for  j = 1 , 2 , \u22ef , m      So far the elements of the coefficient matrix c are obtained. The eigenvector derivatives    X \u02dc    , p   can be then determined by substituting the particular solution P and the coefficient matrix c of the homogeneous solution into Eq. (30).  The complete algorithm for computing the eigenvalue and eigenvector sensitivities with repeated eigenvalues is summarized below:  (1) Solve eigenproblem M X   \u039b   m   2   + C X   \u039b   m   + K X = 0 and obtain the eigenvalues and eigenvectors.  Normalize the eigenvectors X using   X  T  ( 2  \u03bb \u02dc  M + C ) X =   I   m   .  Determine the non-zero constant \u03be by finding the largest element in absolute value of matrix    \u03bb \u02dc   2  M +  \u03bb \u02dc  C + K and dividing the largest element in absolute value of matrix   X  T  ( C + 2  \u03bb \u02dc  M ) .  Compute   \u0391   m   =  [         \u03bb \u02dc   2  M +  \u03bb \u02dc  C + K     \u03be ( 2  \u03bb \u02dc  M + C ) X         X  T  ( C + 2  \u03bb \u02dc  M ) \u03be       \u03be  2    X  T  M X      ]  .  Compute   b   m   = \u2212  [      (    \u03bb \u02dc   2    M   , p   +  \u03bb \u02dc    C   , p   +   K   , p   ) X      0     ]  .  Solve   A   m     \u03b7   m   =   b   m   given by Eqs. (39) and (41) and obtain  P \u02dc  and G.  Solve the subeigenproblem G \u03b1 = \u03b1   \u039b   m , p   where   \u039b   m , p   is the diagonal eigenvalue matrix and \u03b1 is normalized using   \u03b1  T  \u03b1 =   I   m   . Note the diagonal element of   \u039b   m , p   is eigenvalue sensitivities.  The adjacent eigenvectors  X \u02dc  can be obtained by  X \u02dc  = X \u03b1 . Note  X \u02dc  = X \u03b1 is only for p=p 0. For p near p 0, the adjacent eigenvectors should be normalized by    x \u02dc      n   k   k   ( p ) =    x \u02dc      n   k   k   .  Determine the particular solution P =  P \u02dc  \u03b1 .  Compute R = \u2212    X \u02dc   T  [ (    \u03bb \u02dc   2    M   , p   +  \u03bb \u02dc    C   , p   +   K   , p   ) P + ( 2  \u03bb \u02dc    M   , p   +   C   , p   )  X \u02dc    \u039b   m , p   + 0.5 (    \u03bb \u02dc   2    M   , p p   +  \u03bb \u02dc    C   , p p   +   K   , p p   )  X \u02dc  ]    Construct the ( m \u00d7 m ) matrix c by the rule     c   j k   =  {         r   j k    /  (    \u03bb \u02dc    k , p   \u2212    \u03bb \u02dc    j , p   )      if  j \u2260 k       \u2212   (    \u2211   k = 1 , k \u2260 j   m      x \u02dc      n   j   k     c   k j   +   p     n   j   j    )      x \u02dc      n   j   j         if  j = k            Let    X \u02dc    , p   = P +  X \u02dc  c . The columns of    X \u02dc    , p   are the eigenvector derivatives.  Note that the algorithm of the proposed method maintains N-space formulation without using the state-space approach. For this reason, the computational cost of the proposed method can be remarkably reduced in comparison with those of 2N-space approaches.  Consider the formulation for structural modal analysis described by undamped eigenvalue problems  (48)  (   \u03bb   i   ( p ) M ( p ) + K ( p ) )   \u03c6   i   ( p ) = 0      The ith eigenvalue is expressed as   \u03bb   i   =   ( j   \u03c9   i   )  2  where j =   \u2212 1   ,   \u03c9   i   \u2208 \u211d is the ith undamped natural frequency.  The combined normalization of undamped eigenproblems can be obtained in a Similar way, i.e., for p=p 0, we assume the traditional orthogonal normalization is imposed and has the following form:  (49)    \u03c6   i   T   M   \u03c6   i   = 1   and for p near p 0, the normalization is the same as it of the symmetric damped system.  The eigenvalue sensitivities can be obtained [7]:  (50)    \u03bb   i , p   = \u2212   \u03c6   i   T   (   \u03bb   i     M   , p   +   K   , p   )   \u03c6   i        As can be seen, the derivatives of eigenvalues of undamped systems can be considered as a degenerated case of those of damped systems. By following the same procedure as in Eq. (10), the linear system of undamped eigenproblems to determine the eigenvector derivative can be given by  (51)     F \u02dc    i     \u03c6   i , p   =    h \u02dc    i     Here   F   i   ,   h   i   should be modified into   \u03bb   i   M + K and \u2212   \u03bb   i , p   M   \u03c6   i   \u2212 (   \u03bb   i     M   , p   +   K   , p   )   \u03c6   i   . This formula is similar to the computation of the particular solution of Nelson's method [33], but the presented method can find the eigenvector sensitivities directly.  In the case of repeated eigenvalues, the undamped eigenproblem with repeated eigenvalues can be given by  (52)  M ( p ) X ( p )   \u039b   m   ( p ) + K ( p ) X ( p ) = 0      (53)    X  T  M X =   I   m        Note the orthogonal normalization (53) is only for p=p 0 and the normalization is the same as it of the symmetric damped system for p near p 0. Similarly, the adjacent eigenvectors  X \u02dc  ( p ) can be expressed by  X \u02dc  ( p ) = X ( p ) \u03b1 and there is  (54)  M ( p )  X \u02dc  ( p )   \u039b   m   ( p ) + K ( p )  X \u02dc  ( p ) = 0      The differentiation of this eigenproblem with respect to the design parameter p for p=p 0 yields  (55)  (  \u03bb \u02dc  M + K )    X \u02dc    , p   + M  X \u02dc    \u039b   m , p   = \u2212 (  \u03bb \u02dc    M   , p   +   K   , p   )  X \u02dc       The eigenvector sensitivity    X \u02dc    , p   also can be expressed as a particular solution P and a homogeneous solution  (56)     X \u02dc    , p   = P +  X \u02dc  c      The particular solution P can be expressed by a linear combination of the eigenvectors of the N undamped symmetric system except those in eigenvectors  X \u02dc  . Thus, the orthogonal constraint condition of the particular solution can be obtained as  (57)    X  T  M P = 0      The particular solution can be determined by using equation, P =  P \u02dc  \u03b1 where  P \u02dc  can be obtained by solving the following linear system constructed by combining Eqs. (55) and (57).  (58)   [       \u03bb \u02dc  M + K     \u03be M X         X  T  M \u03be    0     ]   [      P \u02dc         \u03be   \u2212 1   G      ]  = \u2212  [      (  \u03bb \u02dc    M   , p   +   K   , p   ) X      0     ]       It is well known that in the context of viscously damped systems, their normalization is inconsistent with undamped or classically damped modal theories. For that reason, Eq. (41) cannot be degenerated to Eq. (58). Alternatively, one can choose the undamped eigenvectors normalized by 2  \u03bb \u02dc    X  T  M X =   I   m   , under such circumstances, it is easy to verify that Eq. (41) can be degenerated to Eq. (58) by following the same procedures. Here, the non-zero constant \u03be can be determined by finding the largest element in absolute value of matrix  \u03bb \u02dc  M + K and dividing the largest element in absolute value of matrix MX.  The derivatives of repeated eigenvalues can be then obtained:  (59)  G \u03b1 = \u03b1   \u039b   m , p        Matrix R can be modified into  (60)  R =    X \u02dc   T  (   K   , p   +  \u03bb \u02dc    M   , p   ) P +    X \u02dc   T    M   , p    X \u02dc    \u039b   m , p   + 0.5    X \u02dc   T  (   K   , p p   +  \u03bb \u02dc    M   , p p   )  X \u02dc       Construct the (m\u00d7m) matrix c by the rule     c   j k   =  {        r   j k   / (    \u03bb \u02dc    k , p   \u2212    \u03bb \u02dc    j , p   )     if  j \u2260 k       \u2212   (   \u03a3   k = 1 , k \u2260 j   m      x \u02dc      n   j   k     c   k j   +   p     n   j   j   )      x \u02dc      n   j   j         if  j = k       .      Finally, the eigenvector derivatives    X \u02dc    , p   can be obtained by the expression,    X \u02dc    , p   = P +  X \u02dc  c . The columns of    X \u02dc    , p   are the eigenvector derivatives.  As can be seen, the derivatives of eigensolutions for symmetric undamped systems with distinct and repeated eigenvalues can be established by utilizing the similar procedures as in the previous subsections (i.e., Subsections 3.2 and 3.3).  The objective in this section is to show that the two systems of algebraic equations expressed in Eqs. (10) and (40) are always solvable, i.e., how to prove that these coefficient matrices will not be singular or badly scaled.  This section will show that the coefficient matrix    F \u02dc    i   of Eq. (10) has a rank N.  The constraint Eq. (9) can be rewritten as an equivalent form  (61)    W   i     \u03c6   i , p   = 0   where     W   i   =  {               n   i   th  column         0   \u22ef   0   1   0   \u22ef   0      \ufe38     N    }    Here   W   i   is a (1\u00d7N) vector and the n  i th component of it associated with the ith eigenvector is set to unity. Let  (62)    G   m   =  [        F   i         W   i   T           W   i      0     ]       By subtracting the last column of G  m from every other column and subtracting the last row of G  m from every other row, one has  (63)    G   m   ( 1 )   =  [              0            0           F   i 11         \u22ee         F   i 13         \u22ee              0            0     0   \u22ef   0   0   0   \u22ef   0   1              0            0           F   i 31         \u22ee         F   i 33         \u22ee              0            0     0   \u22ef   0   1   0   \u22ef   0   0     ]       Exchange the n  i row with the last row, one can obtain  (64)    G   m   ( 2 )   =  [         F ^    i      0     0   1     ]    where      F ^    i   =  [              0                    F   i 11         \u22ee         F   i 13                    0              0   \u22ef   0   1   0   \u22ef   0              0                    F   i 31         \u22ee         F   i 33                    0              ]       Multiply the n  i row of Eq. (64) by the n  i element of F  i yields  (65)    G   m   ( 3 )   =  [         F \u02dc    i      0     0   1     ]       In order to show that the coefficient matrix    F \u02dc    i   of Eq. (10) is non-singular, only need to prove that G  m is a full rank matrix. To prove the coefficient matrix G  m is nonsingular, consider a (N+1)\u00d71 vector d and use the fact that the equation G  m  d=0 has the unique solution d=0.  Assume G  m  d=0 for d =   [    \u03d2   \u03c4    ]  T  \u2208   C   N + 1   . Substitute G  m from Eq. (62) into G  m  d=0. One can obtain:  (66)    F   i   \u03d2 \u2212   W   i   T   \u03c4 = 0      (67)    W   i   \u03d2 = 0      Pre-multiplying Eq. (66) by   \u03c6   i   T   and utilizing Eq. (1) and   W   i     \u03c6   i   =   {   \u03c6   i   }     n   i     , one can show \u03c4 is equal to zero. Substituting \u03c4=0 into Eq. (66) yields  (68)    F   i   \u03d2 = 0      It is clear that   \u03c6   i   is a particular solution of the above equation. Hence \u03d2 can be expressed by  (69)  \u03d2 =   a   i     \u03c6   i     where   a   i   are constant coefficients. Substituting this expression into Eq. (67), and utilizing   W   i     \u03c6   i   =   {   \u03c6   i   }     n   i     , one can obtain   a   i   = 0 , i.e., d=0. Therefore, it can be concluded that G  m is always a full rank matrix and the coefficient matrix    F \u02dc    i   of Eq. (10) is non-singular.  In order to prove the coefficient matrix   A   m   of Eq. (41) is nonsingular, consider a (N+m)\u00d7m matrix s and use the fact that equation   A   m   s = 0 has the unique solution s = 0 . The coefficient matrix   A   m   takes the form  (70)    A   m   =  [         \u03bb \u02dc   2  M +  \u03bb \u02dc  C + K     \u03be ( 2  \u03bb \u02dc  M + C ) X         X  T  ( C + 2  \u03bb \u02dc  M ) \u03be       \u03be  2    X  T  M X      ]       Assume   A   m   s = 0  f o r  s =   [ \u0398  \u039e ]  T  \u2208   C   ( N + m ) \u00d7 m   . That is  (71)  (    \u03bb \u02dc   2  M +  \u03bb \u02dc  C + K ) \u0398 + \u03be ( 2  \u03bb \u02dc  M + C ) X \u039e = 0      (72)  \u03be   X  T  ( C + 2  \u03bb \u02dc  M ) \u0398 +   \u03be  2    X  T  M X \u039e = 0      Pre-multiplying Eq. (71) by   X  T  and utilizing Eq. (20) and normalization (21), one can obtain \u039e is equal to a zero matrix. Substituting \u039e = 0 into Eq. (71) yields  (73)  (    \u03bb \u02dc   2  M +  \u03bb \u02dc  C + K ) \u0398 = 0      Obviously, X is a particular solution of Eq. (73). Thus, \u0398 can be expressed by  (74)  \u0398 = X  {        c   1   1         c   1   2      \u22ef      c   1   m           c   2   1         c   2   2      \u22ef      c   2   m        \u22ee   \u22ee   \u22f1   \u22ee        c   m   1         c   m   2      \u22ef      c   m   m        }    where   c   i   k   for i, k=1,2,\u2026,m are constant coefficients. Substituting Eq. (74) and \u039e = 0 into Eq. (72), and utilizing normalization Eq. (21), one can obtain   c   i   k   = 0 . That is to say, s = 0 . Therefore, it can be concluded that   A   m   is always a full rank matrix.  Consider the following four DOF mass-spring system with viscous damping shown in Fig. 1. The system matrices M, K and C are, respectively,   M =  [     m   0   0   0     0   m   0   0     0   0   m   0     0   0   0   m     ]  ,  K =  [      4 k +   k   1       \u2212   k   1      0   0      \u2212   k   1       5   k   1      0   0     0   0    4 k    0     0   0   0    6 k      ]   and  C =  [      4 c    0   0   0     0    4 c    0   0     0   0    4 c    0     0   0   0    6 c      ]    where k=1000N/m, m=1kg, c=10Ns/m and k 1=1000N/m. The stiffness k was chosen as the design parameter p, and the eigensolution derivatives were considered at p 0=1000N/m. The derivatives of system matrices are     M   , p   = 0 ,    K   , p   =  [     4   0   0   0     0   0   0   0     0   0   4   0     0   0   0   6     ]  ,    C   , p   = 0      The derivatives of eigensolutions are uniquely determined by the proposed method and shown in Table 1.  To illustrate the accuracy of the proposed method and show how to use the new combined normalization, the finite difference method is considered. The finite difference method is the easiest method to implement for calculating the eigensolution derivatives [3,32]. The eigensolution derivatives approximated by the first-order forward differences can be given as     \u0394   \u03bb   i     \u0394 p   =     \u03bb   i   (   p   0   + \u0394 p ) \u2212   \u03bb   i   (   p   0   )   \u0394 p   ,    \u0394   \u03c6   i     \u0394 p   =     \u03c6   i   (   p   0   + \u0394 p ) \u2212   \u03c6   i   (   p   0   )   \u0394 p        As can be seen, for each design parameter p, the finite difference method must perform a complete reanalysis of eigensolutions. In addition, the finite difference method evaluates the true derivative with an error that is on the order of the step size \u0394p. Although the finite difference method is easy to program, it suffers from computational inefficiency and possible errors [32,65,66].    Table 2 lists the approximated eigensolution derivatives computed by the finite difference method with respect to \u0394p=1N/m. It should be noted that normalization Eq. (7) is used to normalize the eigenvectors at design point p 0 and the normalization (8) is used to normalize the eigenvectors at p 0+\u0394p in the case of distinct eigenvalues, and similar procedures for eigenvectors associating to repeated eigenvalues.  As it can be seen from Tables 1 and 2, the eigensolution derivatives computed by the proposed method show a good agreement with the results approximated by the finite difference method.  To illustrate the robustness of the proposed method, a simple but representative three bar truss structure, shown in Fig. 2, is considered. The truss structure has Young's modulus E=2.1\u00d71011 N/m2, material density \u03c1=7860kg/m3. The structure is modeled into three truss elements whose element stiffness and mass matrices are     K   e   =   A E     l   e      [     1    \u2212 1       \u2212 1    1     ]  ,    M   e   =   A \u03c1   l   e    6   [     2   1     1   2     ]    where A is the area of the cross-section of the truss element and l  e is the length of the truss element. After performing assembly and applying the boundary condition, the global system matrices can be given by   K =   A E     l   e      [     2    \u2212 1    0      \u2212 1    2    \u2212 1      0    \u2212 1    1     ]  ,  M =   A \u03c1   l   e    6   [     4   1   0     1   4   1     0   1   2     ]    where l  e is the length of the truss element. Assume the damping matrix is a linear combination of the stiffness and mass matrices, C = M \u03b8 + K \u03d1 where \u03b8 and \u03d1 are the Rayleigh coefficients and equal to 1.0\u00d710\u22126. For numerical study, we suppose A=0.0001m2 and l  e =0.01m. The length l  e was chosen as the design variable p and the eigensolution derivatives were considered at p 0=0.01m. The derivatives of system matrices are     K   , p   = \u2212   A E     l   e   2      [     2    \u2212 1    0      \u2212 1    2    \u2212 1      0    \u2212 1    1     ]  ,    M   , p   =   A \u03c1  6   [     4   1   0     1   4   1     0   1   2     ]   and    C   , p   = 0.0005 (   K   , p   +   M   , p   )      The system has three pairs of conjugate complex eigenvalues: \u22123.7468\u00d7104\u00b12.7117\u00d7105 i, \u22124.0076\u00d7105\u00b18.0057\u00d7105 i, \u22121.3190\u00d7106\u00b19.4777\u00d7105 i. For convenience, the mode shape associated to eigenvalue \u22121.3190\u00d7106+9.4777\u00d7105 i was only considered in this example. In this case, the condition number of coefficient matrix    F \u02dc    i   of Eq. (10) is 11.412 and the coefficient matrix is      F \u02dc    i   =  [      3.0693 \u00d7   10  9  \u2212 9.1201 \u00d7   10  9  i    0   0     0    3.0693 \u00d7   10  9  \u2212 9.1201 \u00d7   10  9  i     1.7720 \u00d7   10  9  \u2212 5.2655 \u00d7   10  9  i      0    1.7720 \u00d7   10  9  \u2212 5.2655 \u00d7   10  9  i     1.5346 \u00d7   10  9  \u2212 4.5601 \u00d7   10  9  i      ]       One would expect numerical stability of computational approach; however, we get quite a big condition number of the similar coefficient matrix    F ^    i   in Nelson's method [36] and the value is 1.32713\u00d71010. The coefficient matrix is given by      F ^    i   =  [     1   0   0     0    3.0693 \u00d7   10  9  \u2212 9.1201 \u00d7   10  9  i     1.7720 \u00d7   10  9  \u2212 5.2655 \u00d7   10  9  i      0    1.7720 \u00d7   10  9  \u2212 5.2655 \u00d7   10  9  i     1.5346 \u00d7   10  9  \u2212 4.5601 \u00d7   10  9  i      ]       As it can be noted, the condition number is remarkably reduced, hence the proposed method is numerically well-conditioned.  To illustrate the application and efficiency of the proposed method, a two-stage floating raft isolation system, shown in Fig. 3 is considered. It should be mentioned that an advanced isolation system used in engineering application is more complicated than the general two-stage isolation system and it can provide a much better vibration reduction than the latter.  As shown in Fig. 3, the machine vibration is attempted to be isolated by mounting two machines on a single intermediate raft structure. The two machines have the values: m 1=200kg, m 2=250kg. The length\u2013width\u2013thickness of raft plate and foundation plate are 1200\u2013800\u201320mm and 2000\u2013800\u201340mm, respectively. The four sides of raft plate are all free. The two short sides of foundation plate are clamped and two long sides of it are free. The raft and foundation plates have  Young's modulus: E=2.0\u00d71011 N/m2   Density: \u03c1=7800kg/m3   Poisson's ratio: \u03bc=0.3.  Two springs and dashpots are used to mount two machines on a single intermediate raft structure, and eight springs and dashpots are fixed between raft plate and foundation plate. The spring stiffness coefficients and the damping coefficients have the following values:     k   1   = 1.0 \u00d7 1   0  5   N / m ,    k   2   = 5.0 \u00d7 1   0  5   N / m ,    c   1   = 1.0 \u00d7 1   0  2   Ns / m ,    c   2   = 5.0 \u00d7 1   0  2   Ns / m .      As shown in Fig. 4, the finite element model, which is meshed using HyperMesh and modeled using the finite element software-NASTRAN [68], is discretized into 192 elements and has 1258 DOF. System matrices of the finite element model are assembled by NASTRAN and exported by means of DAMP language [69]. By using the system matrices, the proposed method is programmed by MATLAB. The distribution of the nonzero terms of system matrices is shown in Fig. 5 obtained by using the MATLAB function spy(\u00b7). This floating raft isolation system is a non-proportionally damped systems since it does not satisfy the mathematical condition presented by Caughey and O'Kelly [70] and Adhikari [71]. The lower 10 complex eigenvalues are listed in Table 3. It should be noted that the imaginary part of each complex eigenvalue is the natural frequency in rad/s. The real part of each complex eigenvalue is a measure of the decay rate of the raft system. The system is stable since all the real parts of eigenvalues or decay rate coefficients are negative.     Case 1: Material density is considered as the design parameter  To illustrate the computation of the first- and second-order derivatives of eigenvalues and eigenvectors, the material density is chosen as the design parameter p. Table 4 lists the first- and second-order derivatives of some eigenvalues with respect to material density at 7800kg/m3. It has been checked that the first- and second-order derivatives of eigenvalues calculated by the presented method have the same results with those calculated by Nelson's method [37]. As can be seen from Eq. (14), the second-order sensitivity of eigenvalue is a function of the first-order sensitivity of the corresponding eigenvector. It implies that the first-order sensitivities of eigenvectors obtained by using the proposed method can be also exactly obtained.  In general, the eigensolution sensitivity can be used to predict the changes of eigenvalues and eigenvectors with respect to the changes of design parameters, select a search direction and form an approximate model for optimization process, and assess the effects of uncertainties of structure or geometry property to system eigensolutions. In this example, the capacity of predicting the changes of eigensolutions with respect to the changes of design parameters is considered. As we know, to avoid performing a complete reanalysis of eigensolutions, Taylor expansion is usually used to approximate the change in eigensolution with respect to an arbitrary change in design parameters based on their first- and second-derivatives of eigensolutions. By using only the first term in Taylor expansion, the change in eigenvalue can be given by     \u03bb   i   c h a n g e d   =   \u03bb   i   i n i t i a l   +   \u2202   \u03bb   i   i n i t i a l     \u2202 p   \u0394 p   while considering the first two terms in Taylor expansion, the change in eigenvalue takes the form     \u03bb   i   c h a n g e d   =   \u03bb   i   i n i t i a l   +   \u2202   \u03bb   i   i n i t i a l     \u2202 p   \u0394 p +  1 2      \u2202  2    \u03bb   i   i n i t i a l     \u2202   p  2      ( \u0394 p )  2       In the same way, the changed eigenvectors   \u03c6   i   c h a n g e d   can be also approximated.  Next, the first- and second-order approximations are used to estimate the change in eigensolution from the change of material density.  Figs. 6 and 7 show the influence of changing the material density from 6500 to 9500kg/m3 on the imaginary and real parts of the third eigenvalue, respectively. The first- and second-order approximations of the third eigenvalue are compared with the exact eigenvalue. As can be seen from Fig. 6, the third natural frequency decrease to the variation of material density, the first-order approximation predicts the change in the frequency with good accuracy in the small perturbation region and the second-order approximation improves the approximate results and can predict the change in the frequency with excellent accuracy, even in the eigenvalues with a big change. As can be seen from Fig. 7, the third decay rate coefficient increase to the variation of material density. It is obvious that the first-order approximation predicts the change in the third decay rate coefficient with good accuracy in the small perturbation region and the second order approximation improves the approximate results. As a measure of the error of the first- and second-order approximations of eigenvectors, the relative error is defined as   E r r o r =     (   \u03c6   i   e x a c t   \u2212   \u03c6   i   a p p r o x   )  H  (   \u03c6   i   e x a c t   \u2212   \u03c6   i   a p p r o x   )     (   \u03c6   i   e x a c t   )  H    \u03c6   i   e x a c t     \u00d7 100 %   where   \u03c6   i   e x a c t   is the exact values,   \u03c6   i   a p p r o x   is the approximate values and the superscript H denote the conjugate transpose. Fig. 8 shows the error of changing the material density on the third eigenvector. As can be observed, the first-order approximation predicts the change in the third eigenvectors with good accuracy and the second-order approximation makes the error reduce.  Case 2: Young's modulus is considered as the design parameter  In this case, the Young's modulus is chosen as the design parameter p. Table 5 lists the first- and second-order derivatives of some eigenvalues with respect to Young's modulus at 2.0\u00d71011 N/m2. It is worth mentioning that the first- and second-order derivatives of eigenvalues calculated by the presented method are identical with those calculated by Nelson's method.  Next, the first- and second-order approximations are used to estimate the change in eigensolution from the change of Young's modulus.  Figs. 9 and 10 show the influence of changing the Young's modulus from 1.6\u00d71011 to 2.4\u00d71011 N/m2 on the imaginary and real parts of the third eigenvalue, respectively. As can be seen from Figs. 9 and 10, the third natural frequency increase to the variation of Young's modulus, the third decay rate coefficient decrease to the variation of Young's modulus. In addition, the first-order approximation predicts the change in the frequency and decay rate coefficient with good accuracy in the small perturbation region and the second-order approximation can predict the change in the frequency with excellent accuracy, even in the eigenvalues with a big change. Fig. 11 shows the error of changing the Young's modulus on the third eigenvector. As we can see, the first-order approximation predicts the change in the third eigenvectors with good accuracy and the second-order approximation makes the error reduce.  To illustrate the computational efficiency of the proposed method, the eigensolution sensitivities have been also computed by Nelson's method [37]. In this case, the Young's modulus is chosen as the design parameter p. The computational time of Nelson's method and the proposed method is considered with respect to the number of the computed eigensolution derivatives. The CPU time of the proposed method to obtain the first-order derivatives of the first 50 eigensolutions is 113.640s, which slightly reduced in comparison with those elapsed by Nelson's method (115.543s). The reason is that the first-order derivatives of eigenvectors need the matrix decomposition (e.g. LDL T or LU decomposition) of    F \u02dc    i   in Eq. (10), which need the number of operations O(N 3), however, only O(N 2) is required for finding the homogeneous solution of Nelson's method [37] that is not required for the proposed method (on the number of operations has been studied in Li et al. [66]). Fig. 12 shows the computational time of Nelson's method and the proposed method with respect to the number of the second-order derivatives of eigensolutions. Since matrix decomposition of    F \u02dc    i   and vector b  i of Eq. (17) are available from the computation process of the first-order sensitivities of eigenvectors and the second-order sensitivities of eigenvalues, the solution of Eq. (17) only takes O(N 2). In addition, the homogeneous solution of Nelson's method also requires O(N 2). For this reason, the computational cost of the second-order sensitivities of eigenvectors is reduced remarkably compared with Nelson's method.   CONCLUSIONS   Design sensitivity analysis of engineering structures deals with the computations of the rate of performance measure from changes in the design parameters describing the structure. The eigensolution sensitivity can be used to predict the changes of eigenvalues and eigenvectors with respect to the changes of design parameters, select a search direction and form an approximate model for optimization process, and assess the effects of uncertainties of structure or geometry property to system eigensolutions. This paper studied the eigensolution derivatives for symmetric viscously damped eigenproblems with distinct and repeated eigenvalues. To simplify the computation, a combined normalization, which combines two traditional normalizations, is presented. Based on the combined normalization, a method for sensitivity analysis of eigenvalues and eigenvectors is studied. In the case of distinct eigenvalues, the proposed method can determine the eigenvector derivatives directly and is robust since the components of coefficient matrices are all of the same order of magnitude. In the case of repeated eigenvalues, an algorithm is presented for computing the eigensolution sensitivities. The algorithm maintains N-space without using state-space equations such that the computational cost is reduced. The method is accurate, compact, numerically stable and easy to be implemented. In addition, the proposed method can be extended to compute the eigensolution derivatives of undamped systems with the similar produces. Finally, three numerical examples have demonstrated the validity of the proposed method. The capacity of predicting the changes of eigensolutions with respect to the changes of design parameters in terms of the first- and second-order eigensensitivities is studied with application to the analysis of a two-stage floating raft isolation system. The computational cost of the second-order sensitivities of eigenvectors can be reduced remarkably since the matrix decomposition of the coefficient matrix is available from the computation process of the first-order eigensensitivities and it has been verified by a two-stage floating raft isolation system with 1258 DOF.   ACKNOWLEDGMENTS   This Research was supported by the National Science and Technology Major Project of China (Grant no. 2012ZX04003-021), the National Basic Research (973) Program of China (Grant no. 2009CB724306), the National Natural Science Foundation of China (Grant nos. 30870605 and 50675077), and the Fundamental Research Funds for Universities supported by central authority (Grant no. 2011QN126).  Finally, the authors would like to thank the two anonymous reviewers for their many valuable comments and suggestions on improvement of this paper.   REFERENCES", "highlights": "This paper considers the computation of eigensolution sensitivity of viscously damped eigensystems with distinct and repeated eigenvalues. To simplify the computation, a combined normalization, which combines two traditional normalizations, is presented. Based on the combined normalization, a method for sensitivity analysis of eigenvalues and eigenvectors is studied. In the case of distinct eigenvalues, the proposed method can determine the eigenvector derivatives directly and is robust since the components of coefficient matrices are all of the same order of magnitude. The computational cost of the second-order sensitivities of eigenvectors can be reduced remarkably since the matrix decomposition of the coefficient matrix is available from the computation process of the first-order eigensensitivities. In the case of repeated eigenvalues, an algorithm is presented for computing the eigensolution sensitivities. The algorithm maintains N-space without using state-space equations such that the computational cost is reduced. The method is accurate, compact, numerically stable and easy to be implemented. Finally, three numerical examples have demonstrated the validity of the proposed method. The capacity of predicting the changes of eigensolutions with respect to the changes of design parameters in terms of the first- and second-order eigensensitivities is studied with application to the analysis of a two-stage floating raft isolation system."}