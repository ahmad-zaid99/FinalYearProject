{"id": "S001044851400181X", "article": "MAIN-TITLE Correct resolution rendering of trimmed spline surfaces   HIGHLIGHTS          Tight estimates relate domain resolution to screen resolution of trimmed surfaces.      Based on the estimates, sub-pixel accuracy of a display algorithm is proven.      The algorithm has been implemented within the standard graphics pipeline.      The implementation enables interactive editing of trimmed surfaces.      The implementation has a small memory footprint.          KEYPHRASES   Trimming  Spline  Accurate  Real-time  Scan density   A standard approach to designing geometry in computer aided design is to \u201coverfit\u201d, i.e. create spline surfaces that are larger than needed and subsequently trim the surfaces back to match functional constraints or join to other surfaces (see Fig.\u00a01 ). This approach persists both for historical reasons and for design simplicity: for historical reasons in that overfitting and trimming pre-dates alternative approaches such as subdivision surfaces\u00a0 [1,2] and finite geometrically-smooth patch complexes (see e.g.\u00a0 [3,4]); for practical reasons, in that it is often more convenient to control the shape of a signature piece in isolation than when constraints have to be taken into consideration. For example, a car\u2019s dashboard can be prepared in one piece without consideration of cut-outs for instrumentation and the steering column.  The prevailing practice in computer aided design environments is to generate and display a fixed-resolution triangulation on the CPU and transfer it to the GPU. This process interrupts the design process and can yield unsatisfactory results as closeups reveal a jagged or otherwise incorrect approximation. Conversely, an overly fine triangulation wastes resources: there is no need to highly resolve a complex trim curve when the corresponding surface measures only a few pixels on the screen.  The computer graphics community has developed a number of clever techniques, reviewed in Section\u00a0 2, to deliver real-time display of trimmed spline surfaces. The present paper advances the state-of-the-art by carefully predicting how fine an evaluation of the trim curves results in correct trim decisions at screen resolution. This tight prediction makes it possible to construct, as a prelude to each modified view or model rendering pass, a slim and adaptive trim-query acceleration table that supports a light-weight per-fragment trim test. This simple add-on to any rendering pass is efficient enough to allow interactive trim-curve editing.   Overview. Section\u00a0 2 reviews existing techniques for fast rendering of trimmed spline surfaces. Section\u00a0 3 reviews basic concepts and establishes notation. Section\u00a0 4 explains how correct resolution can be determined. Section\u00a0 5 explains how to build and use the trim-query acceleration table. Section\u00a0 6 measures the performance of a full implementation.  The trim decision is to determine to which side, of a set of trim curves, lies the u v -pre-image\u00a0of a pixel. The underlying challenge is the same as when determining the fill region of a planar decal\u00a0 [5]\u2014except that in planar filling, the accuracy of rendering is measured in the u v -plane, while for trimmed surfaces, the accuracy is measured in screen space, i.e. after applying the non-linear surface map followed by projection onto the screen.  A straightforward approach, used in 2D vector graphics\u00a0 [6,\u00a0Section 8.7], is to ray-test: each pixel\u2019s u v -pre-image\u00a0in the domain sends a ray to the domain boundary to determine the number of intersections (and possibly the intersection curve orientation). The intersections decide whether the fragment is to be discarded. For example, Pabst et\u00a0al.\u00a0 [7] test scan-line curve intersection directly in the Fragment Shader. Direct testing without an acceleration structure is impractical since the number and complexity of intersections can be unpredictably high so that robustness and accuracy are difficult to assure within a fixed time. It pays to pre-process the trim curves and map them into a hierarchical search structure in order to localize testing to a single trim curve segment. For example, Schollmeyer et\u00a0al.\u00a0 [8] break the segments into monotone pieces and test scan-line curve intersection in the Fragment Shader\u00a0by robust binary search. This pre-processing is view-point independent but becomes expensive for interactive trim-curve manipulation.  An alternative is to generate a trim texture: the u v -pre-image\u00a0of each fragment indexes into a texture that returns whether the point is to be trimmed or not. Such a trim texture can be generated in a separate rendering pass, using the stencil buffer\u00a0 [9]. The trim-test is highly efficient, requiring only a single texture look-up to classify a domain point. However trim textures need to be recomputed for every viewpoint change and the separate pass can noticeably lower overall performance as each segment of every trim curve generates and atomically inserts a triangle into the stencil buffer. Moreover, the trim-texture represents a uniform, limited resolution sampling of the u v -domain. Projective fore-shortening must be accounted for separately: when rendering a curved surface in 3-space, the non-uniform distortion of the domain caused by the non-linear map of the surface followed by perspective projection can result in low render quality even where the texture resolution is high.  Another pre-processing choice is to convert the piecewise rational trim curves into an implicit representation, via resultants (see e.g.\u00a0 [10\u201312]). Evaluating the resultant will generate a signed number and the sign can be used to determine whether a pixel is to be trimmed. In principle, this yields unlimited accuracy. However, there are several caveats to this approach. First, the use of resultants increases the degree and the number of variables. The coefficients of the implicit representation are typically complicated expressions in terms of the coefficients of the trim curve segments. Therefore the evaluation in the Fragment Shader\u00a0can be expensive even if the derivation of the implicit expression is done off-line prior to rendering. There are more efficient approaches than full implicitization, e.g.\u00a0 [13]. While useful for ray-tracing, these expressions do not presently yield a signed test as required for trimming. Second, implicitization converts the entire rational curve, not just the required rational piece. Use of resultants therefore requires a careful restriction of the test region, for example by isolating bounding triangles in the domain that contain a single indicator function whose zero level set represents the trim. Determining such restrictions is in general tricky since the implicit can have extraneous branches. For conics, the conversion expressions are sufficiently simple and for fixed shapes, such as fonts, determining bounding triangles can be done offline once and for all\u00a0 [14]. For less static scenarios, also pre-processing of conics and triangulation of the domain is not easily parallelized. Stencil buffers can avoid careful triangulation\u00a0 [5] but the fixed resolution inherits the challenges of texture-based trimming.  Computing the trim curves from CSG operations addresses a related but somewhat different problem than rendering. Here the trim curves (exact intersection pre-images) are not given. For simple CSG primitives such as quadrics the render decision can be based on an implicit in/out test. For more complex B-reps, faceted models are compared and proper resolution of the B-rep into facets remains a challenge. Practical implementations use stencil operations, depth and occlusion testing\u00a0 [15,16].   Coordinates and projection. In the OpenGL graphics pipeline [17,\u00a0Section 13.6], the non-orthogonal projection P      (    x     y     z     1    )  \u2192  (      x   c         y   c         z   c         w   c      )  \u2254  (      P   11     0   0   0     0     P   22     0   0     0   0     P   33       P   34       0   0   \u2212 1   0    )   (    x     y     z     1    )    maps camera coordinates    ( x , y , z , 1 )    T   with the camera is at the origin pointing in the negative z -direction, to clip coordinates    (   x   c   ,   y   c   ,   z   c   ,   w   c   )    T   . The entries   P   33   \u2254  (   z  \u00af  \u2212   z  \u00af  )  /  (   z  \u00af  +   z  \u00af  )  and   P   34   \u2254 2   z  \u00af    z  \u00af  /  (   z  \u00af  \u2212   z  \u00af  )  define two planes at depth   z  \u00af  (near) and   z  \u00af  (far) such that any geometry with depth outside the range  [   z  \u00af  ,   z  \u00af  ]  is clipped. Perspective division converts the clip coordinates into normalized device coordinates  (   x   d   ,   y   d   ,   z   d   )  \u2254  (   x   c   ,   y   c   ,   z   c   )  /   w   c   , and the viewport transformation converts normalized device coordinates to screen coordinates:  (    x    \u0303   ,    y    \u0303   )  \u2254  ( W   x   d   / 2 +   O   x   , H   y   d   / 2 +   O   y   )  . Here W and H are the width, respectively height of the viewport in pixels and   O   x   and   O   y   are the screen space coordinates of the viewport center, typically   O   x   = W / 2 ,    O   y   = H / 2 . Together, the projection from   R   3   to the rasterized screen is     P  :   R   3   \u2192   R   2      w   x   \u2254     P   11   W   2   ,    w   y   \u2254     P   22   H   2   ,    (1)   ( x , y , z )  \u2192  (    x    \u0303   ,    y    \u0303   )  \u2254  (   w   x     x   z   +   c   x   ,   w   y     y   z   +   c   y   )  .        Surfaces and trim curves. Models consist of rational parametric surfaces   (2)   x  : U \u228a   R   2   \u2192   R   3     ( u , v )  \u2192  x   ( u , v )  \u2254  (    x  ( u , v )      y  ( u , v )      z  ( u , v )     )    defined on a domain U . Often U \u2254    [ 0 . . 1 ]    2   , the unit square, and the surface (patch) is in tensor-product form with basis functions   b   k   ,    x   ( u , v )  \u2254   \u2211   i     \u2211   j      c    i j     b   i    ( u )    b   j    ( v )  .   To trim a patch means to restrict its domain to one side of a piecewise rational trim curve   (3)  \u03b3 : T \u228a R \u2192 U  t \u2192 \u03b3  ( t )  \u2254  ( u  ( t )  , v  ( t )  )  .   There can be multiple trim curves per patch. When trim curves are nested, their orientation can be used to determine which side is trimmed away. A simplified convention applies an alternating count from a domain boundary.  Trim curves are often rational approximations   \u03b3   1    ( t )  ,   \u03b3   2    ( t )  to the solutions of the surface\u2013surface intersection (SSI) problem,    x    1    (   u   1   ,   v   1   )  =    x    2    (   u   2   ,   v   2   )  \u2208   R   3   . We are not concerned with solving such potentially hard SSI systems of 3 equations in 4 unknowns, whose solution is generically a pair of non-rational algebraic curves. Rather we assume that the trim curves are given as rational curves and address the challenge of efficient accurate display of the resulting trimmed surface.  Trim curves can also be artist-defined or they can reference planar shapes. Such shapes, for example fonts, are mapped to the surface reshaped by the curvature of the map  x  .   Faithful surface tessellation. Ray-casting guarantees that each pixel represents a correctly-placed surface piece and finds each pixel\u2019s u v -pre-image. Remarkably, such pixel-accurate rendering can also be achieved using the highly efficient standard graphics pipeline. In\u00a0 [18], a compute shader determines a near-optimal (minimal) tessellation factor \u03c4 for each patch  x  , so that evaluating  x  on a \u03c4 \u00d7 \u03c4 partition of its domain U yields a proxy triangulation    x    \u02c6   \u2208   R   3   of  x  such that its image under the screen projection  P  of (1) agrees with that of the correct image. That is the difference  P   x   (  u  )  \u2212  P     x    \u02c6    (  u  )  is below the visible pixel threshold. We call    x    \u02c6   a faithful triangulation of  x  and will use    x    \u02c6   to render the trimmed surfaces.  Since any screen has a fixed discrete resolution, trimmed surfaces can and need only be resolved up to pixel width (sub-pixel width in the case of subsampling for anti-aliasing). Our approach is to pull back the pixel-grid to the domain U and partition U into u v -cells\u2014in such a way that no two pixels\u2019 u v -pre-images share one  u v -cell\u00a0(cf. Fig.\u00a03 ). This guarantees each pixel, in particular of a group straddling the projection of a surface trim, its individual trim test. Next we evaluate the trim curve in the domain U so that the resulting broken line differs from the exact trim curve by less than one u v -cell. The two discretizations, of the domain U and of the curve \u03b3 , provide a consistent, correct resolution in the sense that testing the u v -pre-image\u00a0of a pixel  (      x    \u0303     i   ,      y    \u0303     i   )  against the broken line yields a correct trim decision up to pixel resolution.  For a non-linear surface  x  followed by a perspective projection  P  , pulling the pixel-grid back exactly is not practical since the Jacobian of  P   x  can vary strongly. Multi-resolution can accommodate adaptively scaled u v -cells, but a complex hierarchical lookup slows down each fragment\u2019s trim test. We therefore opt for a simple two-level hierarchy. At the first level, we partition the domain U into   n   V     v -strips   V   i   :  (4)    V   i   \u2254  {  ( u , v )  \u2208 U ,    \u03bd   i   \u2264 v <   \u03bd   i + 1   }  .   Each v -strip\u00a0is a \u2018fat scan-line\u2019 where the u -coordinate is free while the v -coordinate is sandwiched between an upper and a lower bound. The default choice is to space the   v   i   uniformly and set   n   V   = \u03c4 , where \u03c4 is the surface tessellation factor already computed according to\u00a0 [18]. At the second level, each v -strip   V   i   is uniformly partitioned once more in the v -direction into a minimal number   \u03bc   i   of v -scan lines that guarantees correct trim resolution (see Fig.\u00a04 ). The first level partition isolates the impact of any local feature requiring high resolution to just the relevant v -strip(s). To determine the critical number   \u03bc   i   that guarantees that the distance between the screen images of two v -scan lines is less than one pixel, we proceed as follows. By the mean value theorem, for some   v   \u2217   \u2208  [ v , v + h ]  ,  (5)   |    x    \u0303    ( u , v )  \u2212    x    \u0303    ( u , v + h )  |  = h  |      x    \u0303     v    ( u ,   v   \u2217   )  |  ,       x    \u0303     v   \u2254   \u2202    x    \u0303     \u2202 v   .   If, for all v \u2208   V   i   and the v -scan line-spacing h > 0 ,   (6)  h    \u03c1   i    ( v )  < 1 ,  v \u2208  [   \u03bd   i   . .   \u03bd   i + 1   ]  ,       \u03c1   i    ( v )  \u2254 max  {   sup   u    |      x    \u0303     v    ( u , v )  |  ,   sup   u    |      y    \u0303     v    ( u , v )  |  }  ,    then the    x    \u0303   -distance between the screen images of the two v -scan lines    x    \u0303    ( u ,   v   j   )  and    x    \u0303    ( u ,   v   j   + h )  is less than a pixel and so is the    y    \u0303   -distance. Therefore setting  (7)    \u03bc   i   \u2254  \u2308  (   \u03bd   i + 1   \u2212   \u03bd   i   )    sup   v \u2208  [   \u03bd   i   . .   \u03bd   i + 1   ]      \u03c1   i    ( v )  \u2309    guarantees correct resolution. Fig.\u00a05 (a) shows an artifact when   \u03bc   i   is chosen too small. Fig.\u00a05(c) shows the correct and near-minimal choice. If the projection is orthographic and the surface is a faithful triangulation then   \u03bc   i   is simply the maximum size of the    x    \u0303   and the    y    \u0303   projection of the strip in pixels.   Estimating    \u03c1   i   . Since for any reasonable view the near-plane of the scene is at some minimal distance to the viewer, i.e. z \u2265   z  \u00af  > 0 , the expansion  (8)       x    \u0303     v   =      x    \u0303     x     x   v   +      x    \u0303     y     y   v   +      x    \u0303     z     z   v   =     w   x     z    (   x   v   \u2212   x   z     z   v   )    is well-defined, although potentially large for small z . For our (faithfully) triangulated surface, x and z are piecewise linear maps and   x   v   and   z   v   are piecewise constant. Determining an upper bound on   \u03c1   i    ( v )  over the three vertices  (   u   k   ,   v   k   )  ,  k = 1 , 2 , 3 of each triangle \u25b3 ,     \u03c1   \u25b3   \u2254   max   k    |   1   z   |    max   k    |   w   x    (   x   v   \u2212   x   z     z   v   )  |  ,   and setting   \u03bc   i   to the per-strip maximum,    (   \u03bd   i + 1   \u2212   \u03bd   i   )    max   \u25b3     \u03c1   \u25b3   ,  \u25b3 \u2229   V   i   \u2260 0\u0338 ,   typically yields a tight estimate. However, the estimate   \u03c1   \u25b3   can include (parts of) triangles outside the viewing frustum so that, if a user zooms in and a part of a triangle approaches the camera, z can be arbitrarily small. Instead, using the pixel\u2019s u v -pre-image\u00a0sent down through the graphics pipeline, we compute the analog of (8) for each pixel \u03b1 as  (9)    \u03c1   \u03b1   \u2254 max  {  |      x    \u0303     v    (   u   \u03b1   ,   v   \u03b1   )  |  ,  |      y    \u0303     v    (   u   \u03b1   ,   v   \u03b1   )  |  }    and set  (10)    \u03bc   i   \u2254  (   \u03bd   i + 1   \u2212   \u03bd   i   )    max   \u03b1     \u03c1   \u03b1   ,  \u03b1 \u2208   V   i   .   Now parts of triangles outside the viewing frustum do not contribute, and if a v -strip   V   i   lies outside the viewing frustum, it receives no votes   \u03c1   \u03b1   in (10) and   \u03bc   i   = 0 . That is,   \u03bc   i   provides a correct upper bound on   \u03c1   i    ( v )  for all visible samples, and that is exactly what is needed for correct resolution rendering.  The overall algorithm is summarized as follows. Each trim curve writes itself, at the correct resolution (determined by the v -scan line\u00a0density   \u03bc   i   defined in the previous section) into a slim u -intercept table. Each fragment then makes its trim decision by testing against the table entries.   Initializing the  u  -intercept table. The key data structure is the u -intercept table. There is one u -intercept table\u00a0per sub-surface, e.g. a tensor-product spline patch complex with   \u2113   u   \u00d7   \u2113   v   pieces. The u -intercept table\u00a0is an array of size \u03bc \u00d7 n where \u03bc \u2254   \u2211   i     \u03bc   i   is the total number of v -scan lines and n is an upper bound on the number of trim curves that may cross a v -scan line\u00a0of the surface piece. The row of the table with index  (11)     base    i   + j ,     base    i   \u2254   \u2211   k = 1   i \u2212 1     \u03bc   k   ,  j \u2254   v \u2212   \u03bd   i       \u03bd   i + 1   \u2212   \u03bd   i       \u03bc   i   ,   will contain a sorted list of u -intercepts: the intersections of the (linear segments of the correctly tessellated) trim curves \u03b3 with the v -scan line  ( u , v )  : v =   \u03bd   i   +   j     \u03bc   i      (   \u03bd   i + 1   \u2212   \u03bd   i   )  . Each v -strip   V   i   has its own number of rows   \u03bc   i   computed from the per-pixel densities   \u03c1   \u03b1   by a prefix sum and (10). The per-pixel densities   \u03c1   \u03b1   are computed via (8) at the end of the rendering pass before the next compute pass: the Fragment Shader\u00a0has access to x and z and, the Geometry Shader provides the Fragment Shader\u00a0with the Jacobian of each fragment\u2019s triangle.   Filling the  u  -intercept tables. (see Fig.\u00a04) Intersecting v -scan lines with a piecewise rational trim curve can be tricky. However, the required, correct v resolution of the trim curve,   min   i    (   \u03bd   i + 1   \u2212   \u03bd   i   )  /   \u03bc   i   , is known. Applying formula (9) with      x    \u0303     v   ,      y    \u0303     v   replaced by      x    \u0303     u   ,      y    \u0303     u   yields a u -evaluation density that guarantees correct resolution. (Fig.\u00a05(b) illustrates insufficient trim curve tessellation.) The bounds of\u00a0 [19] could be leveraged, but since the Compute Shader already uses slefe-based bounds to generate the faithful triangulation    x    \u02c6   according to\u00a0 [18], we use the same estimators to determine the curve tessellation factor for correct resolution (as defined in Section\u00a0 4). For the correct curve tessellation number, the Compute Shader\u00a0threads calculate, for each trim curve segment k in parallel, the end points \u03b3  (   t   k \u2212 1   )  and \u03b3  (   t   k   )  . Each thread then inserts the u -coordinates of all v -scan line-intersections with the trim curve segment into the u -intercept table. A second generation of threads subsequently sorts all trim curve intersections of a v -scan line\u00a0by their u -coordinates.   Testing against the  u  -intercept table. To determine whether a fragment with pre-image  ( u , v )  should be discarded, the Fragment Shader\u00a0reads the row of the table determined from v by (11) and locates u , by binary search in the table (see Fig.\u00a04(b)). The complexity of the binary search is   log   2   n , i.e. 4 tests if the number of intercepts is n = 16 . The Fragment Shader\u00a0then makes the trim decision based on the parity of the entry just smaller than u (and possibly orientation hints).   Fig.\u00a06 shows our mapping of accurate trim display onto the graphics pipeline. The main add-on is the Compute Shader\u00a0pass, shown as the top row of the flowchart. The trim test is performed in the Fragment Shader. The Geometry Shader can be removed by adding its work to the Fragment Shader.   Consistent intersections. Care has to be taken to avoid duplicate or missing intersections where the v -scan line\u00a0intersects two consecutive trim curve segments at the common end point. If the v -scan line\u00a0does not cross but just touches the two segments, either two or zero intersections should be recorded, not one. We achieve consistency by treating every segment as a bottom-open top-closed interval (see Fig.\u00a07 ).   Merging  u  -intercept tables. To minimize overhead in launching a compute shader and memory allocation we can and do group together the u -intercept tables of several sub-surfaces, such as several tensor-product NURBS patches (see next).   Jointly trimming multiple patches. The grouping in the previous paragraph was aimed at improving performance. It also makes sense to have collections of patches in irregular layout share one domain. For example, when patches    x    k   :   U   k   \u228a   R   2   \u2192   R   3   are joined in a geometrically-continuous fashion their charts are related by a reparameterization  (   u   \u0303   ,   v   \u0303   )  :   U   k   \u2192 U \u228a   R   2   that maps the individual domains to a joint region U in the plane. Our approach applies to this scenario with  ( u , v )  replaced by  (   u   \u0303   ,   v   \u0303   )  and the joint trim curve specified in U . Fig.\u00a08 illustrates the use of such a joint domain for two multi-spline caps.   Treating trim-data mismatches. When a CAD modeling-kernel exports a trim derived from an intersection, the algebraic trim curves are approximated by rational trim curves whose images only match up to system-default or user-set tolerances. It is not the job of the rendering engine to fill such gaps and adjust SSI tolerances: correct resolution should display such gaps and let the designer know. However, pixel dropout can also result from more subtle sub-pixel resolution mismatches. When multiple surfaces partly cover a pixel, the pixel\u2019s sample location(s) may not be covered at all. Fig.\u00a09 (a) illustrates the concern. While this type of pixel dropout is tricky to deal with in the general setting\u00a0 [20], the faithful triangulation    x    \u02c6   of our setup guarantees that these mismatches are of size less than one pixel. To cover these pixel-sized gaps, our algorithm additionally draws the patch trim boundaries    x    \u02c6   \u2218 \u03b3 (as images of the already correctly tessellated trim curves) and so achieves correct coverage Fig.\u00a09(b).   Multi-sample anti-aliasing. ( m -MSAA) improves silhouettes and (trim) boundaries by testing object coverage at m locations per pixel (compare Fig.\u00a010 (a)\u2013(b) where m = 4 ). We add sub-pixel trim testing by decreasing the screen spacing of v -scan lines in (6) so that the trim has sub-pixel accuracy. The Fragment Shader then corrects the coverage mask according to the per sub-pixel trim test result (see Fig.\u00a010(c)). The main cost of m -MSAA is in creating finer u -intercept tables, not in the Fragment Shader\u00a0test.   Opportunistic optimization. Whenever the view is unchanged, neither the surface tessellation nor the u -intercept table\u00a0need to be recomputed. Whenever the geometry is unchanged, neither the surface slefe-boxes of the pixel-accurate patch rendering nor the trim curve partition need to be recomputed.   RESULTS   We implemented the trim-render add-on in the OpenGL 4.3 API and benchmarked the implementation on the CAD models in Fig.\u00a011 . To make the benchmark easily replicable, we fixed the number of v -strips per spline surface to 128 and packed their v -scan lines into u -intercept tables of 16384 rows. We allocated space so that each row can hold up to n = 32 intersections (but never encountered more than 8 intersections; see Fig.\u00a012 ).   Table\u00a01 breaks down the overall work and the work for maintaining the u -intercept table. To measure the maximally interactive case, we did not use opportunistic optimization. The full algorithm displayed by the flowchart of Fig.\u00a06 was executed for each render pass. As observed in\u00a0 [8], a direct performance comparison to earlier implementations is challenging due to their implementation complexity and choice of scenes and textures. Moreover, recent advances in hardware acceleration, notably the introduction of the tessellation engine, have changed the bottlenecks when rendering spline surfaces. To nevertheless give an idea of the magnitude of acceleration, we note that none of the earlier trim-surface rendering algorithms promised interactive trim adjustment. We show the increased flexibility and speed of the trim-rendering add-on in the accompanying video, by demonstrating interactive trim modification.  The GPU memory used by the add-on is that of the slim u -intercept table, set to 214 rows and 25 columns plus, for each surface piece consisting of many patches, 27  v -strip\u00a0indices and the numbers   \u03bc   i   . The graph in Fig.\u00a013 compares GPU memory usage for different zoom levels and the Mini data set of the u -intercept table\u00a0vs. texture-based trimming. (Identical trimming precision was enforced by setting the texture resolution to   n   V   \u22c5   max   i     \u03bc   i   .)   Performance under zoom and different resolution. Fig.\u00a014 illustrates work distribution when zooming in. As the surface(s) fill more and more of the screen, the surface rendering time (red) dominates, while work for calculating the patch tessellation levels decreases since many patches are recognized to fall outside the viewing frustum by the algorithm of\u00a0 [18] and are discarded. Fig.\u00a015 shows the increase in run time when the render window increases from 400\u00d7400 to 1200\u00d71200. Since the patch-based work, for determining the tessellation factor that guarantees pixel-accuracy\u00a0 [18], stays constant, the proportional increase of work for drawing (pixel fill) and the trim test determine the overall cost.   DISCUSSION   As surveyed in Section\u00a0 2, existing approaches to rendering trimmed spline surfaces require extensive pre-processing and re-approximation of the data or expensive off-line ray casting that interfere with the designer\u2019s work flow. Correct resolution rendering can give immediate and visually accurate feedback by discretizing at just the right level. A direct timing comparison with the implementations of\u00a0 [9,7,8] is not possible since the GPU hardware has evolved, and meaningful comparison requires that all rendering be (sub-)pixel accurate. However timing comparisons are not crucial since a qualitative comparison already brings out the relative strengths. On one hand, the correct resolution approach avoids generating large trim textures that are a bottleneck in\u00a0 [9]. On the other hand, the trim test based on the u -intercept table\u00a0is faster and more robust than retrieving and solving equations in\u00a0 [7]. And while approaches such as\u00a0 [8] leverage extensive pre-processing, the u -intercept table\u00a0is built on the fly, enabling interactive trim adjustment. Accuracy via the simple u -intercept table\u00a0data structure is made possible by tight estimates and the fact that the screen resolution is known at run time.  In terms of GPU usage, an important characteristic of correct resolution rendering is that the Fragment Shader\u00a0does little extra work. This is important since rendering is typically Fragment Shader\u00a0bound due to heavy use by expensive pixel shading operations.   Worst case complications. Computing the per-pixel densities   \u03c1   \u03b1   at the end of the previous rendering pass creates an information lag. This lag is imperceptible at the targeted frame rates above ten.  The v -partition number   \u03bc   i   bounds the arclength of the projection of a (piece of a) u -scan line onto the screen-space. For typical surfaces the arclength, and hence the size of the u -intercept table\u00a0is a (small) fraction of the screen size   w   x   , at ca. 1000 a relatively small number. If, however, a surface piece is highly oscillating,   \u03bc   i   can in principle exceed   w   x   and increase work and memory requirements.   In summary. The light-weight trim add-on, based on tight conservative estimates of correct resolution, enables interactive anti-aliased display (see Fig.\u00a016 ) during the design of trimmed spline surfaces. By adapting the curve tessellation level and the v -scan line\u00a0density, correct resolution rendering can guarantee accuracy while keeping the memory footprint small.   ACKNOWLEDGMENTS   We thank Michael Guthe, Bernd Froehlich and Andre Schollmeyer for pointers to trimmed NURBS models and helpful comments concerning their algorithms. We thank GrabCad.com for providing models. This work was supported in part by NSF grant CCF-1117695.   REFERENCES", "highlights": "Current strategies for real-time rendering of trimmed spline surfaces re-approximate the data, pre-process extensively or introduce visual artifacts. This paper presents a new approach to rendering trimmed spline surfaces that guarantees visual accuracy efficiently, even under interactive adjustment of trim curves and spline surfaces. The technique achieves robustness and speed by discretizing at a near-minimal correct resolution based on a tight, low-cost estimate of adaptive domain griding. The algorithm is highly parallel, with each trim curve writing itself into a slim lookup table. Each surface fragment then makes its trim decision robustly by comparing its parameters against the sorted table entries. Adding the table-and-test to the rendering pass of a modern graphics pipeline achieves anti-aliased sub-pixel accuracy at high render-speed, while using little additional memory and fragment shader effort, even during interactive trim manipulation."}